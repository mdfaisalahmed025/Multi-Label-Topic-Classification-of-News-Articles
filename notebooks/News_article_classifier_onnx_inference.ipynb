{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWZfX9RGxDWn"
      },
      "source": [
        "# Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqbGE5VDwwk4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LIYeFLnxHHE",
        "outputId": "9539bf2d-f20a-464a-9ca1-27608f46dedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q transformers[sentencepiece] fastbook fastai nbdev plum-dispatch evaluate seqeval onnxruntime onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqCGrGu1JJHv",
        "outputId": "a644a2bb-fe8e-4d72-8c6d-3e5cd2b432f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'blurr'...\n",
            "remote: Enumerating objects: 5063, done.\u001b[K\n",
            "remote: Counting objects: 100% (890/890), done.\u001b[K\n",
            "remote: Compressing objects: 100% (310/310), done.\u001b[K\n",
            "remote: Total 5063 (delta 701), reused 699 (delta 573), pack-reused 4173 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5063/5063), 27.00 MiB | 25.22 MiB/s, done.\n",
            "Resolving deltas: 100% (3932/3932), done.\n",
            "/content/blurr\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/msi1427/blurr.git\n",
        "%cd blurr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "5da3eca460664127a364c178a1228067",
            "1f00910d5e5549c4917d601a5c4a56d4",
            "f46c2c4bdb624ac08d6ee56731c19143",
            "e6827bc422ff4b0fa53331cd193f2f8c",
            "bb2ed2d35fb04aa8b710d5da1b25bf08",
            "29c66f470afd42b699805e158736adb2",
            "046a66649a7f4718811474a4edc661a7",
            "06c7f473498b48349458479b01bea4b1",
            "363e896156bb4c99a4710f769796efb6",
            "6715d55e80614626b4d27dca7f6488e5",
            "ba196398a17044e89ef11f09eda2db8d",
            "d78cab50a560408ea5dbbbda451b4368",
            "ff44dd8657b44db2a114fa08d4996751",
            "6cc4bd3bda294711b890ed2dd26bbb11",
            "7d5a6e219cc748ef869f034f51ccd57b",
            "0db651db8d7e4b9e9f872124c540d3bb",
            "4569feb4e77d4b328066d742c8f6a092",
            "3d9a5678429a41599388c07113d4fdf9",
            "e30aed96ce9549499cb1ed1b4d4b0732",
            "c63f276bb31f4051ab3e3e0000cbe274",
            "13dad67435aa48c683cbd9494d591b53",
            "b22dc52b21d949b2b9f8b85923ccef11"
          ]
        },
        "id": "MSDGY3C1xYb3",
        "outputId": "424a933a-b4da-4f56-f5b7-bf29a38f2321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5da3eca460664127a364c178a1228067"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d78cab50a560408ea5dbbbda451b4368"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "from fastai.text.all import *\n",
        "from blurr.text.data.all import *\n",
        "from blurr.text.modeling.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kamDZGsxb_f"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmdjs_Vyxfye",
        "outputId": "1df1a5bc-9b03-4972-e8b9-66939770268a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eTnIaJ4xg2j",
        "outputId": "eabc346e-9911-41ac-cfb1-5cab3e787322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/NLP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/datasets/NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCgEpADxukk"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "_6olMHPyxveX",
        "outputId": "44fb3c1e-2877-4275-bfed-840c17071ec3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                 title  \\\n",
              "0                 Starmer faces rebellion over plan to cut jury trials   \n",
              "1                            UK to ban deepfake AI 'nudification' apps   \n",
              "2                 Scotland Politics | Latest News & Updates | BBC News   \n",
              "3        Foreign prison officers exempted from new stricter visa rules   \n",
              "4  Batters review: 'Bewildered' farmers need new deal to be profitable   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  Nearly 40 Labour MPs have warned the prime minister they are not prepared to support proposals to limit jury trials in England and Wales. In a letter to Sir Keir Starmer, MPs largely from the left of the party said restricting juries to major offences carrying three-year terms was \"madness and will cause more problems than it solves\". Former shadow attorney general Karl Turner, who organised the letter, said he will vote against Labour for the first time since Sir Keir took charge, branding the plans \"simply unworkable\". The government insisted it would go ahead with the plans, adding they...   \n",
              "1  The UK government says it will ban so-called \"nudification\" apps as part of efforts to tackle misogyny online. New laws - announced on Thursday as part of a wider strategy to halve violence against women and girls - will make it illegal to create and supply AI tools letting users edit images to seemingly remove someone's clothing. The new offences would build on existing rules around sexually explicit deepfakes and intimate image abuse, the government said. \"Women and girls deserve to be safe online as well as offline,\" said Technology Secretary Liz Kendall. \"We will not stand by while tec...   \n",
              "2  The Scottish parliament's presiding officer says the incident was part of an employment dispute involving an MSP. Their union BMA Scotland has accused the government of reneging on a commitment to restore pay to 2008 levels. Opposition party leaders grilll John Swinney in the final First Minister's Questions before Christmas Why did the people of Whitburn - and nearby Blackburn - choose Nigel Farage's party over others? The delivery date for MV Glen Rosa is pushed back by up to another six months while there is a further short delay with the Turkish-built Isle of Islay. The first minister ...   \n",
              "3  Foreign nationals working as prison officers in the UK have been given a temporary exemption from new visa rules, following warnings some jails were facing a staffing crisis. Prisons have increasingly been relying on overseas recruits, particularly from Nigeria and Ghana. But organisations representing officers had warned jails faced losing thousands of staff, after the government increased the minimum salary requirement for a skilled worker visa. The Ministry of Justice said the move would \"ensure jails can continue to run safely with the right level of experienced staff\". Under changes i...   \n",
              "4  Farmers are \"bewildered and frightened\" with many questioning the future of their businesses because of the government's proposed changes to inheritance tax, an independent review of farm profitability has found. The long-awaited government-commissioned report was published on Thursday with 57 recommendations designed to improve productivity, investment and resilience in agriculture. But author Baroness Minette Batters, former president of the National Farmers' Union (NFU), warned there was \"no silver bullet\" to making farms in England profitable. Environment secretary Emma Reynolds said t...   \n",
              "\n",
              "         labels source                                                  url  \\\n",
              "0  ['Politics']    BBC       https://www.bbc.com/news/articles/c93w771g14go   \n",
              "1  ['Politics']    BBC       https://www.bbc.com/news/articles/cq8dp2y0z7wo   \n",
              "2  ['Politics']    BBC  https://www.bbc.com/news/scotland/scotland_politics   \n",
              "3  ['Politics']    BBC       https://www.bbc.com/news/articles/cp89p1x26kxo   \n",
              "4  ['Politics']    BBC       https://www.bbc.com/news/articles/c9975z55dr5o   \n",
              "\n",
              "  section  \n",
              "0     NaN  \n",
              "1     NaN  \n",
              "2     NaN  \n",
              "3     NaN  \n",
              "4     NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efeb062b-24d1-401d-91c1-b87f9803d8a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>source</th>\n",
              "      <th>url</th>\n",
              "      <th>section</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Starmer faces rebellion over plan to cut jury trials</td>\n",
              "      <td>Nearly 40 Labour MPs have warned the prime minister they are not prepared to support proposals to limit jury trials in England and Wales. In a letter to Sir Keir Starmer, MPs largely from the left of the party said restricting juries to major offences carrying three-year terms was \"madness and will cause more problems than it solves\". Former shadow attorney general Karl Turner, who organised the letter, said he will vote against Labour for the first time since Sir Keir took charge, branding the plans \"simply unworkable\". The government insisted it would go ahead with the plans, adding they...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/c93w771g14go</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UK to ban deepfake AI 'nudification' apps</td>\n",
              "      <td>The UK government says it will ban so-called \"nudification\" apps as part of efforts to tackle misogyny online. New laws - announced on Thursday as part of a wider strategy to halve violence against women and girls - will make it illegal to create and supply AI tools letting users edit images to seemingly remove someone's clothing. The new offences would build on existing rules around sexually explicit deepfakes and intimate image abuse, the government said. \"Women and girls deserve to be safe online as well as offline,\" said Technology Secretary Liz Kendall. \"We will not stand by while tec...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/cq8dp2y0z7wo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scotland Politics | Latest News &amp; Updates | BBC News</td>\n",
              "      <td>The Scottish parliament's presiding officer says the incident was part of an employment dispute involving an MSP. Their union BMA Scotland has accused the government of reneging on a commitment to restore pay to 2008 levels. Opposition party leaders grilll John Swinney in the final First Minister's Questions before Christmas Why did the people of Whitburn - and nearby Blackburn - choose Nigel Farage's party over others? The delivery date for MV Glen Rosa is pushed back by up to another six months while there is a further short delay with the Turkish-built Isle of Islay. The first minister ...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/scotland/scotland_politics</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Foreign prison officers exempted from new stricter visa rules</td>\n",
              "      <td>Foreign nationals working as prison officers in the UK have been given a temporary exemption from new visa rules, following warnings some jails were facing a staffing crisis. Prisons have increasingly been relying on overseas recruits, particularly from Nigeria and Ghana. But organisations representing officers had warned jails faced losing thousands of staff, after the government increased the minimum salary requirement for a skilled worker visa. The Ministry of Justice said the move would \"ensure jails can continue to run safely with the right level of experienced staff\". Under changes i...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/cp89p1x26kxo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Batters review: 'Bewildered' farmers need new deal to be profitable</td>\n",
              "      <td>Farmers are \"bewildered and frightened\" with many questioning the future of their businesses because of the government's proposed changes to inheritance tax, an independent review of farm profitability has found. The long-awaited government-commissioned report was published on Thursday with 57 recommendations designed to improve productivity, investment and resilience in agriculture. But author Baroness Minette Batters, former president of the National Farmers' Union (NFU), warned there was \"no silver bullet\" to making farms in England profitable. Environment secretary Emma Reynolds said t...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/c9975z55dr5o</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efeb062b-24d1-401d-91c1-b87f9803d8a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-efeb062b-24d1-401d-91c1-b87f9803d8a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-efeb062b-24d1-401d-91c1-b87f9803d8a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e7766a70-3077-4f19-9098-74218dac4e42\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7766a70-3077-4f19-9098-74218dac4e42')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e7766a70-3077-4f19-9098-74218dac4e42 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 95730,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95626,\n        \"samples\": [\n          \"Business is doing nicely, thank you, while workers get steadily poorer\",\n          \"Fresco fragment from Pompeii reopens row over \\u2018looted\\u2019 artefacts\",\n          \"Scheme to stop people being quizzed by abuser in court failing, lawyers say\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95730,\n        \"samples\": [\n          \"Access to BBC websites has been restricted in Russia, hours after the corporation brought back its shortwave radio service in Ukraine and Russia to ensure civilians in both countries can access news during the invasion. State communications watchdog Roskomnadzor restricted access to BBC Russia\\u2019s online presence, as well as Radio Liberty and the Meduza media outlet, the state-owned Russian RIA news agency reported on Friday. On Friday, Globalcheck, a service that tracks internet censorship in the countries of the Commonwealth of Independent States (CIS), reported that the availability of the entire BBC website was at 17% of normal levels in Russia, which suggested some services had been blocked. Later on Friday, the BBC shared methods to circumvent the ban, including the Pisphon app or the Tor browser. BBC Russia also reported that Meta, formerly known as Facebook, also appeared to be blocked, as was Google Play. The signs the BBC was being blocked emerged hours after the BBC\\u2019s decision to revert to a mostly obsolete form of broadcasting, broadcasting four hours of its world service, read in English, to Ukraine and parts of Russia each day. \\u201cIt\\u2019s often said truth is the first casualty of war,\\u201d BBC director general Tim Davie said in announcing the move on Thursday. \\u201cIn a conflict where disinformation and propaganda is rife, there is a clear need for factual and independent news people can trust \\u2026 millions more Russians are turning to the BBC.\\u201d The German public broadcaster Deutsche Welle also reported in Russian that the BBC site was not working in Russia. The BBC\\u2019s shortwave radio broadcast can be found on 15735 kHz from 4pm to 6pm and on 5875 kHz from 10pm to midnight, Ukraine time. The BBC\\u2019s move to bring back shortwave came days after Russia launched two missiles on Kyiv\\u2019s TV tower, killing five people and knocking out some access to news and broadcasts. Ukraine\\u2019s defence minister, Oleksii Reznikov, wrote on Twitter that the Kremlin was preparing to cut off communications and spread \\u201cmassive fake messages that the country\\u2019s leadership has given up\\u201d. Russia has clamped down on public dissent at home, while Kremlin-backed media organisations such as RT have been pulled internationally. The Kremlin has complained about the BBC\\u2019s coverage of the invasion, with Russia\\u2019s foreign ministry spokeswoman Maria Zakharova claiming without evidence that the BBC \\u201cplays a determined role in undermining the Russian stability and security\\u201d. Zakharova also claimed Russia had been the victim of \\u201cunprecedented information terrorism\\u201d that was \\u201ccreating hysteria around Ukrainian events\\u201d. The BBC has reported huge increases in its audiences in Ukraine and Russia since the invasion began. In the last week of February, viewership of BBC\\u2019s Ukrainian language website more than doubled from a year earlier to 3.9 million visitors. Its Russian-language website has reached a record 10.7 million visitors over the past week, more than tripling its weekly 2022 average. Shortwave radio uses frequencies that can travel long distances and are accessible on portable radios, making it the go-to method to reach listeners in conflict zones throughout history. Shortwave was used extensively in Europe to broadcast propaganda during the second world war, and usage peaked during the cold war. But as radio technology developed, along with the mass adoption of online news, shortwave fell out of favour around the world. After 76 years, the BBC World Service ended its shortwave broadcast to Europe in 2008. Press Association contributed to this report This article was updated on 5 March 2022, to correct the local times of the BBC broadcast.\",\n          \"The head of NatWest has apologised to Nigel Farage for what she called \\u201cdeeply inappropriate comments\\u201d about the former Ukip leader in an internal report that led to the closure of his accounts at the group\\u2019s exclusive private bank, Coutts. Dame Alison Rose issued a public statement on Thursday and wrote a letter to Farage apologising for the way the NatWest subsidiary had handled its decision to cut ties with the Brexit campaigner. It came after a series of interventions by the prime minister and senior members of his cabinet, and more than two weeks after Farage first disclosed that his accounts had been closed by Coutts because it no longer wanted him as a client. On Thursday night, Farage welcomed the apology from Rose but said it was only \\u201ca start\\u201d and claimed she had been forced into it by the Treasury. An internal report had described Farage as a \\u201cdisingenuous grifter\\u201d who promotes \\u201cxenophobic, chauvinistic and racist views\\u201d. After obtaining a copy and releasing it to the press, Farage had described the dossier as a \\u201cprejudiced, nasty document\\u201d. Rose said the comments, prepared for Coutts\\u2019 wealth reputation risk committee, \\u201cdo not reflect the view of the bank\\u201d. She added: \\u201cNo individual should have to read such comments and I apologise to Mr Farage for this.\\u201d However, she stopped short of reinstating Farage as a Coutts client, instead reiterating an offer to open a basic account for him at NatWest. It follows days of mounting public and political pressure, with Rishi Sunak intervening to warn it \\u201cwouldn\\u2019t be right if financial services were being denied to anyone exercising their right to lawful free speech\\u201d. The home secretary, Suella Braverman, called the decision \\u201csinister\\u201d. The government announced on Thursday that it would force banks to \\u201cexplain and delay\\u201d any decision to close accounts. Andrew Griffith, economic secretary to the Treasury, said \\u201cbanks occupy a privileged place in society\\u201d and must allow \\u201ceveryone to express themselves freely\\u201d without fear of losing their access to banking. It also ordered the Financial Conduct Authority to begin a review of the rules governing how banks treat \\u201cPolitically Exposed Persons\\u201d (Peps) such as Farage over the next two months. Lasting up to a year, the review will examine whether to ease the tough rules inherited from the EU on domestic Peps while maintaining them on foreigners with bank accounts in the UK, in line with international standards. Before Farage made the dossier public, reports had cited sources at Coutts saying that the decision to \\u201cunbank\\u201d Farage was purely because his account had fallen below its \\u201cwealth limit\\u201d, which requires customers to borrow or invest at least \\u00a31m or hold \\u00a33m in savings. The former politician fell below this threshold after his mortgage expired this year. However, Coutts\\u2019 reputation committee report also said Farage posed a risk, accusing him of being \\u201cseen as xenophobic and racist\\u201d and of making remarks that are \\u201cdistasteful and appear increasingly out of touch with wider society\\u201d. On his GB News programme on Thursday night Farage accused Rose of being forced into an apology by the Treasury. He said: \\u201cIn life it is always good to get an apology, so thank you Dame Alison for apologising. What I\\u2019ve actually been told quietly, privately, is that you were forced into doing this by the Treasury. \\u201cBut at least you\\u2019ve done it, I suppose. But the whole letter smacks of \\u2018not me, guv, I mean I\\u2019m just the chief executive, I mean, don\\u2019t blame me for what the banks under my direct control are doing\\u2019.\\u201d He said there were thousands in the same position as him, as he vowed to battle on. \\u201cI\\u2019m afraid I can\\u2019t just walk away from this. I\\u2019ve started this, and I\\u2019ve got to continue. So thank you for the apology. It\\u2019s a start, but it\\u2019s no more than that.\\u201d Rose had said it was not the bank\\u2019s policy \\u201cto exit a customer on the basis of legally held political and personal views\\u201d. \\u201cBoth freedom of expression and access to banking are fundamental to our society,\\u201d she said. \\u201cDecisions to close an account are not taken lightly and involve a number of factors including commercial viability, reputational considerations, and legal and regulatory requirements.\\u201d Announcing the new account closure rules, which will come into effect later this summer and are likely to be among the first post-Brexit changes to banking rules, Griffith said: \\u201cFreedom of speech is a cornerstone of our democracy, and it must be respected by all institutions. \\u201cThese changes will boost the rights of customers \\u2013 providing real transparency, time to appeal and making it a much fairer playing field.\\u201d The changes will increase the notice period for closing accounts to 90 days \\u2013 giving customers more time to challenge a decision through the Financial Ombudsman Service. Banks will also be required to spell out why they are terminating an account \\u2013 increasing transparency for customers and aiding their efforts to overturn decisions. The Treasury said the changes could only be made due to new powers in the Financial Services and Markets Act 2023. The proposed new rules follow a call for evidence launched in January, after PayPal\\u2019s temporary suspension last year of several accounts, including that of Toby Young, the social commentator and director of the Free Speech Union. The Treasury said the changes were needed to \\u201censure the right balance is being struck between protecting customers, and providers\\u2019 rights to manage commercial risk\\u201d.\",\n          \"There were negatives, of course. Shall we focus on the negatives? Shall we dwell on the frailties a little? The uncharacteristic errors, the double faults, an occasional scruffiness at the net, the frequent slumps in intensity? Shall we marvel at the fact that the lowest-ranked player in the tournament earned more break points (11) than one of the greatest players of his generation (10)? Shall we warn, in a tone of affected sternness, that the defending champion will have to raise his game on this evidence? Of course we shall, because this is Carlos Alcaraz, and because there is an entire cottage industry built around maintaining the idea that Alcaraz is in a state of crisis at all times, a state of crisis so acute that it is necessary to feign round-the-clock concern for him. We just want to see all that rich talent fulfilled. That\\u2019s all it is. Sincerely and genuinely. And definitely not a weirdly prurient interest in his holidays to Ibiza, or whether him and Emma Raducanu are, you know. Just the talent. Thinking of the talent here. And of course the illicit pleasure and enduring appeal of Alcaraz is that he so readily indulges these desires. He emotes. He misses a lot. He pulls off spectacular acrobatic winners immediately after missing a lot. He lives without inhibitions or regrets. Alcaraz is essentially a magic-eye puzzle you can read in whatever way you want, and after the sweat-drenched psychodrama of Fabio Fognini on Monday night there were more danger signs in his 6-1, 6-4, 6-4 win over Britain\\u2019s Oliver Tarvet; if you really, really wanted to see them. The more prosaic truth is that Alcaraz was playing himself as much as he was playing the world No 733 from St Albans. Tarvet is one of those classic British folk heroes the early rounds of Wimbledon always seem to throw up, complete with shaggy-dog backstory and tabloid headline-friendly name. And if Alcaraz was expecting an easy afternoon of Tarvet practice, he quickly discovered that it would be anything but easy shaking off this particular Tarvet from his back. Certainly Tarvet seemed to get an early read on the Alcaraz serve, used his speed and coverage to trade happily from the baseline, rode the early waves of noise from the home crowd. Some of his passes were sublime. Above all he looked untroubled, unfazed, hyper- confident, like a crypto-billionaire who had won a game against Carlos Alcaraz in a charity auction. \\u201cGood serve,\\u201d he called out at one point as his opponent pinned him with a vicious delivery to the body. Alcaraz shot him a look as if to say: yeah. Obviously it\\u2019s a good serve. I\\u2019m Carlos Alcaraz. Who are you again? But of course no read on Alcaraz\\u2019s serve is ever going to rival the read he has on yours. And though both men kept swinging, while Tarvet created break points and saved others, the only real jeopardy here was of the confected variety. Tarvet probably played the best match of his life, and in the end it was like bringing a sword to a sword fight when your opponent has about six far superior swords. There was a sadistic relish to the way Alcaraz kept teasing him with the drop shot, occasionally missing, mostly succeeding. But of course the drop shot, such a staple of the Alcaraz game, is also a stick to beat him with. Missing it costs one point, exactly the same as putting a forehand an inch long. And yet some misses are clearly more moral than others. For his detractors Alcaraz\\u2019s missed drop shots will always be taken in evidence against him, proof of his essential flimsiness. So, once again, we must deal with the principal charge. Alcaraz is inconsistent. It\\u2019s true, because everyone says so, to the point where it has basically passed into objective fact. Like the objective fact that Alcaraz has a win percentage of 90% so far this year, has won five of his six finals, is slowly putting together one of the greatest seasons in the modern era. His grass-court record stands comparison with the all-timers. This is the sort of inconsistency all but one of his rivals would dream of. But of course this is a stylistic as much as it is an empirical judgment. It is true that there is a big gulf between his highest and lowest levels. That he occasionally loses to people like Botic van de Zandschulp. That towards the end of last season and for a small portion of this he has looked a little unmoored, a little rattled. And yet how much of this reaction stems from a desire to see crisis, to armchair-analyse, to draw a straight line from his personal choices to his tennis as a way of justifying our interest in his personal choices? In a way, Alcaraz\\u2019s entire game serves as a kind of rejoinder to all this. This is, remember, still a player with just 34 tour games on grass, still adding levels and tones to his game, still learning how to master the mental side, still trying to work out exactly how famous he wants to be. In the meantime he\\u2019s going to keep going for the lines, keep going to Ibiza, keep trying the drop shot, keep embracing the chaos, because that\\u2019s the only way he knows. What if he misses, you scream. Fine. But what if he doesn\\u2019t?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 65,\n        \"samples\": [\n          \"['Energy']\",\n          \"['Climate', 'Health']\",\n          \"['Politics']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Guardian\",\n          \"BBC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95729,\n        \"samples\": [\n          \"https://www.theguardian.com/society/2025/apr/11/gamechanging-breast-cancer-pill-to-be-offered-on-nhs-in-england-and-wales\",\n          \"https://www.theguardian.com/politics/2023/jul/20/with-all-his-plans-in-tatters-rishi-eyes-some-reshuffle-lolz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"law\",\n          \"media\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/news_multilabel_merged.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPUmbTEIJNfK"
      },
      "outputs": [],
      "source": [
        "# import ast\n",
        "\n",
        "# # 1. Convert the 'labels' column from strings like \"['a', 'b']\" to actual Python lists\n",
        "# # We use ast.literal_eval because it is safer than eval()\n",
        "# def convert_to_list(val):\n",
        "#     if isinstance(val, str):\n",
        "#         try:\n",
        "#             return ast.literal_eval(val)\n",
        "#         except (ValueError, SyntaxError):\n",
        "#             return []  # Return empty list if string is malformed\n",
        "#     return val if isinstance(val, list) else []\n",
        "\n",
        "# df['labels'] = df['labels'].apply(convert_to_list)\n",
        "\n",
        "# # 2. Drop rows that are NOT multi-label\n",
        "# # (This keeps only rows where the number of labels is 2 or more)\n",
        "# df = df[df['labels'].map(len) > 1].reset_index(drop=True)\n",
        "\n",
        "# # 3. Print the first 100 rows to verify the format\n",
        "# print(f\"Remaining rows after filtering: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMgtvdgkJ0Az",
        "outputId": "2a948cee-c855-4afb-a1c0-5761969e99d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95730, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "aj94SrzTJSRn",
        "outputId": "42c0a32d-48aa-45a6-bbf4-07e9c5eafd8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                 title  \\\n",
              "0                 Starmer faces rebellion over plan to cut jury trials   \n",
              "1                            UK to ban deepfake AI 'nudification' apps   \n",
              "2                 Scotland Politics | Latest News & Updates | BBC News   \n",
              "3        Foreign prison officers exempted from new stricter visa rules   \n",
              "4  Batters review: 'Bewildered' farmers need new deal to be profitable   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  Nearly 40 Labour MPs have warned the prime minister they are not prepared to support proposals to limit jury trials in England and Wales. In a letter to Sir Keir Starmer, MPs largely from the left of the party said restricting juries to major offences carrying three-year terms was \"madness and will cause more problems than it solves\". Former shadow attorney general Karl Turner, who organised the letter, said he will vote against Labour for the first time since Sir Keir took charge, branding the plans \"simply unworkable\". The government insisted it would go ahead with the plans, adding they...   \n",
              "1  The UK government says it will ban so-called \"nudification\" apps as part of efforts to tackle misogyny online. New laws - announced on Thursday as part of a wider strategy to halve violence against women and girls - will make it illegal to create and supply AI tools letting users edit images to seemingly remove someone's clothing. The new offences would build on existing rules around sexually explicit deepfakes and intimate image abuse, the government said. \"Women and girls deserve to be safe online as well as offline,\" said Technology Secretary Liz Kendall. \"We will not stand by while tec...   \n",
              "2  The Scottish parliament's presiding officer says the incident was part of an employment dispute involving an MSP. Their union BMA Scotland has accused the government of reneging on a commitment to restore pay to 2008 levels. Opposition party leaders grilll John Swinney in the final First Minister's Questions before Christmas Why did the people of Whitburn - and nearby Blackburn - choose Nigel Farage's party over others? The delivery date for MV Glen Rosa is pushed back by up to another six months while there is a further short delay with the Turkish-built Isle of Islay. The first minister ...   \n",
              "3  Foreign nationals working as prison officers in the UK have been given a temporary exemption from new visa rules, following warnings some jails were facing a staffing crisis. Prisons have increasingly been relying on overseas recruits, particularly from Nigeria and Ghana. But organisations representing officers had warned jails faced losing thousands of staff, after the government increased the minimum salary requirement for a skilled worker visa. The Ministry of Justice said the move would \"ensure jails can continue to run safely with the right level of experienced staff\". Under changes i...   \n",
              "4  Farmers are \"bewildered and frightened\" with many questioning the future of their businesses because of the government's proposed changes to inheritance tax, an independent review of farm profitability has found. The long-awaited government-commissioned report was published on Thursday with 57 recommendations designed to improve productivity, investment and resilience in agriculture. But author Baroness Minette Batters, former president of the National Farmers' Union (NFU), warned there was \"no silver bullet\" to making farms in England profitable. Environment secretary Emma Reynolds said t...   \n",
              "\n",
              "         labels source                                                  url  \\\n",
              "0  ['Politics']    BBC       https://www.bbc.com/news/articles/c93w771g14go   \n",
              "1  ['Politics']    BBC       https://www.bbc.com/news/articles/cq8dp2y0z7wo   \n",
              "2  ['Politics']    BBC  https://www.bbc.com/news/scotland/scotland_politics   \n",
              "3  ['Politics']    BBC       https://www.bbc.com/news/articles/cp89p1x26kxo   \n",
              "4  ['Politics']    BBC       https://www.bbc.com/news/articles/c9975z55dr5o   \n",
              "\n",
              "  section  \n",
              "0     NaN  \n",
              "1     NaN  \n",
              "2     NaN  \n",
              "3     NaN  \n",
              "4     NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8031635b-9316-4b75-8941-46ff004d6ef2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>source</th>\n",
              "      <th>url</th>\n",
              "      <th>section</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Starmer faces rebellion over plan to cut jury trials</td>\n",
              "      <td>Nearly 40 Labour MPs have warned the prime minister they are not prepared to support proposals to limit jury trials in England and Wales. In a letter to Sir Keir Starmer, MPs largely from the left of the party said restricting juries to major offences carrying three-year terms was \"madness and will cause more problems than it solves\". Former shadow attorney general Karl Turner, who organised the letter, said he will vote against Labour for the first time since Sir Keir took charge, branding the plans \"simply unworkable\". The government insisted it would go ahead with the plans, adding they...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/c93w771g14go</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UK to ban deepfake AI 'nudification' apps</td>\n",
              "      <td>The UK government says it will ban so-called \"nudification\" apps as part of efforts to tackle misogyny online. New laws - announced on Thursday as part of a wider strategy to halve violence against women and girls - will make it illegal to create and supply AI tools letting users edit images to seemingly remove someone's clothing. The new offences would build on existing rules around sexually explicit deepfakes and intimate image abuse, the government said. \"Women and girls deserve to be safe online as well as offline,\" said Technology Secretary Liz Kendall. \"We will not stand by while tec...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/cq8dp2y0z7wo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scotland Politics | Latest News &amp; Updates | BBC News</td>\n",
              "      <td>The Scottish parliament's presiding officer says the incident was part of an employment dispute involving an MSP. Their union BMA Scotland has accused the government of reneging on a commitment to restore pay to 2008 levels. Opposition party leaders grilll John Swinney in the final First Minister's Questions before Christmas Why did the people of Whitburn - and nearby Blackburn - choose Nigel Farage's party over others? The delivery date for MV Glen Rosa is pushed back by up to another six months while there is a further short delay with the Turkish-built Isle of Islay. The first minister ...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/scotland/scotland_politics</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Foreign prison officers exempted from new stricter visa rules</td>\n",
              "      <td>Foreign nationals working as prison officers in the UK have been given a temporary exemption from new visa rules, following warnings some jails were facing a staffing crisis. Prisons have increasingly been relying on overseas recruits, particularly from Nigeria and Ghana. But organisations representing officers had warned jails faced losing thousands of staff, after the government increased the minimum salary requirement for a skilled worker visa. The Ministry of Justice said the move would \"ensure jails can continue to run safely with the right level of experienced staff\". Under changes i...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/cp89p1x26kxo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Batters review: 'Bewildered' farmers need new deal to be profitable</td>\n",
              "      <td>Farmers are \"bewildered and frightened\" with many questioning the future of their businesses because of the government's proposed changes to inheritance tax, an independent review of farm profitability has found. The long-awaited government-commissioned report was published on Thursday with 57 recommendations designed to improve productivity, investment and resilience in agriculture. But author Baroness Minette Batters, former president of the National Farmers' Union (NFU), warned there was \"no silver bullet\" to making farms in England profitable. Environment secretary Emma Reynolds said t...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>BBC</td>\n",
              "      <td>https://www.bbc.com/news/articles/c9975z55dr5o</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8031635b-9316-4b75-8941-46ff004d6ef2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8031635b-9316-4b75-8941-46ff004d6ef2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8031635b-9316-4b75-8941-46ff004d6ef2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b8552609-3926-4cab-b2db-9c3039041e1a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8552609-3926-4cab-b2db-9c3039041e1a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b8552609-3926-4cab-b2db-9c3039041e1a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 95730,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95626,\n        \"samples\": [\n          \"Business is doing nicely, thank you, while workers get steadily poorer\",\n          \"Fresco fragment from Pompeii reopens row over \\u2018looted\\u2019 artefacts\",\n          \"Scheme to stop people being quizzed by abuser in court failing, lawyers say\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95730,\n        \"samples\": [\n          \"Access to BBC websites has been restricted in Russia, hours after the corporation brought back its shortwave radio service in Ukraine and Russia to ensure civilians in both countries can access news during the invasion. State communications watchdog Roskomnadzor restricted access to BBC Russia\\u2019s online presence, as well as Radio Liberty and the Meduza media outlet, the state-owned Russian RIA news agency reported on Friday. On Friday, Globalcheck, a service that tracks internet censorship in the countries of the Commonwealth of Independent States (CIS), reported that the availability of the entire BBC website was at 17% of normal levels in Russia, which suggested some services had been blocked. Later on Friday, the BBC shared methods to circumvent the ban, including the Pisphon app or the Tor browser. BBC Russia also reported that Meta, formerly known as Facebook, also appeared to be blocked, as was Google Play. The signs the BBC was being blocked emerged hours after the BBC\\u2019s decision to revert to a mostly obsolete form of broadcasting, broadcasting four hours of its world service, read in English, to Ukraine and parts of Russia each day. \\u201cIt\\u2019s often said truth is the first casualty of war,\\u201d BBC director general Tim Davie said in announcing the move on Thursday. \\u201cIn a conflict where disinformation and propaganda is rife, there is a clear need for factual and independent news people can trust \\u2026 millions more Russians are turning to the BBC.\\u201d The German public broadcaster Deutsche Welle also reported in Russian that the BBC site was not working in Russia. The BBC\\u2019s shortwave radio broadcast can be found on 15735 kHz from 4pm to 6pm and on 5875 kHz from 10pm to midnight, Ukraine time. The BBC\\u2019s move to bring back shortwave came days after Russia launched two missiles on Kyiv\\u2019s TV tower, killing five people and knocking out some access to news and broadcasts. Ukraine\\u2019s defence minister, Oleksii Reznikov, wrote on Twitter that the Kremlin was preparing to cut off communications and spread \\u201cmassive fake messages that the country\\u2019s leadership has given up\\u201d. Russia has clamped down on public dissent at home, while Kremlin-backed media organisations such as RT have been pulled internationally. The Kremlin has complained about the BBC\\u2019s coverage of the invasion, with Russia\\u2019s foreign ministry spokeswoman Maria Zakharova claiming without evidence that the BBC \\u201cplays a determined role in undermining the Russian stability and security\\u201d. Zakharova also claimed Russia had been the victim of \\u201cunprecedented information terrorism\\u201d that was \\u201ccreating hysteria around Ukrainian events\\u201d. The BBC has reported huge increases in its audiences in Ukraine and Russia since the invasion began. In the last week of February, viewership of BBC\\u2019s Ukrainian language website more than doubled from a year earlier to 3.9 million visitors. Its Russian-language website has reached a record 10.7 million visitors over the past week, more than tripling its weekly 2022 average. Shortwave radio uses frequencies that can travel long distances and are accessible on portable radios, making it the go-to method to reach listeners in conflict zones throughout history. Shortwave was used extensively in Europe to broadcast propaganda during the second world war, and usage peaked during the cold war. But as radio technology developed, along with the mass adoption of online news, shortwave fell out of favour around the world. After 76 years, the BBC World Service ended its shortwave broadcast to Europe in 2008. Press Association contributed to this report This article was updated on 5 March 2022, to correct the local times of the BBC broadcast.\",\n          \"The head of NatWest has apologised to Nigel Farage for what she called \\u201cdeeply inappropriate comments\\u201d about the former Ukip leader in an internal report that led to the closure of his accounts at the group\\u2019s exclusive private bank, Coutts. Dame Alison Rose issued a public statement on Thursday and wrote a letter to Farage apologising for the way the NatWest subsidiary had handled its decision to cut ties with the Brexit campaigner. It came after a series of interventions by the prime minister and senior members of his cabinet, and more than two weeks after Farage first disclosed that his accounts had been closed by Coutts because it no longer wanted him as a client. On Thursday night, Farage welcomed the apology from Rose but said it was only \\u201ca start\\u201d and claimed she had been forced into it by the Treasury. An internal report had described Farage as a \\u201cdisingenuous grifter\\u201d who promotes \\u201cxenophobic, chauvinistic and racist views\\u201d. After obtaining a copy and releasing it to the press, Farage had described the dossier as a \\u201cprejudiced, nasty document\\u201d. Rose said the comments, prepared for Coutts\\u2019 wealth reputation risk committee, \\u201cdo not reflect the view of the bank\\u201d. She added: \\u201cNo individual should have to read such comments and I apologise to Mr Farage for this.\\u201d However, she stopped short of reinstating Farage as a Coutts client, instead reiterating an offer to open a basic account for him at NatWest. It follows days of mounting public and political pressure, with Rishi Sunak intervening to warn it \\u201cwouldn\\u2019t be right if financial services were being denied to anyone exercising their right to lawful free speech\\u201d. The home secretary, Suella Braverman, called the decision \\u201csinister\\u201d. The government announced on Thursday that it would force banks to \\u201cexplain and delay\\u201d any decision to close accounts. Andrew Griffith, economic secretary to the Treasury, said \\u201cbanks occupy a privileged place in society\\u201d and must allow \\u201ceveryone to express themselves freely\\u201d without fear of losing their access to banking. It also ordered the Financial Conduct Authority to begin a review of the rules governing how banks treat \\u201cPolitically Exposed Persons\\u201d (Peps) such as Farage over the next two months. Lasting up to a year, the review will examine whether to ease the tough rules inherited from the EU on domestic Peps while maintaining them on foreigners with bank accounts in the UK, in line with international standards. Before Farage made the dossier public, reports had cited sources at Coutts saying that the decision to \\u201cunbank\\u201d Farage was purely because his account had fallen below its \\u201cwealth limit\\u201d, which requires customers to borrow or invest at least \\u00a31m or hold \\u00a33m in savings. The former politician fell below this threshold after his mortgage expired this year. However, Coutts\\u2019 reputation committee report also said Farage posed a risk, accusing him of being \\u201cseen as xenophobic and racist\\u201d and of making remarks that are \\u201cdistasteful and appear increasingly out of touch with wider society\\u201d. On his GB News programme on Thursday night Farage accused Rose of being forced into an apology by the Treasury. He said: \\u201cIn life it is always good to get an apology, so thank you Dame Alison for apologising. What I\\u2019ve actually been told quietly, privately, is that you were forced into doing this by the Treasury. \\u201cBut at least you\\u2019ve done it, I suppose. But the whole letter smacks of \\u2018not me, guv, I mean I\\u2019m just the chief executive, I mean, don\\u2019t blame me for what the banks under my direct control are doing\\u2019.\\u201d He said there were thousands in the same position as him, as he vowed to battle on. \\u201cI\\u2019m afraid I can\\u2019t just walk away from this. I\\u2019ve started this, and I\\u2019ve got to continue. So thank you for the apology. It\\u2019s a start, but it\\u2019s no more than that.\\u201d Rose had said it was not the bank\\u2019s policy \\u201cto exit a customer on the basis of legally held political and personal views\\u201d. \\u201cBoth freedom of expression and access to banking are fundamental to our society,\\u201d she said. \\u201cDecisions to close an account are not taken lightly and involve a number of factors including commercial viability, reputational considerations, and legal and regulatory requirements.\\u201d Announcing the new account closure rules, which will come into effect later this summer and are likely to be among the first post-Brexit changes to banking rules, Griffith said: \\u201cFreedom of speech is a cornerstone of our democracy, and it must be respected by all institutions. \\u201cThese changes will boost the rights of customers \\u2013 providing real transparency, time to appeal and making it a much fairer playing field.\\u201d The changes will increase the notice period for closing accounts to 90 days \\u2013 giving customers more time to challenge a decision through the Financial Ombudsman Service. Banks will also be required to spell out why they are terminating an account \\u2013 increasing transparency for customers and aiding their efforts to overturn decisions. The Treasury said the changes could only be made due to new powers in the Financial Services and Markets Act 2023. The proposed new rules follow a call for evidence launched in January, after PayPal\\u2019s temporary suspension last year of several accounts, including that of Toby Young, the social commentator and director of the Free Speech Union. The Treasury said the changes were needed to \\u201censure the right balance is being struck between protecting customers, and providers\\u2019 rights to manage commercial risk\\u201d.\",\n          \"There were negatives, of course. Shall we focus on the negatives? Shall we dwell on the frailties a little? The uncharacteristic errors, the double faults, an occasional scruffiness at the net, the frequent slumps in intensity? Shall we marvel at the fact that the lowest-ranked player in the tournament earned more break points (11) than one of the greatest players of his generation (10)? Shall we warn, in a tone of affected sternness, that the defending champion will have to raise his game on this evidence? Of course we shall, because this is Carlos Alcaraz, and because there is an entire cottage industry built around maintaining the idea that Alcaraz is in a state of crisis at all times, a state of crisis so acute that it is necessary to feign round-the-clock concern for him. We just want to see all that rich talent fulfilled. That\\u2019s all it is. Sincerely and genuinely. And definitely not a weirdly prurient interest in his holidays to Ibiza, or whether him and Emma Raducanu are, you know. Just the talent. Thinking of the talent here. And of course the illicit pleasure and enduring appeal of Alcaraz is that he so readily indulges these desires. He emotes. He misses a lot. He pulls off spectacular acrobatic winners immediately after missing a lot. He lives without inhibitions or regrets. Alcaraz is essentially a magic-eye puzzle you can read in whatever way you want, and after the sweat-drenched psychodrama of Fabio Fognini on Monday night there were more danger signs in his 6-1, 6-4, 6-4 win over Britain\\u2019s Oliver Tarvet; if you really, really wanted to see them. The more prosaic truth is that Alcaraz was playing himself as much as he was playing the world No 733 from St Albans. Tarvet is one of those classic British folk heroes the early rounds of Wimbledon always seem to throw up, complete with shaggy-dog backstory and tabloid headline-friendly name. And if Alcaraz was expecting an easy afternoon of Tarvet practice, he quickly discovered that it would be anything but easy shaking off this particular Tarvet from his back. Certainly Tarvet seemed to get an early read on the Alcaraz serve, used his speed and coverage to trade happily from the baseline, rode the early waves of noise from the home crowd. Some of his passes were sublime. Above all he looked untroubled, unfazed, hyper- confident, like a crypto-billionaire who had won a game against Carlos Alcaraz in a charity auction. \\u201cGood serve,\\u201d he called out at one point as his opponent pinned him with a vicious delivery to the body. Alcaraz shot him a look as if to say: yeah. Obviously it\\u2019s a good serve. I\\u2019m Carlos Alcaraz. Who are you again? But of course no read on Alcaraz\\u2019s serve is ever going to rival the read he has on yours. And though both men kept swinging, while Tarvet created break points and saved others, the only real jeopardy here was of the confected variety. Tarvet probably played the best match of his life, and in the end it was like bringing a sword to a sword fight when your opponent has about six far superior swords. There was a sadistic relish to the way Alcaraz kept teasing him with the drop shot, occasionally missing, mostly succeeding. But of course the drop shot, such a staple of the Alcaraz game, is also a stick to beat him with. Missing it costs one point, exactly the same as putting a forehand an inch long. And yet some misses are clearly more moral than others. For his detractors Alcaraz\\u2019s missed drop shots will always be taken in evidence against him, proof of his essential flimsiness. So, once again, we must deal with the principal charge. Alcaraz is inconsistent. It\\u2019s true, because everyone says so, to the point where it has basically passed into objective fact. Like the objective fact that Alcaraz has a win percentage of 90% so far this year, has won five of his six finals, is slowly putting together one of the greatest seasons in the modern era. His grass-court record stands comparison with the all-timers. This is the sort of inconsistency all but one of his rivals would dream of. But of course this is a stylistic as much as it is an empirical judgment. It is true that there is a big gulf between his highest and lowest levels. That he occasionally loses to people like Botic van de Zandschulp. That towards the end of last season and for a small portion of this he has looked a little unmoored, a little rattled. And yet how much of this reaction stems from a desire to see crisis, to armchair-analyse, to draw a straight line from his personal choices to his tennis as a way of justifying our interest in his personal choices? In a way, Alcaraz\\u2019s entire game serves as a kind of rejoinder to all this. This is, remember, still a player with just 34 tour games on grass, still adding levels and tones to his game, still learning how to master the mental side, still trying to work out exactly how famous he wants to be. In the meantime he\\u2019s going to keep going for the lines, keep going to Ibiza, keep trying the drop shot, keep embracing the chaos, because that\\u2019s the only way he knows. What if he misses, you scream. Fine. But what if he doesn\\u2019t?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 65,\n        \"samples\": [\n          \"['Energy']\",\n          \"['Climate', 'Health']\",\n          \"['Politics']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Guardian\",\n          \"BBC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95729,\n        \"samples\": [\n          \"https://www.theguardian.com/society/2025/apr/11/gamechanging-breast-cancer-pill-to-be-offered-on-nhs-in-england-and-wales\",\n          \"https://www.theguardian.com/politics/2023/jul/20/with-all-his-plans-in-tatters-rishi-eyes-some-reshuffle-lolz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"law\",\n          \"media\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2N3pvQZyWVO",
        "outputId": "9ff18610-9281-4047-a6e0-5bebe58bfb19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92330, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = df.dropna().reset_index(drop=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbcW-fUlyK02",
        "outputId": "903caf58-04cb-4be0-b3c3-ea4c40e9d061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 12\n",
            "{'Politics': 50623, 'Technology': 32667, 'Business': 19669, 'Economy': 12410, 'Sports': 11, 'International_Affairs': 10040, 'Education': 5938, 'Health': 38, 'Energy': 34, 'Crime': 10, 'Science': 4927, 'Climate': 9972}\n",
            "Threshold (1%): 923\n",
            "Number of rare labels: 4\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Prepare to count\n",
        "label_count = Counter()\n",
        "\n",
        "# 2. Iterate safely\n",
        "for label_entry in df['labels']:\n",
        "    # Convert string \"[...]\" to actual list safely\n",
        "    if isinstance(label_entry, str):\n",
        "        try:\n",
        "            # We use a different variable name (current_labels) to avoid overwriting\n",
        "            current_labels = ast.literal_eval(label_entry)\n",
        "        except:\n",
        "            continue\n",
        "    else:\n",
        "        current_labels = label_entry\n",
        "\n",
        "    # Update our counter with the list of labels in this row\n",
        "    if isinstance(current_labels, list):\n",
        "        label_count.update(current_labels)\n",
        "\n",
        "print(f\"Number of unique labels: {len(label_count)}\")\n",
        "print(dict(label_count))\n",
        "\n",
        "# 3. Calculate threshold and rare labels\n",
        "threshold = int(len(df) * 0.01)\n",
        "rare_labels = [key for key, value in label_count.items() if value < threshold]\n",
        "\n",
        "print(f\"Threshold (1%): {threshold}\")\n",
        "print(f\"Number of rare labels: {len(rare_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIdMhHkKOmq9",
        "outputId": "e2961ca7-ac9e-4660-faec-025f6a411f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original rows dropped: 36\n",
            "New shape: (92294, 7)\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "# 1. Convert to a list of rows\n",
        "raw_labels_list = df['labels'].to_list()\n",
        "revised_label_list = []\n",
        "indices_to_drop = []\n",
        "\n",
        "for idx, item in enumerate(raw_labels_list):\n",
        "    # Convert string \"[...]\" to actual list safely, or keep if already a list\n",
        "    if isinstance(item, str):\n",
        "        try:\n",
        "            current_row_labels = ast.literal_eval(item)\n",
        "        except:\n",
        "            current_row_labels = []\n",
        "    else:\n",
        "        current_row_labels = item\n",
        "\n",
        "    # Filter the labels\n",
        "    # Use a different name (l) to avoid shadowing the list\n",
        "    revised_row = [l for l in current_row_labels if l not in rare_labels]\n",
        "\n",
        "    # Check if any labels are left\n",
        "    if len(revised_row) == 0:\n",
        "        indices_to_drop.append(idx)\n",
        "    else:\n",
        "        revised_label_list.append(revised_row)\n",
        "\n",
        "# 2. Update the DataFrame\n",
        "df = df.drop(indices_to_drop).reset_index(drop=True)\n",
        "df['revised_labels'] = revised_label_list\n",
        "\n",
        "print(f\"Original rows dropped: {len(indices_to_drop)}\")\n",
        "print(f\"New shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxYdpV_GydOR",
        "outputId": "874e5b01-29a1-46fe-d11b-61d1039efc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 8\n"
          ]
        }
      ],
      "source": [
        "revised_label_list = df.revised_labels.to_list()\n",
        "revised_label_count = {}\n",
        "for labels in revised_label_list:\n",
        "  label_list = labels\n",
        "  for label in label_list:\n",
        "    if label in revised_label_count.keys():\n",
        "      revised_label_count[label] += 1\n",
        "    else:\n",
        "      revised_label_count[label] = 1\n",
        "print(f\"Number of labels: {len(revised_label_count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulIjb5nBjhPx"
      },
      "outputs": [],
      "source": [
        "encode_label_types = { key: idx for idx, (key, value) in enumerate(revised_label_count.items())}\n",
        "with open(\"label_types_encoded-v2.json\", \"w\") as fp:\n",
        "  json.dump(encode_label_types, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD4a5qEyP-9w",
        "outputId": "cb985e2e-8132-40cb-ac59-53b3e6d37678"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,\n",
              " ['Politics',\n",
              "  'Technology',\n",
              "  'Business',\n",
              "  'Economy',\n",
              "  'International_Affairs',\n",
              "  'Education',\n",
              "  'Science',\n",
              "  'Climate'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "labels = list(encode_label_types.keys())\n",
        "len(labels), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jUT9E7bhQZdk",
        "outputId": "c7df5094-f9cb-40b0-ddda-80ebb4276d85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                           title  \\\n",
              "0                                ‘It’s rather rude’: Truss accused of trying to poach members of rival Tory club   \n",
              "1  UK politics: ‘Not clear’ who was behind FCDO hack, says minister, amid reports of China link – as it happened   \n",
              "2                                         UK Foreign Office victim of cyber-attack in October, says Chris Bryant   \n",
              "3                                    Society of Editors decries Starmer’s plan to reduce media scrutiny of No 10   \n",
              "4                                    Reform-run Kent council accused of blocking scrutiny of claim it saved £40m   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  For Tory grandees licking their wounds and plotting their return after their disastrous 2024 general election performance, the opulent, fire-lit rooms of the exclusive club 5 Hertford Street are a sanctuary. But in recent weeks, their long lunches have been rudely interrupted by Liz Truss, who has been accused of wandering the premises in search of members to poach for her own rival operation, just one street away, which asks “founding members” for an eye-watering £500,000. The former prime minister’s alleged headhunting is understood to have irritated those who run the Mayfair club, inclu...   \n",
              "1  We are closing this blog soon. For the latest in UK news, follow our coverage here. The UK’s Foreign, Commonwealth and Development Office was hacked in October. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacker group was behind the cyber-attack. The Sun named Storm 1849 as the Chinese cyber gang responsible for the breach, which it said was understood to possibly include tens of thousands of visa details. However, when asked if China was behind the attack, trade minister Chris Bryant told broadcasters it was “not clear” who perpetrated the attack an...   \n",
              "2  The UK’s Foreign, Commonwealth and Development Office was hacked in October, a minister has said. Chris Bryant, a trade minister in Keir Starmer’s government, told Sky News there was a low risk to “any individual” from the cyber-attack. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacking group was behind it. But Bryant told broadcasters it was “not clear” who perpetrated the attack and cautioned against speculation. “There certainly has been a hack at the FCDO and we’ve been aware of that since October,” Bryant told Sky News. The Sun named Storm 1849...   \n",
              "3  The Society of Editors has raised concerns about Keir Starmer’s plan to reduce scrutiny of No 10 by political journalists, saying it risks weakening transparency. The body, which represents news organisations, said regular, open and robust questioning was a cornerstone of democracy and that the plan to reduce briefings was deeply concerning. Downing Street’s director of communications, Tim Allan, unveiled the plan on Thursday without consulting the group of political journalists known as the lobby who traditionally attend briefings twice a day to question the prime minister’s spokesperson....   \n",
              "4  Reform-run Kent council has been accused of trying to block scrutiny after it refused, for more than five months, to produce evidence that it had saved more than £40m by cancelling two environmental projects that did not exist yet. Polly Billington, a Labour MP in Kent, first requested background to the claim via a freedom of information (FoI) request in July. She said the subsequent delay had not been explained and seemed to show the council was embarrassed at what the documents would show. Kent county council said it rejected any suggestion of a cover-up, and that it planned to release t...   \n",
              "\n",
              "         labels    source  \\\n",
              "0  ['Politics']  Guardian   \n",
              "1  ['Politics']  Guardian   \n",
              "2  ['Politics']  Guardian   \n",
              "3  ['Politics']  Guardian   \n",
              "4  ['Politics']  Guardian   \n",
              "\n",
              "                                                                                                                                 url  \\\n",
              "0                           https://www.theguardian.com/politics/2025/dec/19/tory-members-club-accuses-liz-truss-of-poaching-tactics   \n",
              "1  https://www.theguardian.com/politics/live/2025/dec/19/labour-reform-conservatives-tories-lib-dems-uk-politics-latest-news-updates   \n",
              "2                                  https://www.theguardian.com/technology/2025/dec/19/uk-foreign-office-victim-cyber-attack-october-   \n",
              "3              https://www.theguardian.com/politics/2025/dec/19/society-of-editors-starmer-plan-reduce-lobby-briefing-media-scrutiny   \n",
              "4                 https://www.theguardian.com/politics/2025/dec/19/reform-run-kent-council-accused-blocking-scrutiny-claim-saved-40m   \n",
              "\n",
              "    section revised_labels  \n",
              "0  politics     [Politics]  \n",
              "1  politics     [Politics]  \n",
              "2  politics     [Politics]  \n",
              "3  politics     [Politics]  \n",
              "4  politics     [Politics]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab559a4e-a219-42fc-97aa-cfd6a6bff964\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>source</th>\n",
              "      <th>url</th>\n",
              "      <th>section</th>\n",
              "      <th>revised_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>‘It’s rather rude’: Truss accused of trying to poach members of rival Tory club</td>\n",
              "      <td>For Tory grandees licking their wounds and plotting their return after their disastrous 2024 general election performance, the opulent, fire-lit rooms of the exclusive club 5 Hertford Street are a sanctuary. But in recent weeks, their long lunches have been rudely interrupted by Liz Truss, who has been accused of wandering the premises in search of members to poach for her own rival operation, just one street away, which asks “founding members” for an eye-watering £500,000. The former prime minister’s alleged headhunting is understood to have irritated those who run the Mayfair club, inclu...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>Guardian</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/tory-members-club-accuses-liz-truss-of-poaching-tactics</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UK politics: ‘Not clear’ who was behind FCDO hack, says minister, amid reports of China link – as it happened</td>\n",
              "      <td>We are closing this blog soon. For the latest in UK news, follow our coverage here. The UK’s Foreign, Commonwealth and Development Office was hacked in October. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacker group was behind the cyber-attack. The Sun named Storm 1849 as the Chinese cyber gang responsible for the breach, which it said was understood to possibly include tens of thousands of visa details. However, when asked if China was behind the attack, trade minister Chris Bryant told broadcasters it was “not clear” who perpetrated the attack an...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>Guardian</td>\n",
              "      <td>https://www.theguardian.com/politics/live/2025/dec/19/labour-reform-conservatives-tories-lib-dems-uk-politics-latest-news-updates</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UK Foreign Office victim of cyber-attack in October, says Chris Bryant</td>\n",
              "      <td>The UK’s Foreign, Commonwealth and Development Office was hacked in October, a minister has said. Chris Bryant, a trade minister in Keir Starmer’s government, told Sky News there was a low risk to “any individual” from the cyber-attack. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacking group was behind it. But Bryant told broadcasters it was “not clear” who perpetrated the attack and cautioned against speculation. “There certainly has been a hack at the FCDO and we’ve been aware of that since October,” Bryant told Sky News. The Sun named Storm 1849...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>Guardian</td>\n",
              "      <td>https://www.theguardian.com/technology/2025/dec/19/uk-foreign-office-victim-cyber-attack-october-</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Society of Editors decries Starmer’s plan to reduce media scrutiny of No 10</td>\n",
              "      <td>The Society of Editors has raised concerns about Keir Starmer’s plan to reduce scrutiny of No 10 by political journalists, saying it risks weakening transparency. The body, which represents news organisations, said regular, open and robust questioning was a cornerstone of democracy and that the plan to reduce briefings was deeply concerning. Downing Street’s director of communications, Tim Allan, unveiled the plan on Thursday without consulting the group of political journalists known as the lobby who traditionally attend briefings twice a day to question the prime minister’s spokesperson....</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>Guardian</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/society-of-editors-starmer-plan-reduce-lobby-briefing-media-scrutiny</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reform-run Kent council accused of blocking scrutiny of claim it saved £40m</td>\n",
              "      <td>Reform-run Kent council has been accused of trying to block scrutiny after it refused, for more than five months, to produce evidence that it had saved more than £40m by cancelling two environmental projects that did not exist yet. Polly Billington, a Labour MP in Kent, first requested background to the claim via a freedom of information (FoI) request in July. She said the subsequent delay had not been explained and seemed to show the council was embarrassed at what the documents would show. Kent county council said it rejected any suggestion of a cover-up, and that it planned to release t...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>Guardian</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/reform-run-kent-council-accused-blocking-scrutiny-claim-saved-40m</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab559a4e-a219-42fc-97aa-cfd6a6bff964')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab559a4e-a219-42fc-97aa-cfd6a6bff964 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab559a4e-a219-42fc-97aa-cfd6a6bff964');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-83fcfc06-d342-492d-a326-3109d725ce98\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83fcfc06-d342-492d-a326-3109d725ce98')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-83fcfc06-d342-492d-a326-3109d725ce98 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 92294,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92193,\n        \"samples\": [\n          \"Share your views on Matt Hancock\\u2019s I\\u2019m a Celebrity\\u2026 appearance\",\n          \"A piranha: it is boiling the water you\\u2019re swimming in and taking bites out of you\",\n          \"Breaking good: the yakuza gangster who became a lawyer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92294,\n        \"samples\": [\n          \"Three weeks ago, Australia arrived in Europe self-assured and quietly confident of taking a few prized scalps. And why not? They had come within a single refereeing call at the breakdown of claiming a British &amp; Irish Lions series win. They had hammered the world champion Springboks in Johannesburg. They had shown great chutzpah to beat Argentina after the hooter and they still carried the glow of last November\\u2019s win over England. This was a side developing shape and steel, a side capable of the sublime, a side beginning to coax long-dormant fans back to the code while tempting home several stars who had crossed to rugby league. This tour was supposed to confirm, unequivocally, that the Wallabies were back. Instead, they\\u2019ve gone backwards after a sorry performance against Ireland in Dublin where they received a 46\\u201319 shellacking that still managed to flatter them on the scoreboard. This result also confirms their status as a B-tier rugby team. No matter what happens against France in Paris next week, they will leave Europe ranked outside the top six on World Rugby\\u2019s charts, consigning them to the second pot when the 2027 World Cup draw takes place next month. The implications are stark: if the Wallabies want to reach the latter stages of their own tournament on home soil, they will have to knock over one of the giants, a task that, on current evidence, feels well beyond them. Too often in Dublin they looked like a side searching for someone else to take control: to claim the high ball, to marshal the defensive line, to calm a frantic moment, to dictate where the next five minutes should be played. Ireland didn\\u2019t overwhelm them physically or tactically. They simply leaned on the most obvious pressure points \\u2013 contestable kicks, structured phase play, tempo changes \\u2013 and trusted that the Wallabies would crack first. And they did. Australia coughed up possession from their own lineout on six occasions \\u2013 four times inside Ireland\\u2019s 22. Their ineptitude under the high ball has only worsened after similar struggles against England and Italy over the past fortnight. They\\u2019re playing like a team that has run out of energy, which is deeply concerning given they should be entering a new chapter as they gear up for that home World Cup in less than two years. What on earth are they going to do between now and then? First things first: they need to back someone, anyone, at fly-half. James O\\u2019Connor is a supremely gifted athlete. He looks the part too. His socks are down around his ankles, his angular jaw barks instructions to teammates as his sinewy arms wave about. But he is not the metronome this team requires. His inability to marshal the backline into something resembling a cohesive structure was telling. Not that this is O\\u2019Connor\\u2019s fault. In an ideal world, the 35-year-old who made his Test debut 17 years ago would have been watching this match like the rest of us. But Tane Edmed is far from the finished product, and Carter Gordon, while rediscovering his groove in the code, is sidelined with a neck injury. What Schmidt would give for a player like South Africa\\u2019s Handr\\u00e9 Pollard or England\\u2019s George Ford, two fly-halves who have copped their share of criticism for lacking razzle-dazzle, yet provide the one quality Australia are crying out for: control. Australia already have plenty of excitement and instinct and broken-field brilliance. What they\\u2019re missing is a first receiver who dictates the tempo of a Test match, who can put his studs on the ball and slow things down, who can take the sting out of the contest rather than add to the chaos. Under Schmidt, Australian rugby is steadier, but still too dependent on the sting. For Schmidt, this is a version of Jos\\u00e9 Mourinho\\u2019s undersized blanket problem: tug the defence into shape and the attack loses fluidity; focus on structure and the spark disappears; lean on spark and the control evaporates. Every time Australia cover one area, another is left exposed. The cruel irony is that this team\\u2019s ceiling remains genuinely high. But the floor keeps dropping out from beneath them. They don\\u2019t need to reinvent themselves, they need reliability, leadership and a spine that doesn\\u2019t buckle under pressure. The tour was supposed to be a statement of intent. Instead, it has served as a sobering reminder of how far behind the leading nations Australia still sit. Schmidt will speak of learnings and combinations and growth \\u2013 and some of that will be true \\u2013 but he knows better than anyone that this team requires more than incremental improvement. It needs identity, clarity and, crucially, a fly-half who can turn talent into Test-match shape. In the end it\\u2019s the clarity that will hurt most. Australia are not on the cusp of something; they are adrift from it. The raw materials are there, but the polish, the precision, the ruthlessness that defines the very best sides remains absent.\",\n          \"Former Philippines president Rodrigo Duterte has said he will accept responsibility for his government\\u2019s so-called \\u201cwar on drugs\\u201d in a video message filmed on board a plane shortly before he was taken into the custody of the international criminal court (ICC). \\u201cWhatever happened in the past, I will be the front of our law enforcement and the military. I said this already, that I will protect you, and I will be responsible for everything,\\u201d he said. The video message, which appeared to have been filmed on board the aircraft that brought him to the Netherlands to face charges of crimes against humanity, were his first comments to the Philippines public since his dramatic arrest on Tuesday. Dressed in a plain white shirt, and speaking to the camera, he said: \\u201cThis will be a long legal proceeding. But I say to you, I will continue to serve the country. So be it. If that is my destiny. Thank you\\u201d. Duterte has previously said he offered \\u201cno apologies, no excuses\\u201d for his bloody anti drugs crackdowns which activists say may have killed as many as 30,000 people. Duterte\\u2019s plane landed at Rotterdam airport at just before 5pm local time on Wednesday, and he was transferred to a detention unit on the Dutch coast. In a statement the ICC confirmed it had taken custody of the former leader, with the court\\u2019s chief prosecutor, Karim Khan, calling it \\u201ca crucial step in our continuous work to ensure accountability for the victims of the most serious crimes under ICC jurisdiction.\\u201d Duterte is the first former leader of an Asian country to be served an arrest warrant filed by the ICC. In a statement the ICC said its chamber, composed of three judges, had assessed material submitted by office of the prosecutor and found reasonable grounds to believe that Duterte is \\u201cindividually responsible as an indirect co-perpetrator for the crime against humanity of murder, allegedly committed in the Philippines between 1 November 2011 and 16 March 2019.\\u201d A hearing will be scheduled \\u201cin due course\\u201d for Duterte\\u2019s initial appearance before the court, it said, which will confirm his identity and the language in which he is able to follow the proceedings. It is not clear when a trial will begin. Supporters of the former leader gathered at The Hague Penitentiary Institution, waiving the Philippines flag and chanting \\u201cbring him back\\u201d. While rights experts and victims\\u2019 families have been overjoyed by the news of Duterte\\u2019s arrest, the former leader retains a strong support base, especially in the south of the country. \\u201cI am OK, do not worry,\\u201d Duterte, who will turn 80 this month, said in the video message. His daughter, Sara Duterte, the vice-president, also arrived in The Hague on Wednesday evening to offer support. Lawyers for Duterte on Wednesday filed a petition on behalf of his youngest daughter, Veronica, accusing the government of \\u201ckidnapping\\u201d him, and demanding he be returned to the Philippines. Duterte\\u2019s supporters have argued that, as the Philippines withdrew from the Rome statute in 2019, the ICC no longer has jurisdiction. However, the court has previously said it retains jurisdiction for alleged crimes that occurred in the country before its withdrawal. The country\\u2019s president, Ferdinand Marcos Jr, who once ran in an alliance with vice-president Sara Duterte but is now embroiled in a bitter feud with the family, told reporters this week he was confident \\u201cthe arrest was proper, correct and followed all necessary legal procedures\\u201d.\",\n          \"Schools are hiring handwriting specialists to tackle a drop-off in children\\u2019s pen skills caused by the use of laptops and tablets during the pandemic and a lack of opportunities for extended writing. When children return in September, many will spend extra time with the specialists, and also occupational therapists who will be employed specifically to help children who have fallen behind. \\u201cIt is very clear that the pandemic has had a huge impact on learning,\\u201d said Geoff Barton, general secretary of the Association of School and College Leaders. \\u201cThe latest evidence on this was key stage 2 SATs results, which showed that the percentage of pupils achieving the expected standard in writing, maths, grammar, punctuation and spelling was quite markedly down on 2019 before the pandemic began. In the case of writing, it was nearly 10% lower.\\u201d Lockdowns compelled millions of children to do schoolwork at home on laptops and tablets, typing instead of writing. Waves of staff and pupil absences when children were back at school caused further disruption and have taken their toll on handwriting skills. \\u201cThe sounding out of letters and the mapping of that sound to a movement on paper is really quite complex when you are young and learning, so that means explicit teaching,\\u201d said Dr Mellissa Prunty, a senior lecturer in occupational therapy at Brunel University London and chair of the National Handwriting Association. \\u201cChildren need to be taught how to write. They don\\u2019t just pick it up. It really needs to be practised, and the problem during the pandemic is that writing dropped off a cliff. \\u201d To try to tackle the problem, schools are using one-off government Covid catch-up premium funding or national tutoring funding. The latter will see \\u00a3349m going directly to schools in 2022-23. It will cover 75% of the cost of tuition, with schools having to find the other 25%. Mill Lane primary school in Stockton-on-Tees has used its catch-up premium to employ a specialist writing teacher in order to accelerate progress in writing. At King Athelstan school in Kingston, greater London, a specialist teacher has provided phonics intervention three times a week to recap sounds taught in reception and link this to handwriting. Writing support for children in years 5 and 6 at St Peter and St Paul Church of England primary, in Bexhill, Sussex, is being delivered by a specialist teacher working alongside classroom teachers to provide high-quality tuition in writing for classes and groups. Learning loss interventions at Grasmere primary school in Hackney also include addressing the deterioration in handwriting. A new scheme at Brunel University will see occupational therapy students going on placements to local primaries specifically to help children who have fallen behind in writing. \\u201cThe government has invested \\u00a34.9bn into supporting education recovery since 2020-21, which we very much welcome. However, we are not convinced that this is sufficient for the scale of the task,\\u201d said Barton.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"['Politics']\",\n          \"['International_Affairs', 'Politics']\",\n          \"['Science', 'Education']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Guardian\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92294,\n        \"samples\": [\n          \"https://www.theguardian.com/sport/2025/nov/16/the-wallabies-were-meant-to-prove-theyre-back-but-instead-they-have-gone-backwards\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"law\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"revised_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2fJ0FjsREe7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Count all occurrences to build the dictionary\n",
        "all_labels = [label for sublist in df.revised_labels for label in sublist]\n",
        "label_counts = Counter(all_labels)\n",
        "\n",
        "# Create the mapping: {\"Politics\": 0, \"Technology\": 1, ...}\n",
        "encode_label_types = { key: idx for idx, key in enumerate(label_counts.keys())}\n",
        "\n",
        "# Save it for later use in your model/inference\n",
        "with open(\"label_types_encoded.json\", \"w\") as fp:\n",
        "    json.dump(encode_label_types, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7sUvWHRFqc",
        "outputId": "62ea512f-246e-44dd-a62b-865c4e2d892a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (92294, 8)\n",
            "Encoded vector length: 8\n"
          ]
        }
      ],
      "source": [
        "categorical_label_list = []\n",
        "num_classes = len(encode_label_types)\n",
        "revised_labels_list = df.revised_labels.to_list()\n",
        "\n",
        "for row_labels in revised_labels_list:\n",
        "    # 1. Create a fresh list of zeros for THIS row\n",
        "    categorical_vector = [0] * num_classes\n",
        "\n",
        "    # 2. Fill with 1s where the label exists\n",
        "    for label in row_labels:\n",
        "        if label in encode_label_types:\n",
        "            index = encode_label_types[label]\n",
        "            categorical_vector[index] = 1\n",
        "\n",
        "    # 3. Add this row's vector to our master list\n",
        "    categorical_label_list.append(categorical_vector)\n",
        "\n",
        "# 4. Add the new column to your dataframe\n",
        "df['label_cat_list'] = categorical_label_list\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Encoded vector length: {len(df['label_cat_list'].iloc[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKXwzyUuSlKd",
        "outputId": "89d0534a-0846-40d2-aeb3-fe85c06f9802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns remaining: ['title', 'text', 'labels', 'url', 'section', 'revised_labels', 'label_cat_list']\n",
            "File saved successfully! Final shape: (92294, 7)\n"
          ]
        }
      ],
      "source": [
        "# 1. Delete the specified columns\n",
        "# We use 'errors=ignore' just in case the code is run twice\n",
        "df = df.drop(columns=['genre_cat_list', 'source'], errors='ignore')\n",
        "\n",
        "# 2. Save to CSV format\n",
        "# index=False prevents pandas from adding an extra column for the row numbers\n",
        "df.to_csv('cleaned_multilabel_data.csv', index=False)\n",
        "\n",
        "# 3. Verify the final structure\n",
        "print(\"Columns remaining:\", df.columns.tolist())\n",
        "print(f\"File saved successfully! Final shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BJah6OCb8R_x",
        "outputId": "2caddaca-e272-45d4-fb2f-85d4f11cd3b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                           title  \\\n",
              "0                                ‘It’s rather rude’: Truss accused of trying to poach members of rival Tory club   \n",
              "1  UK politics: ‘Not clear’ who was behind FCDO hack, says minister, amid reports of China link – as it happened   \n",
              "2                                         UK Foreign Office victim of cyber-attack in October, says Chris Bryant   \n",
              "3                                    Society of Editors decries Starmer’s plan to reduce media scrutiny of No 10   \n",
              "4                                    Reform-run Kent council accused of blocking scrutiny of claim it saved £40m   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  For Tory grandees licking their wounds and plotting their return after their disastrous 2024 general election performance, the opulent, fire-lit rooms of the exclusive club 5 Hertford Street are a sanctuary. But in recent weeks, their long lunches have been rudely interrupted by Liz Truss, who has been accused of wandering the premises in search of members to poach for her own rival operation, just one street away, which asks “founding members” for an eye-watering £500,000. The former prime minister’s alleged headhunting is understood to have irritated those who run the Mayfair club, inclu...   \n",
              "1  We are closing this blog soon. For the latest in UK news, follow our coverage here. The UK’s Foreign, Commonwealth and Development Office was hacked in October. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacker group was behind the cyber-attack. The Sun named Storm 1849 as the Chinese cyber gang responsible for the breach, which it said was understood to possibly include tens of thousands of visa details. However, when asked if China was behind the attack, trade minister Chris Bryant told broadcasters it was “not clear” who perpetrated the attack an...   \n",
              "2  The UK’s Foreign, Commonwealth and Development Office was hacked in October, a minister has said. Chris Bryant, a trade minister in Keir Starmer’s government, told Sky News there was a low risk to “any individual” from the cyber-attack. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacking group was behind it. But Bryant told broadcasters it was “not clear” who perpetrated the attack and cautioned against speculation. “There certainly has been a hack at the FCDO and we’ve been aware of that since October,” Bryant told Sky News. The Sun named Storm 1849...   \n",
              "3  The Society of Editors has raised concerns about Keir Starmer’s plan to reduce scrutiny of No 10 by political journalists, saying it risks weakening transparency. The body, which represents news organisations, said regular, open and robust questioning was a cornerstone of democracy and that the plan to reduce briefings was deeply concerning. Downing Street’s director of communications, Tim Allan, unveiled the plan on Thursday without consulting the group of political journalists known as the lobby who traditionally attend briefings twice a day to question the prime minister’s spokesperson....   \n",
              "4  Reform-run Kent council has been accused of trying to block scrutiny after it refused, for more than five months, to produce evidence that it had saved more than £40m by cancelling two environmental projects that did not exist yet. Polly Billington, a Labour MP in Kent, first requested background to the claim via a freedom of information (FoI) request in July. She said the subsequent delay had not been explained and seemed to show the council was embarrassed at what the documents would show. Kent county council said it rejected any suggestion of a cover-up, and that it planned to release t...   \n",
              "\n",
              "         labels  \\\n",
              "0  ['Politics']   \n",
              "1  ['Politics']   \n",
              "2  ['Politics']   \n",
              "3  ['Politics']   \n",
              "4  ['Politics']   \n",
              "\n",
              "                                                                                                                                 url  \\\n",
              "0                           https://www.theguardian.com/politics/2025/dec/19/tory-members-club-accuses-liz-truss-of-poaching-tactics   \n",
              "1  https://www.theguardian.com/politics/live/2025/dec/19/labour-reform-conservatives-tories-lib-dems-uk-politics-latest-news-updates   \n",
              "2                                  https://www.theguardian.com/technology/2025/dec/19/uk-foreign-office-victim-cyber-attack-october-   \n",
              "3              https://www.theguardian.com/politics/2025/dec/19/society-of-editors-starmer-plan-reduce-lobby-briefing-media-scrutiny   \n",
              "4                 https://www.theguardian.com/politics/2025/dec/19/reform-run-kent-council-accused-blocking-scrutiny-claim-saved-40m   \n",
              "\n",
              "    section revised_labels            label_cat_list  \n",
              "0  politics     [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
              "1  politics     [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
              "2  politics     [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
              "3  politics     [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
              "4  politics     [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48c27d73-1ea0-49d8-8e7d-33f5773c68be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>url</th>\n",
              "      <th>section</th>\n",
              "      <th>revised_labels</th>\n",
              "      <th>label_cat_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>‘It’s rather rude’: Truss accused of trying to poach members of rival Tory club</td>\n",
              "      <td>For Tory grandees licking their wounds and plotting their return after their disastrous 2024 general election performance, the opulent, fire-lit rooms of the exclusive club 5 Hertford Street are a sanctuary. But in recent weeks, their long lunches have been rudely interrupted by Liz Truss, who has been accused of wandering the premises in search of members to poach for her own rival operation, just one street away, which asks “founding members” for an eye-watering £500,000. The former prime minister’s alleged headhunting is understood to have irritated those who run the Mayfair club, inclu...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/tory-members-club-accuses-liz-truss-of-poaching-tactics</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UK politics: ‘Not clear’ who was behind FCDO hack, says minister, amid reports of China link – as it happened</td>\n",
              "      <td>We are closing this blog soon. For the latest in UK news, follow our coverage here. The UK’s Foreign, Commonwealth and Development Office was hacked in October. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacker group was behind the cyber-attack. The Sun named Storm 1849 as the Chinese cyber gang responsible for the breach, which it said was understood to possibly include tens of thousands of visa details. However, when asked if China was behind the attack, trade minister Chris Bryant told broadcasters it was “not clear” who perpetrated the attack an...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/politics/live/2025/dec/19/labour-reform-conservatives-tories-lib-dems-uk-politics-latest-news-updates</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UK Foreign Office victim of cyber-attack in October, says Chris Bryant</td>\n",
              "      <td>The UK’s Foreign, Commonwealth and Development Office was hacked in October, a minister has said. Chris Bryant, a trade minister in Keir Starmer’s government, told Sky News there was a low risk to “any individual” from the cyber-attack. Details of the hack emerged on Friday in a report by the Sun that claimed a Chinese hacking group was behind it. But Bryant told broadcasters it was “not clear” who perpetrated the attack and cautioned against speculation. “There certainly has been a hack at the FCDO and we’ve been aware of that since October,” Bryant told Sky News. The Sun named Storm 1849...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/technology/2025/dec/19/uk-foreign-office-victim-cyber-attack-october-</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Society of Editors decries Starmer’s plan to reduce media scrutiny of No 10</td>\n",
              "      <td>The Society of Editors has raised concerns about Keir Starmer’s plan to reduce scrutiny of No 10 by political journalists, saying it risks weakening transparency. The body, which represents news organisations, said regular, open and robust questioning was a cornerstone of democracy and that the plan to reduce briefings was deeply concerning. Downing Street’s director of communications, Tim Allan, unveiled the plan on Thursday without consulting the group of political journalists known as the lobby who traditionally attend briefings twice a day to question the prime minister’s spokesperson....</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/society-of-editors-starmer-plan-reduce-lobby-briefing-media-scrutiny</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reform-run Kent council accused of blocking scrutiny of claim it saved £40m</td>\n",
              "      <td>Reform-run Kent council has been accused of trying to block scrutiny after it refused, for more than five months, to produce evidence that it had saved more than £40m by cancelling two environmental projects that did not exist yet. Polly Billington, a Labour MP in Kent, first requested background to the claim via a freedom of information (FoI) request in July. She said the subsequent delay had not been explained and seemed to show the council was embarrassed at what the documents would show. Kent county council said it rejected any suggestion of a cover-up, and that it planned to release t...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/politics/2025/dec/19/reform-run-kent-council-accused-blocking-scrutiny-claim-saved-40m</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48c27d73-1ea0-49d8-8e7d-33f5773c68be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48c27d73-1ea0-49d8-8e7d-33f5773c68be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48c27d73-1ea0-49d8-8e7d-33f5773c68be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-630f7808-8659-45ba-9aad-56a9b0077cbb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-630f7808-8659-45ba-9aad-56a9b0077cbb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-630f7808-8659-45ba-9aad-56a9b0077cbb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 92294,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92193,\n        \"samples\": [\n          \"Share your views on Matt Hancock\\u2019s I\\u2019m a Celebrity\\u2026 appearance\",\n          \"A piranha: it is boiling the water you\\u2019re swimming in and taking bites out of you\",\n          \"Breaking good: the yakuza gangster who became a lawyer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92294,\n        \"samples\": [\n          \"Three weeks ago, Australia arrived in Europe self-assured and quietly confident of taking a few prized scalps. And why not? They had come within a single refereeing call at the breakdown of claiming a British &amp; Irish Lions series win. They had hammered the world champion Springboks in Johannesburg. They had shown great chutzpah to beat Argentina after the hooter and they still carried the glow of last November\\u2019s win over England. This was a side developing shape and steel, a side capable of the sublime, a side beginning to coax long-dormant fans back to the code while tempting home several stars who had crossed to rugby league. This tour was supposed to confirm, unequivocally, that the Wallabies were back. Instead, they\\u2019ve gone backwards after a sorry performance against Ireland in Dublin where they received a 46\\u201319 shellacking that still managed to flatter them on the scoreboard. This result also confirms their status as a B-tier rugby team. No matter what happens against France in Paris next week, they will leave Europe ranked outside the top six on World Rugby\\u2019s charts, consigning them to the second pot when the 2027 World Cup draw takes place next month. The implications are stark: if the Wallabies want to reach the latter stages of their own tournament on home soil, they will have to knock over one of the giants, a task that, on current evidence, feels well beyond them. Too often in Dublin they looked like a side searching for someone else to take control: to claim the high ball, to marshal the defensive line, to calm a frantic moment, to dictate where the next five minutes should be played. Ireland didn\\u2019t overwhelm them physically or tactically. They simply leaned on the most obvious pressure points \\u2013 contestable kicks, structured phase play, tempo changes \\u2013 and trusted that the Wallabies would crack first. And they did. Australia coughed up possession from their own lineout on six occasions \\u2013 four times inside Ireland\\u2019s 22. Their ineptitude under the high ball has only worsened after similar struggles against England and Italy over the past fortnight. They\\u2019re playing like a team that has run out of energy, which is deeply concerning given they should be entering a new chapter as they gear up for that home World Cup in less than two years. What on earth are they going to do between now and then? First things first: they need to back someone, anyone, at fly-half. James O\\u2019Connor is a supremely gifted athlete. He looks the part too. His socks are down around his ankles, his angular jaw barks instructions to teammates as his sinewy arms wave about. But he is not the metronome this team requires. His inability to marshal the backline into something resembling a cohesive structure was telling. Not that this is O\\u2019Connor\\u2019s fault. In an ideal world, the 35-year-old who made his Test debut 17 years ago would have been watching this match like the rest of us. But Tane Edmed is far from the finished product, and Carter Gordon, while rediscovering his groove in the code, is sidelined with a neck injury. What Schmidt would give for a player like South Africa\\u2019s Handr\\u00e9 Pollard or England\\u2019s George Ford, two fly-halves who have copped their share of criticism for lacking razzle-dazzle, yet provide the one quality Australia are crying out for: control. Australia already have plenty of excitement and instinct and broken-field brilliance. What they\\u2019re missing is a first receiver who dictates the tempo of a Test match, who can put his studs on the ball and slow things down, who can take the sting out of the contest rather than add to the chaos. Under Schmidt, Australian rugby is steadier, but still too dependent on the sting. For Schmidt, this is a version of Jos\\u00e9 Mourinho\\u2019s undersized blanket problem: tug the defence into shape and the attack loses fluidity; focus on structure and the spark disappears; lean on spark and the control evaporates. Every time Australia cover one area, another is left exposed. The cruel irony is that this team\\u2019s ceiling remains genuinely high. But the floor keeps dropping out from beneath them. They don\\u2019t need to reinvent themselves, they need reliability, leadership and a spine that doesn\\u2019t buckle under pressure. The tour was supposed to be a statement of intent. Instead, it has served as a sobering reminder of how far behind the leading nations Australia still sit. Schmidt will speak of learnings and combinations and growth \\u2013 and some of that will be true \\u2013 but he knows better than anyone that this team requires more than incremental improvement. It needs identity, clarity and, crucially, a fly-half who can turn talent into Test-match shape. In the end it\\u2019s the clarity that will hurt most. Australia are not on the cusp of something; they are adrift from it. The raw materials are there, but the polish, the precision, the ruthlessness that defines the very best sides remains absent.\",\n          \"Former Philippines president Rodrigo Duterte has said he will accept responsibility for his government\\u2019s so-called \\u201cwar on drugs\\u201d in a video message filmed on board a plane shortly before he was taken into the custody of the international criminal court (ICC). \\u201cWhatever happened in the past, I will be the front of our law enforcement and the military. I said this already, that I will protect you, and I will be responsible for everything,\\u201d he said. The video message, which appeared to have been filmed on board the aircraft that brought him to the Netherlands to face charges of crimes against humanity, were his first comments to the Philippines public since his dramatic arrest on Tuesday. Dressed in a plain white shirt, and speaking to the camera, he said: \\u201cThis will be a long legal proceeding. But I say to you, I will continue to serve the country. So be it. If that is my destiny. Thank you\\u201d. Duterte has previously said he offered \\u201cno apologies, no excuses\\u201d for his bloody anti drugs crackdowns which activists say may have killed as many as 30,000 people. Duterte\\u2019s plane landed at Rotterdam airport at just before 5pm local time on Wednesday, and he was transferred to a detention unit on the Dutch coast. In a statement the ICC confirmed it had taken custody of the former leader, with the court\\u2019s chief prosecutor, Karim Khan, calling it \\u201ca crucial step in our continuous work to ensure accountability for the victims of the most serious crimes under ICC jurisdiction.\\u201d Duterte is the first former leader of an Asian country to be served an arrest warrant filed by the ICC. In a statement the ICC said its chamber, composed of three judges, had assessed material submitted by office of the prosecutor and found reasonable grounds to believe that Duterte is \\u201cindividually responsible as an indirect co-perpetrator for the crime against humanity of murder, allegedly committed in the Philippines between 1 November 2011 and 16 March 2019.\\u201d A hearing will be scheduled \\u201cin due course\\u201d for Duterte\\u2019s initial appearance before the court, it said, which will confirm his identity and the language in which he is able to follow the proceedings. It is not clear when a trial will begin. Supporters of the former leader gathered at The Hague Penitentiary Institution, waiving the Philippines flag and chanting \\u201cbring him back\\u201d. While rights experts and victims\\u2019 families have been overjoyed by the news of Duterte\\u2019s arrest, the former leader retains a strong support base, especially in the south of the country. \\u201cI am OK, do not worry,\\u201d Duterte, who will turn 80 this month, said in the video message. His daughter, Sara Duterte, the vice-president, also arrived in The Hague on Wednesday evening to offer support. Lawyers for Duterte on Wednesday filed a petition on behalf of his youngest daughter, Veronica, accusing the government of \\u201ckidnapping\\u201d him, and demanding he be returned to the Philippines. Duterte\\u2019s supporters have argued that, as the Philippines withdrew from the Rome statute in 2019, the ICC no longer has jurisdiction. However, the court has previously said it retains jurisdiction for alleged crimes that occurred in the country before its withdrawal. The country\\u2019s president, Ferdinand Marcos Jr, who once ran in an alliance with vice-president Sara Duterte but is now embroiled in a bitter feud with the family, told reporters this week he was confident \\u201cthe arrest was proper, correct and followed all necessary legal procedures\\u201d.\",\n          \"Schools are hiring handwriting specialists to tackle a drop-off in children\\u2019s pen skills caused by the use of laptops and tablets during the pandemic and a lack of opportunities for extended writing. When children return in September, many will spend extra time with the specialists, and also occupational therapists who will be employed specifically to help children who have fallen behind. \\u201cIt is very clear that the pandemic has had a huge impact on learning,\\u201d said Geoff Barton, general secretary of the Association of School and College Leaders. \\u201cThe latest evidence on this was key stage 2 SATs results, which showed that the percentage of pupils achieving the expected standard in writing, maths, grammar, punctuation and spelling was quite markedly down on 2019 before the pandemic began. In the case of writing, it was nearly 10% lower.\\u201d Lockdowns compelled millions of children to do schoolwork at home on laptops and tablets, typing instead of writing. Waves of staff and pupil absences when children were back at school caused further disruption and have taken their toll on handwriting skills. \\u201cThe sounding out of letters and the mapping of that sound to a movement on paper is really quite complex when you are young and learning, so that means explicit teaching,\\u201d said Dr Mellissa Prunty, a senior lecturer in occupational therapy at Brunel University London and chair of the National Handwriting Association. \\u201cChildren need to be taught how to write. They don\\u2019t just pick it up. It really needs to be practised, and the problem during the pandemic is that writing dropped off a cliff. \\u201d To try to tackle the problem, schools are using one-off government Covid catch-up premium funding or national tutoring funding. The latter will see \\u00a3349m going directly to schools in 2022-23. It will cover 75% of the cost of tuition, with schools having to find the other 25%. Mill Lane primary school in Stockton-on-Tees has used its catch-up premium to employ a specialist writing teacher in order to accelerate progress in writing. At King Athelstan school in Kingston, greater London, a specialist teacher has provided phonics intervention three times a week to recap sounds taught in reception and link this to handwriting. Writing support for children in years 5 and 6 at St Peter and St Paul Church of England primary, in Bexhill, Sussex, is being delivered by a specialist teacher working alongside classroom teachers to provide high-quality tuition in writing for classes and groups. Learning loss interventions at Grasmere primary school in Hackney also include addressing the deterioration in handwriting. A new scheme at Brunel University will see occupational therapy students going on placements to local primaries specifically to help children who have fallen behind in writing. \\u201cThe government has invested \\u00a34.9bn into supporting education recovery since 2020-21, which we very much welcome. However, we are not convinced that this is sufficient for the scale of the task,\\u201d said Barton.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"['Politics']\",\n          \"['International_Affairs', 'Politics']\",\n          \"['Science', 'Education']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 92294,\n        \"samples\": [\n          \"https://www.theguardian.com/sport/2025/nov/16/the-wallabies-were-meant-to-prove-theyre-back-but-instead-they-have-gone-backwards\",\n          \"https://www.theguardian.com/world/2025/mar/13/rodrigo-duterte-former-philippines-president-video-message-icc-arrest-over-war-on-drugs-ntwnfb\",\n          \"https://www.theguardian.com/education/2022/jul/23/writing-has-dropped-off-a-cliff-englands-lockdown-hit-pupils-get-extra-pen-lessons\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"law\",\n          \"media\",\n          \"politics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"revised_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_cat_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emcfDKz2yfQg",
        "outputId": "3b8e579b-80bc-4fdf-ea68-b99e754b3c36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92294, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqozxSKSymTR",
        "outputId": "f83f23b8-be5b-4c17-8105-3a1d5627574d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,\n",
              " ['Politics',\n",
              "  'Technology',\n",
              "  'Business',\n",
              "  'Economy',\n",
              "  'International_Affairs',\n",
              "  'Education',\n",
              "  'Science',\n",
              "  'Climate'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "labels = list(encode_label_types.keys())\n",
        "len(labels), labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UIk-nebzjp3"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlCDrzRN0BLn",
        "outputId": "ebf12dc6-6695-4653-af67-afc2109cd4f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83065, 9229)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "splitter = RandomSplitter(valid_pct=0.1, seed=42)\n",
        "train_ids, valid_ids = splitter(df)\n",
        "len(train_ids), len(valid_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ONSE-r4UEiWD",
        "outputId": "d7426e1d-29f4-4313-b599-b26bc9759105"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                     title  \\\n",
              "45416                                   'After assembly, I cried': Surrey school grapples with race issues   \n",
              "9144                Cop27 failed on keeping global heating to just 1.5C, Ed Miliband says – as it happened   \n",
              "11922                           Former EU diplomats urge bloc to suspend cooperation agreement with Israel   \n",
              "50441  Tour de France 2025: Tim Merlier wins stage nine as Van der Poel break falls short – as it happened   \n",
              "31180                Australian-style social media ban for under-16s ‘a retrograde step’, say UK charities   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
              "45416  The clip, which lasts just under a minute, shows a schoolgirl, blurred out to protect her identity, addressing a crowd of children – most in their school uniforms – carrying placards. Her voice echoes across the playground, shaking with emotion. “After the assembly on Tuesday, I cried. I cried out of sadness because I felt the school had invalidated my feelings and experience as a black person. I cried out of frustration because I was told that instead of standing up for myself I should stay silent,” the girl says. The speech was made in the playground of Nonsuch high school for girls in S...   \n",
              "9144   Rishi Sunak has kept open the door to closer ties with the European Union but tried to pacify angry Brexiters in his own party by laying down a red line that the UK must remain free to set its own standards and regulation. The boss of Britain’s biggest airport group has said there is “no doubt” that Brexit has damaged the UK economy, adding that it has “massively exacerbated” worker shortages. Ed Miliband, the shadow secretary for climate change and net zero, has said Cop27 failed on the key issue of keeping global heating to just 1.5C above pre-industrial levels. (See 4.23pm.) He was spea...   \n",
              "11922  More than 300 former European diplomats and officials have written to EU leaders urging a “far more” decisive response to the war in Gaza, including a full suspension of the bloc’s cooperation agreement with Israel. Published soon after the opening of the UN general assembly in New York, the letter from the 314 signatories also calls on all EU member states to recognise the state of Palestine, joining 147 countries to have done so. France, Belgium, the UK, Canada and Australia, among others, are expected to show support for a Palestinian state later this month at the UN. The statement foll...   \n",
              "50441  Tour de France 2025 stage nine race report: Thank you for following the blog and getting in touch. It ended up being quite an exciting day! After a breakaway by Alpecin-Deceuninck’s Mathieu Van der Poel and Jonas Rickaert, where the duo stayed away for the majority of today’s stage, the peloton caught the attackers and the day ended in a bunch sprint. Tim Merlier of Soudal Quick-Step took his second stage win of this year’s Tour, but Van der Poel and Rickaert were successful in getting the latter on to the podium with a most combative award. Elsewhere, Elisa Longo Borghini has retained her...   \n",
              "31180  Child safety experts have warned the UK government against enacting an Australian-style social media ban for children under 16, which they called a “retrograde step” that would “do more harm than good”. On Thursday, Australia became the first country in the world to ban under-16s from using social media platforms. The move was supported by a large majority of the Australian public – but academics, politicians and child rights groups said it could backfire, driving teenagers to the dark web, or make them feel more isolated. These concerns were echoed by child safety experts in the UK, who c...   \n",
              "\n",
              "                                      labels  \\\n",
              "45416               ['Economy', 'Education']   \n",
              "9144                            ['Politics']   \n",
              "11922  ['International_Affairs', 'Politics']   \n",
              "50441                         ['Technology']   \n",
              "31180             ['Politics', 'Technology']   \n",
              "\n",
              "                                                                                                                                 url  \\\n",
              "45416               https://www.theguardian.com/education/2021/mar/28/after-assembly-i-cried-surrey-school-grapples-with-race-issues   \n",
              "9144   https://www.theguardian.com/politics/live/2022/nov/21/brexit-rishi-sunak-swiss-deal-cbi-chief-uk-politics-live-latest-updates   \n",
              "11922  https://www.theguardian.com/world/2025/sep/11/former-eu-diplomats-urge-suspension-of-blocs-co-operation-agreement-with-israel   \n",
              "50441              https://www.theguardian.com/sport/live/2025/jul/13/tour-de-france-2025-stage-nine-from-chinon-to-chateauroux-live   \n",
              "31180                  https://www.theguardian.com/technology/2024/nov/29/australia-social-media-ban-under-16s-uk-charities-children   \n",
              "\n",
              "          section                     revised_labels            label_cat_list  \n",
              "45416   education               [Economy, Education]  [0, 0, 0, 1, 0, 1, 0, 0]  \n",
              "9144     politics                         [Politics]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
              "11922       world  [International_Affairs, Politics]  [1, 0, 0, 0, 1, 0, 0, 0]  \n",
              "50441       sport                       [Technology]  [0, 1, 0, 0, 0, 0, 0, 0]  \n",
              "31180  technology             [Politics, Technology]  [1, 1, 0, 0, 0, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25d49b06-a76f-47ee-b117-ebcaea43c0bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "      <th>url</th>\n",
              "      <th>section</th>\n",
              "      <th>revised_labels</th>\n",
              "      <th>label_cat_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45416</th>\n",
              "      <td>'After assembly, I cried': Surrey school grapples with race issues</td>\n",
              "      <td>The clip, which lasts just under a minute, shows a schoolgirl, blurred out to protect her identity, addressing a crowd of children – most in their school uniforms – carrying placards. Her voice echoes across the playground, shaking with emotion. “After the assembly on Tuesday, I cried. I cried out of sadness because I felt the school had invalidated my feelings and experience as a black person. I cried out of frustration because I was told that instead of standing up for myself I should stay silent,” the girl says. The speech was made in the playground of Nonsuch high school for girls in S...</td>\n",
              "      <td>['Economy', 'Education']</td>\n",
              "      <td>https://www.theguardian.com/education/2021/mar/28/after-assembly-i-cried-surrey-school-grapples-with-race-issues</td>\n",
              "      <td>education</td>\n",
              "      <td>[Economy, Education]</td>\n",
              "      <td>[0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9144</th>\n",
              "      <td>Cop27 failed on keeping global heating to just 1.5C, Ed Miliband says – as it happened</td>\n",
              "      <td>Rishi Sunak has kept open the door to closer ties with the European Union but tried to pacify angry Brexiters in his own party by laying down a red line that the UK must remain free to set its own standards and regulation. The boss of Britain’s biggest airport group has said there is “no doubt” that Brexit has damaged the UK economy, adding that it has “massively exacerbated” worker shortages. Ed Miliband, the shadow secretary for climate change and net zero, has said Cop27 failed on the key issue of keeping global heating to just 1.5C above pre-industrial levels. (See 4.23pm.) He was spea...</td>\n",
              "      <td>['Politics']</td>\n",
              "      <td>https://www.theguardian.com/politics/live/2022/nov/21/brexit-rishi-sunak-swiss-deal-cbi-chief-uk-politics-live-latest-updates</td>\n",
              "      <td>politics</td>\n",
              "      <td>[Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11922</th>\n",
              "      <td>Former EU diplomats urge bloc to suspend cooperation agreement with Israel</td>\n",
              "      <td>More than 300 former European diplomats and officials have written to EU leaders urging a “far more” decisive response to the war in Gaza, including a full suspension of the bloc’s cooperation agreement with Israel. Published soon after the opening of the UN general assembly in New York, the letter from the 314 signatories also calls on all EU member states to recognise the state of Palestine, joining 147 countries to have done so. France, Belgium, the UK, Canada and Australia, among others, are expected to show support for a Palestinian state later this month at the UN. The statement foll...</td>\n",
              "      <td>['International_Affairs', 'Politics']</td>\n",
              "      <td>https://www.theguardian.com/world/2025/sep/11/former-eu-diplomats-urge-suspension-of-blocs-co-operation-agreement-with-israel</td>\n",
              "      <td>world</td>\n",
              "      <td>[International_Affairs, Politics]</td>\n",
              "      <td>[1, 0, 0, 0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50441</th>\n",
              "      <td>Tour de France 2025: Tim Merlier wins stage nine as Van der Poel break falls short – as it happened</td>\n",
              "      <td>Tour de France 2025 stage nine race report: Thank you for following the blog and getting in touch. It ended up being quite an exciting day! After a breakaway by Alpecin-Deceuninck’s Mathieu Van der Poel and Jonas Rickaert, where the duo stayed away for the majority of today’s stage, the peloton caught the attackers and the day ended in a bunch sprint. Tim Merlier of Soudal Quick-Step took his second stage win of this year’s Tour, but Van der Poel and Rickaert were successful in getting the latter on to the podium with a most combative award. Elsewhere, Elisa Longo Borghini has retained her...</td>\n",
              "      <td>['Technology']</td>\n",
              "      <td>https://www.theguardian.com/sport/live/2025/jul/13/tour-de-france-2025-stage-nine-from-chinon-to-chateauroux-live</td>\n",
              "      <td>sport</td>\n",
              "      <td>[Technology]</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31180</th>\n",
              "      <td>Australian-style social media ban for under-16s ‘a retrograde step’, say UK charities</td>\n",
              "      <td>Child safety experts have warned the UK government against enacting an Australian-style social media ban for children under 16, which they called a “retrograde step” that would “do more harm than good”. On Thursday, Australia became the first country in the world to ban under-16s from using social media platforms. The move was supported by a large majority of the Australian public – but academics, politicians and child rights groups said it could backfire, driving teenagers to the dark web, or make them feel more isolated. These concerns were echoed by child safety experts in the UK, who c...</td>\n",
              "      <td>['Politics', 'Technology']</td>\n",
              "      <td>https://www.theguardian.com/technology/2024/nov/29/australia-social-media-ban-under-16s-uk-charities-children</td>\n",
              "      <td>technology</td>\n",
              "      <td>[Politics, Technology]</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25d49b06-a76f-47ee-b117-ebcaea43c0bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25d49b06-a76f-47ee-b117-ebcaea43c0bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25d49b06-a76f-47ee-b117-ebcaea43c0bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a80881ec-8a6d-48f8-9744-340cc238a81d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a80881ec-8a6d-48f8-9744-340cc238a81d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a80881ec-8a6d-48f8-9744-340cc238a81d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "valid_df",
              "summary": "{\n  \"name\": \"valid_df\",\n  \"rows\": 9229,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9222,\n        \"samples\": [\n          \"UK dentists should give antibiotics to patients at risk of heart infection \\u2013 study \",\n          \"Unremembering Rishi retunes his songbook for the Tory multiverse | John Crace\",\n          \"Post Office chief Nick Read cleared of misconduct in separate inquiry \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9229,\n        \"samples\": [\n          \"Fans and followers of Theranos and its founder Elizabeth Holmes can now take home an expensive original piece of the company. On eBay, more than a dozen allegedly authentic products from the now-defunct Silicon Valley firm are being sold \\u2013 and much like the company itself, are listed at inflated prices. A set of five Theranos branded pens recently sold for $150. A water bottle is currently listed for $1,500. For $11,000, you can purchase an \\u201cauthentic\\u201d Theranos lab coat (notably \\u201cnever worn\\u201d). Many of these products are typical of Silicon Valley firms, which are known to hand out branded pens, shirts and water bottles at conferences. But the demand for those emblazoned with the \\u201cTheranos\\u201d logo have soared following the company\\u2019s spectacular downfall. Holmes was convicted in January on four of 11 charges of fraud following a weeks-long trial chronicling her role in the blood testing firm. The trial attracted massive attention, but months after the verdict was read, the hype has not waned. A Hulu television series has drawn major viewership, a movie is in the works, and the trial of Holmes co-conspirator Sunny Balwani is currently underway. \\u201cWe are seeing a ton of interest following the Holmes trial, and I don\\u2019t think it\\u2019s going to go away,\\u201d said Aron Solomon, a chief legal analyst for legal marketing firm Esquire Digital who has followed Theranos extensively. The fascination with Holmes has created a micro-industry for new and vintage Theranos merch. And former employees are cashing in. John, a seller behind one account listing Theranos products, worked at the company from 2012 to 2018 and was left with a handful of merchandise. \\u201cI heard people were buying stuff online, so I listed some old clothes I had, kind of as a joke,\\u201d said John, who asked to be quoted by pseudonym due to privacy concerns. \\u201cBut they sold for outrageous amounts of money.\\u201d John worked in manufacturing and \\u2013 like many at Theranos \\u2013 was largely unaware of the issues being experienced with lab testing until it was too late. \\u201cWe were pretty oblivious to what was going on,\\u201d he said. He said he lost his job when the company collapsed. After months of trying to find new employment, he gave up and retired early. He has made more than $1,200 reselling his old Theranos gear, but said it is little consolation. \\u201cI had stock options, so it would have been much better for me if the company had not been a lie and had actually produced something,\\u201d he said. Solomon, the analyst, said the prices for Theranos gear is owed more to the enduring fascination with Holmes herself than with the former company . \\u201cPeople were infinitely fascinated by her as a character,\\u201d he said. \\u201cIt has become a cultural phenomenon, and that\\u2019s never going to change.\\u201d Indeed, Holmes\\u2019s co-conspirator and former romantic partner Balwani is currently facing a jury in the same courtroom where Holmes stood trial. But the proceedings have garnered a substantially smaller following. \\u201cThis college dropout wearing a black turtleneck who managed to rip off Henry Kissinger \\u2013 it\\u2019s become a cultural obsession,\\u201d Solomon said. \\u201cNobody would ever collect Sunny Balwani merchandise.\\u201d\",\n          \"A billion dollars worth of bitcoins linked to the shuttered darknet market Silk Road has changed hands for the first time in seven years, prompting renewed speculation about the fate of the illicit fortune. Almost 70,000 bitcoins stored in the account which, like all bitcoin wallets, is visible to the public, had lain untouched since April 2013. The website was shut down by an FBI raid six months after they were deposited, and they have not moved since. Late on Tuesday night, however, the full amount less a $12 (\\u00a39) transaction fee was transferred to a new bitcoin address, records show. \\u201cThrough blockchain analysis we can determine that these funds likely originated from the Silk Road,\\u201d said Tom Robinson, chief scientist at the cryptocurrency analysts Elliptic. \\u201cThey left the Silk Road\\u2019s wallet back on 6 May 2012 when they were worth around $350,000 and then remained dormant for nearly a year, before being moved \\u2026 in April 2013.\\u201d From there, the funds have lain dormant. After the marketplace was shut down in late 2013, its founder and boss, 36-year-old San Franciscan Ross Ulbricht, was sentenced to a double life sentence plus 40 years without possibility of parole. The FBI managed to seize 174,000 bitcoins, then worth about $100m, but an estimated 450,000 earned by the marketplace remain unaccounted for. Robinson says it is unclear who moved the money. \\u201cThe movement of these bitcoins today, now worth around $955m, may represent Ulbricht or a Silk Road vendor moving their funds,\\u201d he said. \\u201cHowever, it seems unlikely that Ulbricht would be able to conduct a bitcoin transaction from prison.\\u201d One possibility is that an individual or group has managed to \\u201ccrack\\u201d the wallet, effectively guessing its password and stealing the funds. A file that some claimed was an encrypted bitcoin wallet containing the keys to the funds has been circulated in cryptocurrency communities for the past year, and \\u2013 if it is what it was claimed to be \\u2013 then a combination of brute computing power and good luck could have successfully decrypted the wallet. Simply guessing the private key of a bitcoin wallet is functionally impossible. Using the world\\u2019s fastest supercomputer to try every combination would take as many times more than the age of the universe as there have been seconds since the Big Bang. But if the actual wallet file leaks, then the task is much simpler because it only involves guessing the password that protects the private key. \\u201cIt is likely that it was \\u2018brute-forced\\u2019, ie all possible passwords were tried. This is computationally feasible if the password is short enough,\\u201d Robinson said. \\u201cNote that you can\\u2019t do this with any old bitcoin address. What is unusual here is that an encrypted wallet file for this address has apparently become available [if it\\u2019s real]. \\u201cEither way, the funds are now on the move, and whoever now controls the bitcoins may want to cash them out,\\u201d Robinson said. As for whether it was an insider or a hacker, his guess it\\u2019s as likely either way. \\u201cI\\u2019d say I\\u2019m 50/50 right now, perhaps leaning towards a Silk Roader. They were clearly biding their time and waiting for a busy news day in order to do this without it getting too much attention.\\u201d\",\n          \"George Calombaris has said he takes \\u201cfull responsibility\\u201d for the $7.8m underpayment scandal that has engulfed his restaurant empire, pleading with the public not to \\u201cpunish my people\\u201d. But as the celebrity chef sought to draw a line under the scandal with an emotional television apology to his current and former staff and a vow to keep his eateries open, at least one former employee expressed scepticism at his \\u201cdisingenuous\\u201d pledge to be a \\u201cvoice for change\\u201d in the industry. Calombaris\\u2019s reputation has taken a hit after the Fair Work Ombudsman fined his Made Establishment company $200,000 over a failure to pay award rates, penalty rates, casual loadings, overtime and other allowances. In an interview on ABC\\u2019s 7.30 program on Wednesday night, Calombaris denied his company had systemically tried to deny people what they were owed. \\u201cIt\\u2019s called not having the proper infrastructure in the background to make sure that the classifications are being checked and done correctly,\\u201d he said. \\u201cTo be on top of that, all this stuff, there\\u2019s a whole myriad of stuff that needs to be ticked and checked and checked and tripled-checked that weren\\u2019t being done. \\u201cThere is no excuse for what I did. There is no excuse.\\u201d Calombaris said the company had \\u201cself-reported\\u201d when it discovered the underpayment in 2017. It went on to pay out $2.6m to more than 160 people, but this month a Fair Work Ombudsman investigation put the actual figure at nearly $8m. Asked if he had first become aware of the issue in 2017, he said: \\u201cIn 2015 there was a letter that came to us about some issues that we needed to fix in terms of [classifications]. \\u201cI\\u2019ll own up to that. And assuming that it would all be fixed, that\\u2019s when in 2017 we discovered that there was a problem.\\u201d Calombaris\\u2019s Melbourne-based restaurant empire includes the Hellenic Republic, Gazi and Jimmy Grants. Orlaith Belfrage, a former Hellenic Republic worker and Hospo Voice leader, said she welcomed Calombaris\\u2019s mea culpa. But she was unconvinced by a pledge from Calombaris to become a \\u201cvoice for change\\u201d in the industry. She claimed she had not received the full backpay she believed she was owed from her two years at Hellenic Republic, while Nine newspapers has also reported that two other staff have come forward saying they were also underpaid. \\u201cI find it a little bit disingenuous to be apologising before there\\u2019s been a full resolution,\\u201d Belfrage told Guardian Australia. \\u201cThere have been employers that have just been paying people properly from the very beginning of their businesses \\u2026 I just think they should be the voice for change and should be supported, not people that have historically made mistakes.\\u201d A Made spokeswoman said it had now correctly compensated \\u201call known affected former and current employees\\u201d. The company has sought to contact former employees \\u201cin light of their recent accusations to discuss their concerns\\u201d but had not yet heard back, she said. In the interview, Calombaris also discussed the news that he and his two fellow MasterChef hosts were leaving the show. Reports claimed the trio had left after Network Ten refused to grant them large pay increases. Calombaris did not answer when asked if the hosts had asked for pay rises of up to 40%. \\u201cWhat I can say is that the sticking point that we got to with MasterChef and with Channel Ten was simply time.\\u201d he said. \\u201cThe dollars were all signed off.\\u201d He also vowed to keep his restaurants open. \\u201cDon\\u2019t punish my people,\\u201d he said. \\u201cJust know when you come into one of our restaurants, know when you pay the bill, that \\u2026 my people are getting paid and paid correctly.\\u201d He said he took \\u201cfull responsibility for this\\u201d and did not \\u201cblame anyone\\u201d else. Belfrage said the scandal gave her hope that there would be a move towards improved business models in the hospitality industry. \\u201cI don\\u2019t think there\\u2019s any real reason to personally shame him at all,\\u201d she said. \\u201cBut it definitely should cause the public to think really think more about this industry and maybe how they can change it.\\u201d\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"['Science', 'Business']\",\n          \"['Science', 'Politics']\",\n          \"['Business', 'Technology']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9229,\n        \"samples\": [\n          \"https://www.theguardian.com/technology/2022/apr/29/elizabeth-holmes-theranos-ebay-merchandise\",\n          \"https://www.theguardian.com/technology/2020/nov/04/silk-road-bitcoins-worth-1bn-change-hands-after-seven-years\",\n          \"https://www.theguardian.com/media/2019/jul/31/george-calombaris-former-masterchef-judge-says-theres-no-excuse-for-underpaying-workers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"environment\",\n          \"media\",\n          \"education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"revised_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_cat_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "valid_df = df.loc[valid_ids]\n",
        "valid_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVWpZARu0PPf"
      },
      "source": [
        "We will be using `valid_df` for all inference testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k48Mzr8yy97P"
      },
      "source": [
        "# Fastai & Blurr Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWDpv__nzCYX"
      },
      "outputs": [],
      "source": [
        "model_path = \"models/news-final-classifier-stage.pkl\"\n",
        "learner_inf = load_learner(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDSxzq9-WHOd",
        "outputId": "4baf3d46-9cba-4f42-9aef-2e939efbee35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'labels': ['Technology'],\n",
              "  'scores': [0.9305159449577332],\n",
              "  'class_indices': [0, 1, 0, 0, 0, 0, 0, 0],\n",
              "  'class_labels': ['Politics', 'Technology', 'Business', 'Economy', 'International_Affairs', 'Education', 'Science', 'Climate'],\n",
              "  'probs': [0.04822034388780594,\n",
              "   0.9305159449577332,\n",
              "   0.040772438049316406,\n",
              "   0.003963741939514875,\n",
              "   0.04992474615573883,\n",
              "   0.00906728208065033,\n",
              "   0.4973832964897156,\n",
              "   0.015563529916107655]}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "learner_inf.blurr_predict(\"random placeholder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDtYv7bxWMVF",
        "outputId": "da07bf1c-9f3e-419e-ea37-09c5b1dc7dea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Technology']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "learner_inf.blurr_predict(\"random placeholder\")[0]['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5uoheElKzURd",
        "outputId": "0d9ceaeb-6639-42f2-83bb-e86016b7e1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Categories: ['Technology']\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "pred = learner_inf.blurr_predict(\"random placeholder\")[0]\n",
        "labels = pred['labels']\n",
        "\n",
        "# If it's wrapped in a string, unpack it\n",
        "if len(labels) == 1 and labels[0].startswith('['):\n",
        "    labels = ast.literal_eval(labels[0])\n",
        "\n",
        "print(f\"Predicted Categories: {labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRg4ExrCi4P"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzQc7GDWCkbX"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def metric_measures(test_df, preds):\n",
        "\n",
        "  targets = [np.asarray(target) for target in test_df['label_cat_list'].to_list()]\n",
        "  outputs = [np.asarray(pred) for pred in preds]\n",
        "\n",
        "\n",
        "  accuracy = metrics.accuracy_score(targets, outputs)\n",
        "  f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "  f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "\n",
        "  print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "  print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a86cfcab08cc40fdb7477bebd3591b05",
            "a1ecbee732e7415081d916340787de6a",
            "0b0aafb41ece4d3892e53b266c07a4c4",
            "c452f56070b94004abeba3850c969bb0",
            "e5627b537eba4b098b797a461ed38427",
            "d7661cc330db4af5a6928c2bb2bd52cb",
            "f72c69178b124b389df63c8c28268c31",
            "6c603a5193304cd09204c17390946997",
            "fdcafa6fde03483a95c8ee1315d68641",
            "b2e34b0d41094d4a82228fb49e1eb9a5",
            "acff09a9072f468d85fa55fb6a4d5b52"
          ]
        },
        "id": "c4-5Qmr1C4aA",
        "outputId": "b2f2e950-b6a2-4eca-e4c6-1de57285d2b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9229 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a86cfcab08cc40fdb7477bebd3591b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0, 1, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "preds = []\n",
        "for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
        "  desc = row['text']\n",
        "  labels = learner_inf.blurr_predict(desc)[0]['class_indices']\n",
        "  # pred_genres = [0] * len(encode_genre_types)\n",
        "  # for label in labels:\n",
        "  #   pred_genres[encode_genre_types[label]] = 1\n",
        "  preds.append(labels)\n",
        "\n",
        "preds[0][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aa01198a6c0b49eda0556de50a065bea",
            "ee7acbc31a3e4fadba58e37bcca3ca16",
            "c5d841848a98435bae0046c4ce5772c9",
            "c32d84d5b6f14a1b96cdce53b78eb07e",
            "ca2a539b07254fd6aa4a7ef638d6345f",
            "a4d1ed7c712c4ba09be125f845a4bc0f",
            "e4862d60556b4e999f6ac605520b90ba",
            "51e450aa711742d6ac966abe32777d87",
            "e9b791579ee7413381f0fa7531b38825",
            "1519c680ad14433ca2e4e34e1afe09c7",
            "0169202727ad4819b577f54621336be7"
          ]
        },
        "id": "qSKu-e6wpB-x",
        "outputId": "f4beca7c-fb98-4779-eaa7-2b58c6795f3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa01198a6c0b49eda0556de50a065bea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9229 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# num_labels = len(encode_label_types)  # total labels\n",
        "# preds = []\n",
        "\n",
        "# for _, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
        "#     desc = row['text']\n",
        "#     label_indices = learner_inf.blurr_predict(desc)[0]['class_indices']\n",
        "\n",
        "#     pred_vec = [0] * num_labels\n",
        "#     for idx in label_indices:\n",
        "#         pred_vec[idx] = 1   # directly use index\n",
        "\n",
        "#     preds.append(pred_vec)\n",
        "\n",
        "# preds = np.array(preds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqxu69BImVR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc5cd0e-36e0-454a-cc96-098e603e8bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "num_labels = len(encode_label_types)  # = 57\n",
        "print(num_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBWGFT7pMA5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36026a2a-34ef-4af4-e81f-758f2fd2add2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Micro) = 0.8396309160439251\n",
            "F1 Score (Macro) = 0.8253650989383114\n"
          ]
        }
      ],
      "source": [
        "metric_measures(valid_df, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGk37-eOI83k"
      },
      "source": [
        "# Convert to ONNX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StfZ_qBHKKpJ"
      },
      "source": [
        "### ONNX\n",
        "ONNX (Open Neural Network Exchange) is a open standard for representing machine learning models.\n",
        "- It allows developers to move models between different frameworks (such as PyTorch, TensorFlow, and Caffe2) without losing performance or accuracy.\n",
        "ONNX makes it easier to build and run AI models on a variety of hardware, including GPUs, CPUs, and custom accelerators.\n",
        "- This standard helps eliminate the need for multiple conversion tools and provides a unified representation of the model across different tools and frameworks.\n",
        "- ONNX is being developed by a collaboration of companies including Microsoft, Facebook, Amazon, and IBM, among others, making it a well-supported and widely-adopted standard.\n",
        "- Converting to ONNX runtime often makes model faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOnIS2HamCS5"
      },
      "outputs": [],
      "source": [
        "model_path = \"models/news-final-classifier-stage.pkl\"\n",
        "learner_inf = load_learner(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gZI8_vQpJSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3807018-7f25-473e-9496-af21433643f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "learner_inf.model.hf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6fYnDJ705HX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d468d34-0f4e-4804-a509-ccd57311dc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.2)\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.13-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxscript-0.5.7-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.4/693.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.13-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx_ir, onnxscript\n",
            "Successfully installed onnx_ir-0.1.13 onnxscript-0.5.7\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime onnxscript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRd5hf3--LDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6db8c60-6238-4457-8504-2e8dacf71604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0106 06:57:55.217000 990 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 14 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `RobertaForSequenceClassification([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `RobertaForSequenceClassification([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 14).\n",
            "WARNING:onnxscript.version_converter:Failed to convert the model to the target version 14 using the ONNX C API. The model was not modified\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
            "    converted_proto = _c_api_utils.call_onnx_api(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
            "    result = func(proto)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
            "    return onnx.version_converter.convert_version(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/version_converter.py\", line 39, in convert_version\n",
            "    converted_model_str = C.convert_version(model_str, target_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: /github/workspace/onnx/version_converter/adapters/no_previous_version.h:26: adapt: Assertion `false` failed: No Previous Version of LayerNormalization exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 21 of general pattern rewrite rules.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ONNXProgram(\n",
              "    model=\n",
              "        <\n",
              "            ir_version=10,\n",
              "            opset_imports={'': 18},\n",
              "            producer_name='pytorch',\n",
              "            producer_version='2.9.0+cpu',\n",
              "            domain=None,\n",
              "            model_version=None,\n",
              "        >\n",
              "        graph(\n",
              "            name=main_graph,\n",
              "            inputs=(\n",
              "                %\"input_ids\"<INT64,[s72,s70]>\n",
              "            ),\n",
              "            outputs=(\n",
              "                %\"output\"<FLOAT,[1,8]>\n",
              "            ),\n",
              "            initializers=(\n",
              "                %\"roberta.embeddings.token_type_embeddings.weight\"<FLOAT,[1,768]>{TorchTensor(...)},\n",
              "                %\"roberta.embeddings.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.embeddings.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.self.query.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.self.key.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.self.value.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.attention.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"classifier.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"classifier.out_proj.bias\"<FLOAT,[8]>{TorchTensor<FLOAT,[8]>(Parameter containing: tensor([ 0.0114, -0.0163, -0.0178, -0.0291, -0.0301, -0.0281, -0.0320, -0.0389], requires_grad=True), name='classifier.out_proj.bias')},\n",
              "                %\"roberta.embeddings.token_type_ids\"<INT64,[1,514]>{TorchTensor(...)},\n",
              "                %\"roberta.embeddings.word_embeddings.weight\"<FLOAT,[50265,768]>{TorchTensor(...)},\n",
              "                %\"roberta.embeddings.position_embeddings.weight\"<FLOAT,[514,768]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.0.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.1.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.2.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.3.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.4.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"roberta.encoder.layer.5.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
              "                %\"classifier.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
              "                %\"classifier.out_proj.weight\"<FLOAT,[8,768]>{TorchTensor(...)},\n",
              "                %\"val_9\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_9')},\n",
              "                %\"val_26\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='val_26')},\n",
              "                %\"val_47\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_47')},\n",
              "                %\"val_48\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_56\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_64\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_95\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.35355338], dtype=float32), name='val_95')},\n",
              "                %\"val_107\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_111\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_120\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_124\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_131\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, -1, 12, 64]), name='val_131')},\n",
              "                %\"val_132\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_140\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_181\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_185\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_194\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_198\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_206\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_214\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_255\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_259\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_268\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_272\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_280\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_288\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_329\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_333\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_342\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_346\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_354\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_362\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_403\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_407\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_416\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_420\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_428\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_436\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_477\"<FLOAT,[768,768]>{Tensor(...)},\n",
              "                %\"val_481\"<FLOAT,[768,3072]>{Tensor(...)},\n",
              "                %\"val_490\"<FLOAT,[3072,768]>{Tensor(...)},\n",
              "                %\"val_2\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_2')},\n",
              "                %\"val_6\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_6')},\n",
              "                %\"val_16\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_16')},\n",
              "                %\"val_494\"<INT64,[2]>{Tensor<INT64,[2]>(array([1, 2]), name='val_494')},\n",
              "                %\"val_52\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_52')},\n",
              "                %\"val_53\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_53')},\n",
              "                %\"val_54\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_54')},\n",
              "                %\"val_82\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_82')},\n",
              "                %\"val_85\"<INT64,[1]>{Tensor<INT64,[1]>(array([-2]), name='val_85')},\n",
              "                %\"val_87\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_87')},\n",
              "                %\"val_105\"<INT64,[1]>{Tensor<INT64,[1]>(array([768]), name='val_105')},\n",
              "                %\"val_113\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name='val_113')},\n",
              "                %\"val_118\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name='val_118')}\n",
              "            ),\n",
              "        ) {\n",
              "              0 |  # node_Shape_0\n",
              "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"input_ids\") {end=1, start=0}\n",
              "              1 |  # node_Shape_1\n",
              "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"input_ids\") {end=2, start=1}\n",
              "              2 |  # node_slice_1\n",
              "                   %\"slice_1\"<INT64,[1,s70]> ⬅️ ::Slice(%\"roberta.embeddings.token_type_ids\"{...}, %\"val_2\"{[0]}, %\"val_1\", %\"val_9\"{[1]}, %\"val_9\"{[1]})\n",
              "              3 |  # node_Concat_14\n",
              "                   %\"val_14\"<INT64,[2]> ⬅️ ::Concat(%\"val_0\", %\"val_1\") {axis=0}\n",
              "              4 |  # node_expand\n",
              "                   %\"expand\"<INT64,[s72,s70]> ⬅️ ::Expand(%\"slice_1\", %\"val_14\")\n",
              "              5 |  # node_Equal_15\n",
              "                   %\"val_15\"<BOOL,[s72,s70]> ⬅️ ::Equal(%\"input_ids\", %\"val_6\"{1})\n",
              "              6 |  # node_ne\n",
              "                   %\"ne\"<BOOL,[s72,s70]> ⬅️ ::Not(%\"val_15\")\n",
              "              7 |  # node__to_copy\n",
              "                   %\"_to_copy\"<INT32,[s72,s70]> ⬅️ ::Cast(%\"ne\") {to=6}\n",
              "              8 |  # node_convert_element_type_default\n",
              "                   %\"convert_element_type_default\"<INT64,[s72,s70]> ⬅️ ::Cast(%\"_to_copy\") {to=7}\n",
              "              9 |  # node_cumsum\n",
              "                   %\"cumsum\"<INT64,[1,s70]> ⬅️ ::CumSum(%\"convert_element_type_default\", %\"val_6\"{1}) {reverse=0, exclusive=0}\n",
              "             10 |  # node_type_as\n",
              "                   %\"type_as\"<INT32,[1,s70]> ⬅️ ::Cast(%\"cumsum\") {to=6}\n",
              "             11 |  # node_mul_12\n",
              "                   %\"mul_12\"<INT32,[s72,s70]> ⬅️ ::Mul(%\"type_as\", %\"_to_copy\")\n",
              "             12 |  # node__to_copy_1\n",
              "                   %\"_to_copy_1\"<INT64,[s72,s70]> ⬅️ ::Cast(%\"mul_12\") {to=7}\n",
              "             13 |  # node_add_26\n",
              "                   %\"add_26\"<INT64,[s72,s70]> ⬅️ ::Add(%\"_to_copy_1\", %\"val_6\"{1})\n",
              "             14 |  # node_embedding\n",
              "                   %\"embedding\"<FLOAT,[s72,s70,768]> ⬅️ ::Gather(%\"roberta.embeddings.word_embeddings.weight\"{...}, %\"input_ids\") {axis=0}\n",
              "             15 |  # node_embedding_1\n",
              "                   %\"embedding_1\"<FLOAT,[s72,s70,768]> ⬅️ ::Gather(%\"roberta.embeddings.token_type_embeddings.weight\"{...}, %\"expand\") {axis=0}\n",
              "             16 |  # node_add_38\n",
              "                   %\"add_38\"<FLOAT,[s72,s70,768]> ⬅️ ::Add(%\"embedding\", %\"embedding_1\")\n",
              "             17 |  # node_embedding_2\n",
              "                   %\"embedding_2\"<FLOAT,[s72,s70,768]> ⬅️ ::Gather(%\"roberta.embeddings.position_embeddings.weight\"{...}, %\"add_26\") {axis=0}\n",
              "             18 |  # node_add_55\n",
              "                   %\"add_55\"<FLOAT,[s72,s70,768]> ⬅️ ::Add(%\"add_38\", %\"embedding_2\")\n",
              "             19 |  # node_layer_norm\n",
              "                   %\"layer_norm\"<FLOAT,[s72,s70,768]> ⬅️ ::LayerNormalization(%\"add_55\", %\"roberta.embeddings.LayerNorm.weight\"{...}, %\"roberta.embeddings.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             20 |  # node_ones\n",
              "                   %\"ones\"<FLOAT,[s72,s70]> ⬅️ ::Expand(%\"val_26\"{1.0}, %\"val_14\")\n",
              "             21 |  # node_Unsqueeze_508\n",
              "                   %\"unsqueeze_1\"<FLOAT,[s72,1,1,s70]> ⬅️ ::Unsqueeze(%\"ones\", %\"val_494\"{[1, 2]})\n",
              "             22 |  # node_Concat_43\n",
              "                   %\"val_45\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_9\"{[1]}, %\"val_1\", %\"val_1\") {axis=0}\n",
              "             23 |  # node_expand_1\n",
              "                   %\"expand_1\"<FLOAT,[s72,1,s70,s70]> ⬅️ ::Expand(%\"unsqueeze_1\", %\"val_45\")\n",
              "             24 |  # node_sub_48\n",
              "                   %\"sub_48\"<FLOAT,[s72,1,s70,s70]> ⬅️ ::Sub(%\"val_26\"{1.0}, %\"expand_1\")\n",
              "             25 |  # node__to_copy_2\n",
              "                   %\"_to_copy_2\"<BOOL,[s72,1,s70,s70]> ⬅️ ::Cast(%\"sub_48\") {to=9}\n",
              "             26 |  # node_masked_fill\n",
              "                   %\"masked_fill\"<FLOAT,[s72,1,s70,s70]> ⬅️ ::Where(%\"_to_copy_2\", %\"val_47\"{-3.4028234663852886e+38}, %\"sub_48\")\n",
              "             27 |  # node_MatMul_47\n",
              "                   %\"val_49\"<FLOAT,[s72,s70,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_48\"{...})\n",
              "             28 |  # node_linear\n",
              "                   %\"linear\"<FLOAT,[s72,(s70//s72),768]> ⬅️ ::Add(%\"val_49\", %\"roberta.encoder.layer.0.attention.self.query.bias\"{...})\n",
              "             29 |  # node_Concat_53\n",
              "                   %\"val_55\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_52\"{[-1]}, %\"val_53\"{[12]}, %\"val_54\"{[64]}) {axis=0}\n",
              "             30 |  # node_view\n",
              "                   %\"view\"<FLOAT,[s72,(s70//s72),12,64]> ⬅️ ::Reshape(%\"linear\", %\"val_55\") {allowzero=1}\n",
              "             31 |  # node_transpose\n",
              "                   %\"transpose\"<FLOAT,[s72,12,(s70//s72),64]> ⬅️ ::Transpose(%\"view\") {perm=(0, 2, 1, 3)}\n",
              "             32 |  # node_MatMul_55\n",
              "                   %\"val_57\"<FLOAT,[s72,s70,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_56\"{...})\n",
              "             33 |  # node_linear_1\n",
              "                   %\"linear_1\"<FLOAT,[s72,(s70//s72),768]> ⬅️ ::Add(%\"val_57\", %\"roberta.encoder.layer.0.attention.self.key.bias\"{...})\n",
              "             34 |  # node_view_1\n",
              "                   %\"view_1\"<FLOAT,[s72,(s70//s72),12,64]> ⬅️ ::Reshape(%\"linear_1\", %\"val_55\") {allowzero=1}\n",
              "             35 |  # node_transpose_1\n",
              "                   %\"transpose_1\"<FLOAT,[s72,12,(s70//s72),64]> ⬅️ ::Transpose(%\"view_1\") {perm=(0, 2, 1, 3)}\n",
              "             36 |  # node_MatMul_63\n",
              "                   %\"val_65\"<FLOAT,[s72,s70,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_64\"{...})\n",
              "             37 |  # node_linear_2\n",
              "                   %\"linear_2\"<FLOAT,[s72,(s70//s72),768]> ⬅️ ::Add(%\"val_65\", %\"roberta.encoder.layer.0.attention.self.value.bias\"{...})\n",
              "             38 |  # node_view_2\n",
              "                   %\"view_2\"<FLOAT,[s72,(s70//s72),12,64]> ⬅️ ::Reshape(%\"linear_2\", %\"val_55\") {allowzero=1}\n",
              "             39 |  # node_transpose_2\n",
              "                   %\"transpose_2\"<FLOAT,[s72,12,(s70//s72),64]> ⬅️ ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
              "             40 |  # node_Shape_79\n",
              "                   %\"val_81\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_1\") {start=0}\n",
              "             41 |  # node_Slice_82\n",
              "                   %\"val_84\"<INT64,[1]> ⬅️ ::Slice(%\"val_81\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "             42 |  # node_Slice_84\n",
              "                   %\"val_86\"<INT64,[1]> ⬅️ ::Slice(%\"val_81\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "             43 |  # node_Slice_86\n",
              "                   %\"val_88\"<INT64,[2]> ⬅️ ::Slice(%\"val_81\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "             44 |  # node_Concat_88\n",
              "                   %\"val_90\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_86\", %\"val_84\") {axis=0}\n",
              "             45 |  # node_Reshape_89\n",
              "                   %\"val_91\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_1\", %\"val_90\") {allowzero=0}\n",
              "             46 |  # node_Transpose_90\n",
              "                   %\"val_92\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_91\") {perm=(0, 2, 1)}\n",
              "             47 |  # node_Concat_91\n",
              "                   %\"val_93\"<INT64,[4]> ⬅️ ::Concat(%\"val_88\", %\"val_84\", %\"val_86\") {axis=0}\n",
              "             48 |  # node_Reshape_92\n",
              "                   %\"val_94\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_92\", %\"val_93\") {allowzero=0}\n",
              "             49 |  # node_Mul_94\n",
              "                   %\"val_96\"<FLOAT,[s72,12,(s70//s72),64]> ⬅️ ::Mul(%\"transpose\", %\"val_95\"{[0.3535533845424652]})\n",
              "             50 |  # node_Mul_96\n",
              "                   %\"val_98\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_94\", %\"val_95\"{[0.3535533845424652]})\n",
              "             51 |  # node_MatMul_97\n",
              "                   %\"val_99\"<FLOAT,[None,12,(s70//s72),None]> ⬅️ ::MatMul(%\"val_96\", %\"val_98\")\n",
              "             52 |  # node_Add_98\n",
              "                   %\"val_100\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_99\", %\"masked_fill\")\n",
              "             53 |  # node_Softmax_99\n",
              "                   %\"val_101\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_100\") {axis=-1}\n",
              "             54 |  # node_scaled_dot_product_attention\n",
              "                   %\"scaled_dot_product_attention\"<FLOAT,[s72,12,(s70//s72),64]> ⬅️ ::MatMul(%\"val_101\", %\"transpose_2\")\n",
              "             55 |  # node_transpose_3\n",
              "                   %\"transpose_3\"<FLOAT,[s72,(s70//s72),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention\") {perm=(0, 2, 1, 3)}\n",
              "             56 |  # node_Concat_104\n",
              "                   %\"val_106\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_1\", %\"val_105\"{[768]}) {axis=0}\n",
              "             57 |  # node_view_3\n",
              "                   %\"view_3\"<FLOAT,[s72,(s70//s72),768]> ⬅️ ::Reshape(%\"transpose_3\", %\"val_106\") {allowzero=1}\n",
              "             58 |  # node_MatMul_106\n",
              "                   %\"val_108\"<FLOAT,[s72,(s70//s72),768]> ⬅️ ::MatMul(%\"view_3\", %\"val_107\"{...})\n",
              "             59 |  # node_linear_3\n",
              "                   %\"linear_3\"<FLOAT,[s72,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_108\", %\"roberta.encoder.layer.0.attention.output.dense.bias\"{...})\n",
              "             60 |  # node_add_167\n",
              "                   %\"add_167\"<FLOAT,[s72,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_3\", %\"layer_norm\")\n",
              "             61 |  # node_layer_norm_1\n",
              "                   %\"layer_norm_1\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_167\", %\"roberta.encoder.layer.0.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.0.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             62 |  # node_MatMul_108\n",
              "                   %\"val_112\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_111\"{...})\n",
              "             63 |  # node_linear_4\n",
              "                   %\"linear_4\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_112\", %\"roberta.encoder.layer.0.intermediate.dense.bias\"{...})\n",
              "             64 |  # node_Div_110\n",
              "                   %\"val_114\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_4\", %\"val_113\"{1.4142135381698608})\n",
              "             65 |  # node_Erf_111\n",
              "                   %\"val_115\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_114\")\n",
              "             66 |  # node_Add_113\n",
              "                   %\"val_117\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_115\", %\"val_26\"{1.0})\n",
              "             67 |  # node_Mul_115\n",
              "                   %\"val_119\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_117\")\n",
              "             68 |  # node_gelu\n",
              "                   %\"gelu\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_4\", %\"val_119\")\n",
              "             69 |  # node_MatMul_117\n",
              "                   %\"val_121\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu\", %\"val_120\"{...})\n",
              "             70 |  # node_linear_5\n",
              "                   %\"linear_5\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_121\", %\"roberta.encoder.layer.0.output.dense.bias\"{...})\n",
              "             71 |  # node_add_187\n",
              "                   %\"add_187\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_5\", %\"layer_norm_1\")\n",
              "             72 |  # node_layer_norm_2\n",
              "                   %\"layer_norm_2\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_187\", %\"roberta.encoder.layer.0.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.0.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             73 |  # node_MatMul_119\n",
              "                   %\"val_125\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_124\"{...})\n",
              "             74 |  # node_linear_6\n",
              "                   %\"linear_6\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_125\", %\"roberta.encoder.layer.1.attention.self.query.bias\"{...})\n",
              "             75 |  # node_view_4\n",
              "                   %\"view_4\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_6\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "             76 |  # node_transpose_4\n",
              "                   %\"transpose_4\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
              "             77 |  # node_MatMul_127\n",
              "                   %\"val_133\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_132\"{...})\n",
              "             78 |  # node_linear_7\n",
              "                   %\"linear_7\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_133\", %\"roberta.encoder.layer.1.attention.self.key.bias\"{...})\n",
              "             79 |  # node_view_5\n",
              "                   %\"view_5\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_7\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "             80 |  # node_transpose_5\n",
              "                   %\"transpose_5\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_5\") {perm=(0, 2, 1, 3)}\n",
              "             81 |  # node_MatMul_135\n",
              "                   %\"val_141\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_140\"{...})\n",
              "             82 |  # node_linear_8\n",
              "                   %\"linear_8\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_141\", %\"roberta.encoder.layer.1.attention.self.value.bias\"{...})\n",
              "             83 |  # node_view_6\n",
              "                   %\"view_6\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "             84 |  # node_transpose_6\n",
              "                   %\"transpose_6\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_6\") {perm=(0, 2, 1, 3)}\n",
              "             85 |  # node_Shape_151\n",
              "                   %\"val_157\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_5\") {start=0}\n",
              "             86 |  # node_Slice_153\n",
              "                   %\"val_159\"<INT64,[1]> ⬅️ ::Slice(%\"val_157\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "             87 |  # node_Slice_154\n",
              "                   %\"val_160\"<INT64,[1]> ⬅️ ::Slice(%\"val_157\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "             88 |  # node_Slice_156\n",
              "                   %\"val_162\"<INT64,[2]> ⬅️ ::Slice(%\"val_157\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "             89 |  # node_Concat_158\n",
              "                   %\"val_164\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_160\", %\"val_159\") {axis=0}\n",
              "             90 |  # node_Reshape_159\n",
              "                   %\"val_165\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_5\", %\"val_164\") {allowzero=0}\n",
              "             91 |  # node_Transpose_160\n",
              "                   %\"val_166\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_165\") {perm=(0, 2, 1)}\n",
              "             92 |  # node_Concat_161\n",
              "                   %\"val_167\"<INT64,[4]> ⬅️ ::Concat(%\"val_162\", %\"val_159\", %\"val_160\") {axis=0}\n",
              "             93 |  # node_Reshape_162\n",
              "                   %\"val_168\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_166\", %\"val_167\") {allowzero=0}\n",
              "             94 |  # node_Mul_164\n",
              "                   %\"val_170\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Mul(%\"transpose_4\", %\"val_95\"{[0.3535533845424652]})\n",
              "             95 |  # node_Mul_166\n",
              "                   %\"val_172\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_168\", %\"val_95\"{[0.3535533845424652]})\n",
              "             96 |  # node_MatMul_167\n",
              "                   %\"val_173\"<FLOAT,[None,12,(s70//(s72**2)),None]> ⬅️ ::MatMul(%\"val_170\", %\"val_172\")\n",
              "             97 |  # node_Add_168\n",
              "                   %\"val_174\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_173\", %\"masked_fill\")\n",
              "             98 |  # node_Softmax_169\n",
              "                   %\"val_175\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_174\") {axis=-1}\n",
              "             99 |  # node_scaled_dot_product_attention_1\n",
              "                   %\"scaled_dot_product_attention_1\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::MatMul(%\"val_175\", %\"transpose_6\")\n",
              "            100 |  # node_transpose_7\n",
              "                   %\"transpose_7\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(0, 2, 1, 3)}\n",
              "            101 |  # node_Concat_174\n",
              "                   %\"val_180\"<INT64,[3]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_1\", %\"val_105\"{[768]}) {axis=0}\n",
              "            102 |  # node_view_7\n",
              "                   %\"view_7\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Reshape(%\"transpose_7\", %\"val_180\") {allowzero=1}\n",
              "            103 |  # node_MatMul_176\n",
              "                   %\"val_182\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"view_7\", %\"val_181\"{...})\n",
              "            104 |  # node_linear_9\n",
              "                   %\"linear_9\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_182\", %\"roberta.encoder.layer.1.attention.output.dense.bias\"{...})\n",
              "            105 |  # node_add_240\n",
              "                   %\"add_240\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_9\", %\"layer_norm_2\")\n",
              "            106 |  # node_layer_norm_3\n",
              "                   %\"layer_norm_3\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_240\", %\"roberta.encoder.layer.1.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.1.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            107 |  # node_MatMul_178\n",
              "                   %\"val_186\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_185\"{...})\n",
              "            108 |  # node_linear_10\n",
              "                   %\"linear_10\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_186\", %\"roberta.encoder.layer.1.intermediate.dense.bias\"{...})\n",
              "            109 |  # node_Div_180\n",
              "                   %\"val_188\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_10\", %\"val_113\"{1.4142135381698608})\n",
              "            110 |  # node_Erf_181\n",
              "                   %\"val_189\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_188\")\n",
              "            111 |  # node_Add_183\n",
              "                   %\"val_191\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_189\", %\"val_26\"{1.0})\n",
              "            112 |  # node_Mul_185\n",
              "                   %\"val_193\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_191\")\n",
              "            113 |  # node_gelu_1\n",
              "                   %\"gelu_1\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_10\", %\"val_193\")\n",
              "            114 |  # node_MatMul_187\n",
              "                   %\"val_195\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_194\"{...})\n",
              "            115 |  # node_linear_11\n",
              "                   %\"linear_11\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_195\", %\"roberta.encoder.layer.1.output.dense.bias\"{...})\n",
              "            116 |  # node_add_259\n",
              "                   %\"add_259\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_11\", %\"layer_norm_3\")\n",
              "            117 |  # node_layer_norm_4\n",
              "                   %\"layer_norm_4\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_259\", %\"roberta.encoder.layer.1.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.1.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            118 |  # node_MatMul_189\n",
              "                   %\"val_199\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_198\"{...})\n",
              "            119 |  # node_linear_12\n",
              "                   %\"linear_12\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_199\", %\"roberta.encoder.layer.2.attention.self.query.bias\"{...})\n",
              "            120 |  # node_view_8\n",
              "                   %\"view_8\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            121 |  # node_transpose_8\n",
              "                   %\"transpose_8\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
              "            122 |  # node_MatMul_197\n",
              "                   %\"val_207\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_206\"{...})\n",
              "            123 |  # node_linear_13\n",
              "                   %\"linear_13\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_207\", %\"roberta.encoder.layer.2.attention.self.key.bias\"{...})\n",
              "            124 |  # node_view_9\n",
              "                   %\"view_9\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_13\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            125 |  # node_transpose_9\n",
              "                   %\"transpose_9\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_9\") {perm=(0, 2, 1, 3)}\n",
              "            126 |  # node_MatMul_205\n",
              "                   %\"val_215\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_214\"{...})\n",
              "            127 |  # node_linear_14\n",
              "                   %\"linear_14\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_215\", %\"roberta.encoder.layer.2.attention.self.value.bias\"{...})\n",
              "            128 |  # node_view_10\n",
              "                   %\"view_10\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_14\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            129 |  # node_transpose_10\n",
              "                   %\"transpose_10\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
              "            130 |  # node_Shape_221\n",
              "                   %\"val_231\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_9\") {start=0}\n",
              "            131 |  # node_Slice_223\n",
              "                   %\"val_233\"<INT64,[1]> ⬅️ ::Slice(%\"val_231\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "            132 |  # node_Slice_224\n",
              "                   %\"val_234\"<INT64,[1]> ⬅️ ::Slice(%\"val_231\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "            133 |  # node_Slice_226\n",
              "                   %\"val_236\"<INT64,[2]> ⬅️ ::Slice(%\"val_231\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "            134 |  # node_Concat_228\n",
              "                   %\"val_238\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_234\", %\"val_233\") {axis=0}\n",
              "            135 |  # node_Reshape_229\n",
              "                   %\"val_239\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_9\", %\"val_238\") {allowzero=0}\n",
              "            136 |  # node_Transpose_230\n",
              "                   %\"val_240\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_239\") {perm=(0, 2, 1)}\n",
              "            137 |  # node_Concat_231\n",
              "                   %\"val_241\"<INT64,[4]> ⬅️ ::Concat(%\"val_236\", %\"val_233\", %\"val_234\") {axis=0}\n",
              "            138 |  # node_Reshape_232\n",
              "                   %\"val_242\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_240\", %\"val_241\") {allowzero=0}\n",
              "            139 |  # node_Mul_234\n",
              "                   %\"val_244\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Mul(%\"transpose_8\", %\"val_95\"{[0.3535533845424652]})\n",
              "            140 |  # node_Mul_236\n",
              "                   %\"val_246\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_242\", %\"val_95\"{[0.3535533845424652]})\n",
              "            141 |  # node_MatMul_237\n",
              "                   %\"val_247\"<FLOAT,[None,12,(s70//(s72**2)),None]> ⬅️ ::MatMul(%\"val_244\", %\"val_246\")\n",
              "            142 |  # node_Add_238\n",
              "                   %\"val_248\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_247\", %\"masked_fill\")\n",
              "            143 |  # node_Softmax_239\n",
              "                   %\"val_249\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_248\") {axis=-1}\n",
              "            144 |  # node_scaled_dot_product_attention_2\n",
              "                   %\"scaled_dot_product_attention_2\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::MatMul(%\"val_249\", %\"transpose_10\")\n",
              "            145 |  # node_transpose_11\n",
              "                   %\"transpose_11\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(0, 2, 1, 3)}\n",
              "            146 |  # node_view_11\n",
              "                   %\"view_11\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Reshape(%\"transpose_11\", %\"val_180\") {allowzero=1}\n",
              "            147 |  # node_MatMul_246\n",
              "                   %\"val_256\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"view_11\", %\"val_255\"{...})\n",
              "            148 |  # node_linear_15\n",
              "                   %\"linear_15\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_256\", %\"roberta.encoder.layer.2.attention.output.dense.bias\"{...})\n",
              "            149 |  # node_add_312\n",
              "                   %\"add_312\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_15\", %\"layer_norm_4\")\n",
              "            150 |  # node_layer_norm_5\n",
              "                   %\"layer_norm_5\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_312\", %\"roberta.encoder.layer.2.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.2.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            151 |  # node_MatMul_248\n",
              "                   %\"val_260\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_259\"{...})\n",
              "            152 |  # node_linear_16\n",
              "                   %\"linear_16\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_260\", %\"roberta.encoder.layer.2.intermediate.dense.bias\"{...})\n",
              "            153 |  # node_Div_250\n",
              "                   %\"val_262\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_16\", %\"val_113\"{1.4142135381698608})\n",
              "            154 |  # node_Erf_251\n",
              "                   %\"val_263\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_262\")\n",
              "            155 |  # node_Add_253\n",
              "                   %\"val_265\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_263\", %\"val_26\"{1.0})\n",
              "            156 |  # node_Mul_255\n",
              "                   %\"val_267\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_265\")\n",
              "            157 |  # node_gelu_2\n",
              "                   %\"gelu_2\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_16\", %\"val_267\")\n",
              "            158 |  # node_MatMul_257\n",
              "                   %\"val_269\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_268\"{...})\n",
              "            159 |  # node_linear_17\n",
              "                   %\"linear_17\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_269\", %\"roberta.encoder.layer.2.output.dense.bias\"{...})\n",
              "            160 |  # node_add_331\n",
              "                   %\"add_331\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_17\", %\"layer_norm_5\")\n",
              "            161 |  # node_layer_norm_6\n",
              "                   %\"layer_norm_6\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_331\", %\"roberta.encoder.layer.2.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.2.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            162 |  # node_MatMul_259\n",
              "                   %\"val_273\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_272\"{...})\n",
              "            163 |  # node_linear_18\n",
              "                   %\"linear_18\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_273\", %\"roberta.encoder.layer.3.attention.self.query.bias\"{...})\n",
              "            164 |  # node_view_12\n",
              "                   %\"view_12\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_18\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            165 |  # node_transpose_12\n",
              "                   %\"transpose_12\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_12\") {perm=(0, 2, 1, 3)}\n",
              "            166 |  # node_MatMul_267\n",
              "                   %\"val_281\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_280\"{...})\n",
              "            167 |  # node_linear_19\n",
              "                   %\"linear_19\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_281\", %\"roberta.encoder.layer.3.attention.self.key.bias\"{...})\n",
              "            168 |  # node_view_13\n",
              "                   %\"view_13\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_19\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            169 |  # node_transpose_13\n",
              "                   %\"transpose_13\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_13\") {perm=(0, 2, 1, 3)}\n",
              "            170 |  # node_MatMul_275\n",
              "                   %\"val_289\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_288\"{...})\n",
              "            171 |  # node_linear_20\n",
              "                   %\"linear_20\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_289\", %\"roberta.encoder.layer.3.attention.self.value.bias\"{...})\n",
              "            172 |  # node_view_14\n",
              "                   %\"view_14\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            173 |  # node_transpose_14\n",
              "                   %\"transpose_14\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_14\") {perm=(0, 2, 1, 3)}\n",
              "            174 |  # node_Shape_291\n",
              "                   %\"val_305\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_13\") {start=0}\n",
              "            175 |  # node_Slice_293\n",
              "                   %\"val_307\"<INT64,[1]> ⬅️ ::Slice(%\"val_305\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "            176 |  # node_Slice_294\n",
              "                   %\"val_308\"<INT64,[1]> ⬅️ ::Slice(%\"val_305\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "            177 |  # node_Slice_296\n",
              "                   %\"val_310\"<INT64,[2]> ⬅️ ::Slice(%\"val_305\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "            178 |  # node_Concat_298\n",
              "                   %\"val_312\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_308\", %\"val_307\") {axis=0}\n",
              "            179 |  # node_Reshape_299\n",
              "                   %\"val_313\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_13\", %\"val_312\") {allowzero=0}\n",
              "            180 |  # node_Transpose_300\n",
              "                   %\"val_314\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_313\") {perm=(0, 2, 1)}\n",
              "            181 |  # node_Concat_301\n",
              "                   %\"val_315\"<INT64,[4]> ⬅️ ::Concat(%\"val_310\", %\"val_307\", %\"val_308\") {axis=0}\n",
              "            182 |  # node_Reshape_302\n",
              "                   %\"val_316\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_314\", %\"val_315\") {allowzero=0}\n",
              "            183 |  # node_Mul_304\n",
              "                   %\"val_318\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Mul(%\"transpose_12\", %\"val_95\"{[0.3535533845424652]})\n",
              "            184 |  # node_Mul_306\n",
              "                   %\"val_320\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_316\", %\"val_95\"{[0.3535533845424652]})\n",
              "            185 |  # node_MatMul_307\n",
              "                   %\"val_321\"<FLOAT,[None,12,(s70//(s72**2)),None]> ⬅️ ::MatMul(%\"val_318\", %\"val_320\")\n",
              "            186 |  # node_Add_308\n",
              "                   %\"val_322\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_321\", %\"masked_fill\")\n",
              "            187 |  # node_Softmax_309\n",
              "                   %\"val_323\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_322\") {axis=-1}\n",
              "            188 |  # node_scaled_dot_product_attention_3\n",
              "                   %\"scaled_dot_product_attention_3\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::MatMul(%\"val_323\", %\"transpose_14\")\n",
              "            189 |  # node_transpose_15\n",
              "                   %\"transpose_15\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(0, 2, 1, 3)}\n",
              "            190 |  # node_view_15\n",
              "                   %\"view_15\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Reshape(%\"transpose_15\", %\"val_180\") {allowzero=1}\n",
              "            191 |  # node_MatMul_316\n",
              "                   %\"val_330\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"view_15\", %\"val_329\"{...})\n",
              "            192 |  # node_linear_21\n",
              "                   %\"linear_21\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_330\", %\"roberta.encoder.layer.3.attention.output.dense.bias\"{...})\n",
              "            193 |  # node_add_384\n",
              "                   %\"add_384\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_21\", %\"layer_norm_6\")\n",
              "            194 |  # node_layer_norm_7\n",
              "                   %\"layer_norm_7\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_384\", %\"roberta.encoder.layer.3.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.3.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            195 |  # node_MatMul_318\n",
              "                   %\"val_334\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_333\"{...})\n",
              "            196 |  # node_linear_22\n",
              "                   %\"linear_22\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_334\", %\"roberta.encoder.layer.3.intermediate.dense.bias\"{...})\n",
              "            197 |  # node_Div_320\n",
              "                   %\"val_336\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_22\", %\"val_113\"{1.4142135381698608})\n",
              "            198 |  # node_Erf_321\n",
              "                   %\"val_337\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_336\")\n",
              "            199 |  # node_Add_323\n",
              "                   %\"val_339\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_337\", %\"val_26\"{1.0})\n",
              "            200 |  # node_Mul_325\n",
              "                   %\"val_341\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_339\")\n",
              "            201 |  # node_gelu_3\n",
              "                   %\"gelu_3\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_22\", %\"val_341\")\n",
              "            202 |  # node_MatMul_327\n",
              "                   %\"val_343\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_342\"{...})\n",
              "            203 |  # node_linear_23\n",
              "                   %\"linear_23\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_343\", %\"roberta.encoder.layer.3.output.dense.bias\"{...})\n",
              "            204 |  # node_add_403\n",
              "                   %\"add_403\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_23\", %\"layer_norm_7\")\n",
              "            205 |  # node_layer_norm_8\n",
              "                   %\"layer_norm_8\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_403\", %\"roberta.encoder.layer.3.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.3.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            206 |  # node_MatMul_329\n",
              "                   %\"val_347\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_346\"{...})\n",
              "            207 |  # node_linear_24\n",
              "                   %\"linear_24\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_347\", %\"roberta.encoder.layer.4.attention.self.query.bias\"{...})\n",
              "            208 |  # node_view_16\n",
              "                   %\"view_16\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            209 |  # node_transpose_16\n",
              "                   %\"transpose_16\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_16\") {perm=(0, 2, 1, 3)}\n",
              "            210 |  # node_MatMul_337\n",
              "                   %\"val_355\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_354\"{...})\n",
              "            211 |  # node_linear_25\n",
              "                   %\"linear_25\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_355\", %\"roberta.encoder.layer.4.attention.self.key.bias\"{...})\n",
              "            212 |  # node_view_17\n",
              "                   %\"view_17\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_25\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            213 |  # node_transpose_17\n",
              "                   %\"transpose_17\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_17\") {perm=(0, 2, 1, 3)}\n",
              "            214 |  # node_MatMul_345\n",
              "                   %\"val_363\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_362\"{...})\n",
              "            215 |  # node_linear_26\n",
              "                   %\"linear_26\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_363\", %\"roberta.encoder.layer.4.attention.self.value.bias\"{...})\n",
              "            216 |  # node_view_18\n",
              "                   %\"view_18\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_26\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            217 |  # node_transpose_18\n",
              "                   %\"transpose_18\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
              "            218 |  # node_Shape_361\n",
              "                   %\"val_379\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_17\") {start=0}\n",
              "            219 |  # node_Slice_363\n",
              "                   %\"val_381\"<INT64,[1]> ⬅️ ::Slice(%\"val_379\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "            220 |  # node_Slice_364\n",
              "                   %\"val_382\"<INT64,[1]> ⬅️ ::Slice(%\"val_379\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "            221 |  # node_Slice_366\n",
              "                   %\"val_384\"<INT64,[2]> ⬅️ ::Slice(%\"val_379\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "            222 |  # node_Concat_368\n",
              "                   %\"val_386\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_382\", %\"val_381\") {axis=0}\n",
              "            223 |  # node_Reshape_369\n",
              "                   %\"val_387\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_17\", %\"val_386\") {allowzero=0}\n",
              "            224 |  # node_Transpose_370\n",
              "                   %\"val_388\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_387\") {perm=(0, 2, 1)}\n",
              "            225 |  # node_Concat_371\n",
              "                   %\"val_389\"<INT64,[4]> ⬅️ ::Concat(%\"val_384\", %\"val_381\", %\"val_382\") {axis=0}\n",
              "            226 |  # node_Reshape_372\n",
              "                   %\"val_390\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_388\", %\"val_389\") {allowzero=0}\n",
              "            227 |  # node_Mul_374\n",
              "                   %\"val_392\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Mul(%\"transpose_16\", %\"val_95\"{[0.3535533845424652]})\n",
              "            228 |  # node_Mul_376\n",
              "                   %\"val_394\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_390\", %\"val_95\"{[0.3535533845424652]})\n",
              "            229 |  # node_MatMul_377\n",
              "                   %\"val_395\"<FLOAT,[None,12,(s70//(s72**2)),None]> ⬅️ ::MatMul(%\"val_392\", %\"val_394\")\n",
              "            230 |  # node_Add_378\n",
              "                   %\"val_396\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_395\", %\"masked_fill\")\n",
              "            231 |  # node_Softmax_379\n",
              "                   %\"val_397\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_396\") {axis=-1}\n",
              "            232 |  # node_scaled_dot_product_attention_4\n",
              "                   %\"scaled_dot_product_attention_4\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::MatMul(%\"val_397\", %\"transpose_18\")\n",
              "            233 |  # node_transpose_19\n",
              "                   %\"transpose_19\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(0, 2, 1, 3)}\n",
              "            234 |  # node_view_19\n",
              "                   %\"view_19\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Reshape(%\"transpose_19\", %\"val_180\") {allowzero=1}\n",
              "            235 |  # node_MatMul_386\n",
              "                   %\"val_404\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"view_19\", %\"val_403\"{...})\n",
              "            236 |  # node_linear_27\n",
              "                   %\"linear_27\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_404\", %\"roberta.encoder.layer.4.attention.output.dense.bias\"{...})\n",
              "            237 |  # node_add_456\n",
              "                   %\"add_456\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_27\", %\"layer_norm_8\")\n",
              "            238 |  # node_layer_norm_9\n",
              "                   %\"layer_norm_9\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_456\", %\"roberta.encoder.layer.4.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.4.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            239 |  # node_MatMul_388\n",
              "                   %\"val_408\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_407\"{...})\n",
              "            240 |  # node_linear_28\n",
              "                   %\"linear_28\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_408\", %\"roberta.encoder.layer.4.intermediate.dense.bias\"{...})\n",
              "            241 |  # node_Div_390\n",
              "                   %\"val_410\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_28\", %\"val_113\"{1.4142135381698608})\n",
              "            242 |  # node_Erf_391\n",
              "                   %\"val_411\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_410\")\n",
              "            243 |  # node_Add_393\n",
              "                   %\"val_413\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_411\", %\"val_26\"{1.0})\n",
              "            244 |  # node_Mul_395\n",
              "                   %\"val_415\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_413\")\n",
              "            245 |  # node_gelu_4\n",
              "                   %\"gelu_4\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_28\", %\"val_415\")\n",
              "            246 |  # node_MatMul_397\n",
              "                   %\"val_417\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_416\"{...})\n",
              "            247 |  # node_linear_29\n",
              "                   %\"linear_29\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_417\", %\"roberta.encoder.layer.4.output.dense.bias\"{...})\n",
              "            248 |  # node_add_475\n",
              "                   %\"add_475\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_29\", %\"layer_norm_9\")\n",
              "            249 |  # node_layer_norm_10\n",
              "                   %\"layer_norm_10\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_475\", %\"roberta.encoder.layer.4.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.4.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            250 |  # node_MatMul_399\n",
              "                   %\"val_421\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_420\"{...})\n",
              "            251 |  # node_linear_30\n",
              "                   %\"linear_30\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_421\", %\"roberta.encoder.layer.5.attention.self.query.bias\"{...})\n",
              "            252 |  # node_view_20\n",
              "                   %\"view_20\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_30\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            253 |  # node_transpose_20\n",
              "                   %\"transpose_20\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_20\") {perm=(0, 2, 1, 3)}\n",
              "            254 |  # node_MatMul_407\n",
              "                   %\"val_429\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_428\"{...})\n",
              "            255 |  # node_linear_31\n",
              "                   %\"linear_31\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_429\", %\"roberta.encoder.layer.5.attention.self.key.bias\"{...})\n",
              "            256 |  # node_view_21\n",
              "                   %\"view_21\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_31\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            257 |  # node_transpose_21\n",
              "                   %\"transpose_21\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_21\") {perm=(0, 2, 1, 3)}\n",
              "            258 |  # node_MatMul_415\n",
              "                   %\"val_437\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_436\"{...})\n",
              "            259 |  # node_linear_32\n",
              "                   %\"linear_32\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_437\", %\"roberta.encoder.layer.5.attention.self.value.bias\"{...})\n",
              "            260 |  # node_view_22\n",
              "                   %\"view_22\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_131\"{[1, -1, 12, 64]}) {allowzero=1}\n",
              "            261 |  # node_transpose_22\n",
              "                   %\"transpose_22\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
              "            262 |  # node_Shape_431\n",
              "                   %\"val_453\"<INT64,[4]> ⬅️ ::Shape(%\"transpose_21\") {start=0}\n",
              "            263 |  # node_Slice_433\n",
              "                   %\"val_455\"<INT64,[1]> ⬅️ ::Slice(%\"val_453\", %\"val_52\"{[-1]}, %\"val_82\"{[9223372036854775807]})\n",
              "            264 |  # node_Slice_434\n",
              "                   %\"val_456\"<INT64,[1]> ⬅️ ::Slice(%\"val_453\", %\"val_85\"{[-2]}, %\"val_52\"{[-1]})\n",
              "            265 |  # node_Slice_436\n",
              "                   %\"val_458\"<INT64,[2]> ⬅️ ::Slice(%\"val_453\", %\"val_87\"{[-9223372036854775808]}, %\"val_85\"{[-2]})\n",
              "            266 |  # node_Concat_438\n",
              "                   %\"val_460\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\"{[-1]}, %\"val_456\", %\"val_455\") {axis=0}\n",
              "            267 |  # node_Reshape_439\n",
              "                   %\"val_461\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"transpose_21\", %\"val_460\") {allowzero=0}\n",
              "            268 |  # node_Transpose_440\n",
              "                   %\"val_462\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_461\") {perm=(0, 2, 1)}\n",
              "            269 |  # node_Concat_441\n",
              "                   %\"val_463\"<INT64,[4]> ⬅️ ::Concat(%\"val_458\", %\"val_455\", %\"val_456\") {axis=0}\n",
              "            270 |  # node_Reshape_442\n",
              "                   %\"val_464\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_462\", %\"val_463\") {allowzero=0}\n",
              "            271 |  # node_Mul_444\n",
              "                   %\"val_466\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::Mul(%\"transpose_20\", %\"val_95\"{[0.3535533845424652]})\n",
              "            272 |  # node_Mul_446\n",
              "                   %\"val_468\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_464\", %\"val_95\"{[0.3535533845424652]})\n",
              "            273 |  # node_MatMul_447\n",
              "                   %\"val_469\"<FLOAT,[None,12,(s70//(s72**2)),None]> ⬅️ ::MatMul(%\"val_466\", %\"val_468\")\n",
              "            274 |  # node_Add_448\n",
              "                   %\"val_470\"<FLOAT,[None,12,None,None]> ⬅️ ::Add(%\"val_469\", %\"masked_fill\")\n",
              "            275 |  # node_Softmax_449\n",
              "                   %\"val_471\"<FLOAT,[None,12,None,None]> ⬅️ ::Softmax(%\"val_470\") {axis=-1}\n",
              "            276 |  # node_scaled_dot_product_attention_5\n",
              "                   %\"scaled_dot_product_attention_5\"<FLOAT,[1,12,(s70//(s72**2)),64]> ⬅️ ::MatMul(%\"val_471\", %\"transpose_22\")\n",
              "            277 |  # node_transpose_23\n",
              "                   %\"transpose_23\"<FLOAT,[1,(s70//(s72**2)),12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(0, 2, 1, 3)}\n",
              "            278 |  # node_view_23\n",
              "                   %\"view_23\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Reshape(%\"transpose_23\", %\"val_180\") {allowzero=1}\n",
              "            279 |  # node_MatMul_456\n",
              "                   %\"val_478\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"view_23\", %\"val_477\"{...})\n",
              "            280 |  # node_linear_33\n",
              "                   %\"linear_33\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_478\", %\"roberta.encoder.layer.5.attention.output.dense.bias\"{...})\n",
              "            281 |  # node_add_528\n",
              "                   %\"add_528\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_33\", %\"layer_norm_10\")\n",
              "            282 |  # node_layer_norm_11\n",
              "                   %\"layer_norm_11\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_528\", %\"roberta.encoder.layer.5.attention.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.5.attention.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            283 |  # node_MatMul_458\n",
              "                   %\"val_482\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_481\"{...})\n",
              "            284 |  # node_linear_34\n",
              "                   %\"linear_34\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_482\", %\"roberta.encoder.layer.5.intermediate.dense.bias\"{...})\n",
              "            285 |  # node_Div_460\n",
              "                   %\"val_484\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Div(%\"linear_34\", %\"val_113\"{1.4142135381698608})\n",
              "            286 |  # node_Erf_461\n",
              "                   %\"val_485\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Erf(%\"val_484\")\n",
              "            287 |  # node_Add_463\n",
              "                   %\"val_487\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Add(%\"val_485\", %\"val_26\"{1.0})\n",
              "            288 |  # node_Mul_465\n",
              "                   %\"val_489\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"val_118\"{0.5}, %\"val_487\")\n",
              "            289 |  # node_gelu_5\n",
              "                   %\"gelu_5\"<FLOAT,[1,(s70//(s72**2)),3072]> ⬅️ ::Mul(%\"linear_34\", %\"val_489\")\n",
              "            290 |  # node_MatMul_467\n",
              "                   %\"val_491\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_490\"{...})\n",
              "            291 |  # node_linear_35\n",
              "                   %\"linear_35\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"val_491\", %\"roberta.encoder.layer.5.output.dense.bias\"{...})\n",
              "            292 |  # node_add_547\n",
              "                   %\"add_547\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::Add(%\"linear_35\", %\"layer_norm_11\")\n",
              "            293 |  # node_layer_norm_12\n",
              "                   %\"layer_norm_12\"<FLOAT,[1,(s70//(s72**2)),768]> ⬅️ ::LayerNormalization(%\"add_547\", %\"roberta.encoder.layer.5.output.LayerNorm.weight\"{...}, %\"roberta.encoder.layer.5.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            294 |  # node_select\n",
              "                   %\"select\"<FLOAT,[1,768]> ⬅️ ::Gather(%\"layer_norm_12\", %\"val_16\"{0}) {axis=1}\n",
              "            295 |  # node_linear_36\n",
              "                   %\"linear_36\"<FLOAT,[1,768]> ⬅️ ::Gemm(%\"select\", %\"classifier.dense.weight\"{...}, %\"classifier.dense.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
              "            296 |  # node_tanh\n",
              "                   %\"tanh\"<FLOAT,[1,768]> ⬅️ ::Tanh(%\"linear_36\")\n",
              "            297 |  # node_linear_37\n",
              "                   %\"output\"<FLOAT,[1,8]> ⬅️ ::Gemm(%\"tanh\", %\"classifier.out_proj.weight\"{...}, %\"classifier.out_proj.bias\"{[0.011389694176614285, -0.016341187059879303, -0.017837688326835632, -0.02911410480737686, -0.030149057507514954, -0.02810641936957836, -0.03197411075234413, -0.03893521428108215]}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
              "            return %\"output\"<FLOAT,[1,8]>\n",
              "        }\n",
              "\n",
              "\n",
              "    ,\n",
              "    exported_program=\n",
              "        ExportedProgram:\n",
              "            class GraphModule(torch.nn.Module):\n",
              "                def forward(self, p_roberta_embeddings_word_embeddings_weight: \"f32[50265, 768]\", p_roberta_embeddings_position_embeddings_weight: \"f32[514, 768]\", p_roberta_embeddings_token_type_embeddings_weight: \"f32[1, 768]\", p_roberta_embeddings_layernorm_weight: \"f32[768]\", p_roberta_embeddings_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_0_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_0_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_0_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_0_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_0_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_0_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_0_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_0_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_0_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_0_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_0_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_0_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_0_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_0_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_0_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_0_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_1_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_1_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_1_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_1_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_1_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_1_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_1_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_1_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_1_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_1_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_1_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_1_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_1_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_1_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_1_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_1_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_2_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_2_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_2_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_2_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_2_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_2_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_2_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_2_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_2_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_2_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_2_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_2_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_2_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_2_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_2_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_2_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_3_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_3_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_3_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_3_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_3_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_3_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_3_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_3_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_3_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_3_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_3_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_3_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_3_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_3_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_3_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_3_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_4_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_4_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_4_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_4_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_4_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_4_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_4_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_4_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_4_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_4_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_4_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_4_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_4_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_4_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_4_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_4_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_5_attention_self_query_weight: \"f32[768, 768]\", p_roberta_encoder_layer_5_attention_self_query_bias: \"f32[768]\", p_roberta_encoder_layer_5_attention_self_key_weight: \"f32[768, 768]\", p_roberta_encoder_layer_5_attention_self_key_bias: \"f32[768]\", p_roberta_encoder_layer_5_attention_self_value_weight: \"f32[768, 768]\", p_roberta_encoder_layer_5_attention_self_value_bias: \"f32[768]\", p_roberta_encoder_layer_5_attention_output_dense_weight: \"f32[768, 768]\", p_roberta_encoder_layer_5_attention_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_5_attention_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_5_attention_output_layernorm_bias: \"f32[768]\", p_roberta_encoder_layer_5_intermediate_dense_weight: \"f32[3072, 768]\", p_roberta_encoder_layer_5_intermediate_dense_bias: \"f32[3072]\", p_roberta_encoder_layer_5_output_dense_weight: \"f32[768, 3072]\", p_roberta_encoder_layer_5_output_dense_bias: \"f32[768]\", p_roberta_encoder_layer_5_output_layernorm_weight: \"f32[768]\", p_roberta_encoder_layer_5_output_layernorm_bias: \"f32[768]\", p_classifier_dense_weight: \"f32[768, 768]\", p_classifier_dense_bias: \"f32[768]\", p_classifier_out_proj_weight: \"f32[8, 768]\", p_classifier_out_proj_bias: \"f32[8]\", c_roberta_lifted_tensor_0: \"f32[]\", b_roberta_embeddings_position_ids: \"i64[1, 514]\", b_roberta_embeddings_token_type_ids: \"i64[1, 514]\", input_ids: \"i64[s72, s70]\"):\n",
              "                     # \n",
              "                    sym_size_int_41: \"Sym(s72)\" = torch.ops.aten.sym_size.int(input_ids, 0)\n",
              "                    sym_size_int_42: \"Sym(s70)\" = torch.ops.aten.sym_size.int(input_ids, 1)\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:792 in forward, code: buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
              "                    slice_1: \"i64[1, s70]\" = torch.ops.aten.slice.Tensor(b_roberta_embeddings_token_type_ids, 1, None, sym_size_int_42);  b_roberta_embeddings_token_type_ids = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:793 in forward, code: buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
              "                    expand: \"i64[s72, s70]\" = torch.ops.aten.expand.default(slice_1, [sym_size_int_41, sym_size_int_42]);  slice_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:87 in forward, code: position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
              "                    ne: \"b8[s72, s70]\" = torch.ops.aten.ne.Scalar(input_ids, 1)\n",
              "                    _to_copy: \"i32[s72, s70]\" = torch.ops.aten._to_copy.default(ne, dtype = torch.int32);  ne = None\n",
              "                    convert_element_type_default: \"i64[s72, s70]\" = torch.ops.prims.convert_element_type.default(_to_copy, dtype = torch.int64)\n",
              "                    cumsum: \"i64[1, s70]\" = torch.ops.aten.cumsum.default(convert_element_type_default, 1);  convert_element_type_default = None\n",
              "                    type_as: \"i32[s72, s70]\" = torch.ops.aten.type_as.default(cumsum, _to_copy);  cumsum = None\n",
              "                    scalar_tensor_default: \"i32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int32)\n",
              "                    add_16: \"i32[1, s70]\" = torch.ops.aten.add.Tensor(type_as, scalar_tensor_default);  type_as = scalar_tensor_default = None\n",
              "                    mul_12: \"i32[s72, s70]\" = torch.ops.aten.mul.Tensor(add_16, _to_copy);  add_16 = _to_copy = None\n",
              "                    _to_copy_1: \"i64[s72, s70]\" = torch.ops.aten._to_copy.default(mul_12, dtype = torch.int64);  mul_12 = None\n",
              "                    add_26: \"i64[s72, s70]\" = torch.ops.aten.add.Tensor(_to_copy_1, 1);  _to_copy_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
              "                    embedding: \"f32[s72, s70, 768]\" = torch.ops.aten.embedding.default(p_roberta_embeddings_word_embeddings_weight, input_ids, 1);  p_roberta_embeddings_word_embeddings_weight = input_ids = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
              "                    embedding_1: \"f32[s72, s70, 768]\" = torch.ops.aten.embedding.default(p_roberta_embeddings_token_type_embeddings_weight, expand);  p_roberta_embeddings_token_type_embeddings_weight = expand = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:113 in forward, code: embeddings = inputs_embeds + token_type_embeddings\n",
              "                    add_38: \"f32[s72, s70, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
              "                    embedding_2: \"f32[s72, s70, 768]\" = torch.ops.aten.embedding.default(p_roberta_embeddings_position_embeddings_weight, add_26, 1);  p_roberta_embeddings_position_embeddings_weight = add_26 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:116 in forward, code: embeddings += position_embeddings\n",
              "                    add_55: \"f32[s72, s70, 768]\" = torch.ops.aten.add.Tensor(add_38, embedding_2);  add_38 = embedding_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm: \"f32[s72, s70, 768]\" = torch.ops.aten.layer_norm.default(add_55, [768], p_roberta_embeddings_layernorm_weight, p_roberta_embeddings_layernorm_bias);  add_55 = p_roberta_embeddings_layernorm_weight = p_roberta_embeddings_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone: \"f32[s72, s70, 768]\" = torch.ops.aten.clone.default(layer_norm);  layer_norm = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:807 in forward, code: attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
              "                    ones: \"f32[s72, s70]\" = torch.ops.aten.ones.default([sym_size_int_41, sym_size_int_42], device = device(type='cpu'), pin_memory = False)\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:828 in forward, code: extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
              "                    slice_2: \"f32[s72, s70]\" = torch.ops.aten.slice.Tensor(ones, 0, 0, 9223372036854775807);  ones = None\n",
              "                    unsqueeze: \"f32[s72, 1, s70]\" = torch.ops.aten.unsqueeze.default(slice_2, 1);  slice_2 = None\n",
              "                    unsqueeze_1: \"f32[s72, 1, 1, s70]\" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None\n",
              "                    expand_1: \"f32[s72, 1, s70, s70]\" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_41, 1, sym_size_int_42, sym_size_int_42]);  unsqueeze_1 = None\n",
              "                    clone_1: \"f32[]\" = torch.ops.aten.clone.default(c_roberta_lifted_tensor_0);  c_roberta_lifted_tensor_0 = None\n",
              "                    sub_48: \"f32[s72, 1, s70, s70]\" = torch.ops.aten.sub.Tensor(clone_1, expand_1);  clone_1 = expand_1 = None\n",
              "                    _to_copy_2: \"b8[s72, 1, s70, s70]\" = torch.ops.aten._to_copy.default(sub_48, dtype = torch.bool)\n",
              "                    masked_fill: \"f32[s72, 1, s70, s70]\" = torch.ops.aten.masked_fill.Scalar(sub_48, _to_copy_2, -3.4028234663852886e+38);  sub_48 = _to_copy_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear: \"f32[s72, (s70//s72), 768]\" = torch.ops.aten.linear.default(clone, p_roberta_encoder_layer_0_attention_self_query_weight, p_roberta_encoder_layer_0_attention_self_query_bias);  p_roberta_encoder_layer_0_attention_self_query_weight = p_roberta_encoder_layer_0_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view: \"f32[s72, (s70//s72), 12, 64]\" = torch.ops.aten.view.default(linear, [sym_size_int_41, -1, 12, 64]);  linear = None\n",
              "                    transpose: \"f32[s72, 12, (s70//s72), 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_1: \"f32[s72, (s70//s72), 768]\" = torch.ops.aten.linear.default(clone, p_roberta_encoder_layer_0_attention_self_key_weight, p_roberta_encoder_layer_0_attention_self_key_bias);  p_roberta_encoder_layer_0_attention_self_key_weight = p_roberta_encoder_layer_0_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_1: \"f32[s72, (s70//s72), 12, 64]\" = torch.ops.aten.view.default(linear_1, [sym_size_int_41, -1, 12, 64]);  linear_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_1: \"f32[s72, 12, (s70//s72), 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_2: \"f32[s72, (s70//s72), 768]\" = torch.ops.aten.linear.default(clone, p_roberta_encoder_layer_0_attention_self_value_weight, p_roberta_encoder_layer_0_attention_self_value_bias);  p_roberta_encoder_layer_0_attention_self_value_weight = p_roberta_encoder_layer_0_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_2: \"f32[s72, (s70//s72), 12, 64]\" = torch.ops.aten.view.default(linear_2, [sym_size_int_41, -1, 12, 64]);  linear_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_2: \"f32[s72, 12, (s70//s72), 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention: \"f32[s72, 12, (s70//s72), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose, transpose_1, transpose_2, masked_fill);  transpose = transpose_1 = transpose_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_3: \"f32[s72, (s70//s72), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_3: \"f32[s72, (s70//s72), 768]\" = torch.ops.aten.view.default(transpose_3, [sym_size_int_41, sym_size_int_42, 768]);  transpose_3 = sym_size_int_41 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_3: \"f32[s72, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_3, p_roberta_encoder_layer_0_attention_output_dense_weight, p_roberta_encoder_layer_0_attention_output_dense_bias);  view_3 = p_roberta_encoder_layer_0_attention_output_dense_weight = p_roberta_encoder_layer_0_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_2: \"f32[s72, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_167: \"f32[s72, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_2, clone);  clone_2 = clone = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_1: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_167, [768], p_roberta_encoder_layer_0_attention_output_layernorm_weight, p_roberta_encoder_layer_0_attention_output_layernorm_bias);  add_167 = p_roberta_encoder_layer_0_attention_output_layernorm_weight = p_roberta_encoder_layer_0_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_4: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_1, p_roberta_encoder_layer_0_intermediate_dense_weight, p_roberta_encoder_layer_0_intermediate_dense_bias);  p_roberta_encoder_layer_0_intermediate_dense_weight = p_roberta_encoder_layer_0_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_5: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu, p_roberta_encoder_layer_0_output_dense_weight, p_roberta_encoder_layer_0_output_dense_bias);  gelu = p_roberta_encoder_layer_0_output_dense_weight = p_roberta_encoder_layer_0_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_3: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_187: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_3, layer_norm_1);  clone_3 = layer_norm_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_2: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_187, [768], p_roberta_encoder_layer_0_output_layernorm_weight, p_roberta_encoder_layer_0_output_layernorm_bias);  add_187 = p_roberta_encoder_layer_0_output_layernorm_weight = p_roberta_encoder_layer_0_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_6: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_roberta_encoder_layer_1_attention_self_query_weight, p_roberta_encoder_layer_1_attention_self_query_bias);  p_roberta_encoder_layer_1_attention_self_query_weight = p_roberta_encoder_layer_1_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view_4: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_6, [1, -1, 12, 64]);  linear_6 = None\n",
              "                    transpose_4: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_7: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_roberta_encoder_layer_1_attention_self_key_weight, p_roberta_encoder_layer_1_attention_self_key_bias);  p_roberta_encoder_layer_1_attention_self_key_weight = p_roberta_encoder_layer_1_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_5: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_7, [1, -1, 12, 64]);  linear_7 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_5: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_8: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_roberta_encoder_layer_1_attention_self_value_weight, p_roberta_encoder_layer_1_attention_self_value_bias);  p_roberta_encoder_layer_1_attention_self_value_weight = p_roberta_encoder_layer_1_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_6: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_8, [1, -1, 12, 64]);  linear_8 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_6: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention_1: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_4, transpose_5, transpose_6, masked_fill);  transpose_4 = transpose_5 = transpose_6 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_7: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_7: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.view.default(transpose_7, [1, sym_size_int_42, 768]);  transpose_7 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_9: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_7, p_roberta_encoder_layer_1_attention_output_dense_weight, p_roberta_encoder_layer_1_attention_output_dense_bias);  view_7 = p_roberta_encoder_layer_1_attention_output_dense_weight = p_roberta_encoder_layer_1_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_4: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_240: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_4, layer_norm_2);  clone_4 = layer_norm_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_3: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_240, [768], p_roberta_encoder_layer_1_attention_output_layernorm_weight, p_roberta_encoder_layer_1_attention_output_layernorm_bias);  add_240 = p_roberta_encoder_layer_1_attention_output_layernorm_weight = p_roberta_encoder_layer_1_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_10: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_3, p_roberta_encoder_layer_1_intermediate_dense_weight, p_roberta_encoder_layer_1_intermediate_dense_bias);  p_roberta_encoder_layer_1_intermediate_dense_weight = p_roberta_encoder_layer_1_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu_1: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_11: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu_1, p_roberta_encoder_layer_1_output_dense_weight, p_roberta_encoder_layer_1_output_dense_bias);  gelu_1 = p_roberta_encoder_layer_1_output_dense_weight = p_roberta_encoder_layer_1_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_5: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_259: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_5, layer_norm_3);  clone_5 = layer_norm_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_4: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_259, [768], p_roberta_encoder_layer_1_output_layernorm_weight, p_roberta_encoder_layer_1_output_layernorm_bias);  add_259 = p_roberta_encoder_layer_1_output_layernorm_weight = p_roberta_encoder_layer_1_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_12: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_roberta_encoder_layer_2_attention_self_query_weight, p_roberta_encoder_layer_2_attention_self_query_bias);  p_roberta_encoder_layer_2_attention_self_query_weight = p_roberta_encoder_layer_2_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view_8: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_12, [1, -1, 12, 64]);  linear_12 = None\n",
              "                    transpose_8: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_13: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_roberta_encoder_layer_2_attention_self_key_weight, p_roberta_encoder_layer_2_attention_self_key_bias);  p_roberta_encoder_layer_2_attention_self_key_weight = p_roberta_encoder_layer_2_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_9: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_13, [1, -1, 12, 64]);  linear_13 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_9: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_14: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_roberta_encoder_layer_2_attention_self_value_weight, p_roberta_encoder_layer_2_attention_self_value_bias);  p_roberta_encoder_layer_2_attention_self_value_weight = p_roberta_encoder_layer_2_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_10: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_14, [1, -1, 12, 64]);  linear_14 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_10: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention_2: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_8, transpose_9, transpose_10, masked_fill);  transpose_8 = transpose_9 = transpose_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_11: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_11: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.view.default(transpose_11, [1, sym_size_int_42, 768]);  transpose_11 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_15: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_11, p_roberta_encoder_layer_2_attention_output_dense_weight, p_roberta_encoder_layer_2_attention_output_dense_bias);  view_11 = p_roberta_encoder_layer_2_attention_output_dense_weight = p_roberta_encoder_layer_2_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_6: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_312: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_6, layer_norm_4);  clone_6 = layer_norm_4 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_5: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_312, [768], p_roberta_encoder_layer_2_attention_output_layernorm_weight, p_roberta_encoder_layer_2_attention_output_layernorm_bias);  add_312 = p_roberta_encoder_layer_2_attention_output_layernorm_weight = p_roberta_encoder_layer_2_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_16: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_5, p_roberta_encoder_layer_2_intermediate_dense_weight, p_roberta_encoder_layer_2_intermediate_dense_bias);  p_roberta_encoder_layer_2_intermediate_dense_weight = p_roberta_encoder_layer_2_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu_2: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_17: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu_2, p_roberta_encoder_layer_2_output_dense_weight, p_roberta_encoder_layer_2_output_dense_bias);  gelu_2 = p_roberta_encoder_layer_2_output_dense_weight = p_roberta_encoder_layer_2_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_7: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_331: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_7, layer_norm_5);  clone_7 = layer_norm_5 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_6: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_331, [768], p_roberta_encoder_layer_2_output_layernorm_weight, p_roberta_encoder_layer_2_output_layernorm_bias);  add_331 = p_roberta_encoder_layer_2_output_layernorm_weight = p_roberta_encoder_layer_2_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_18: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_roberta_encoder_layer_3_attention_self_query_weight, p_roberta_encoder_layer_3_attention_self_query_bias);  p_roberta_encoder_layer_3_attention_self_query_weight = p_roberta_encoder_layer_3_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view_12: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_18, [1, -1, 12, 64]);  linear_18 = None\n",
              "                    transpose_12: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_12, 1, 2);  view_12 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_19: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_roberta_encoder_layer_3_attention_self_key_weight, p_roberta_encoder_layer_3_attention_self_key_bias);  p_roberta_encoder_layer_3_attention_self_key_weight = p_roberta_encoder_layer_3_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_13: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_19, [1, -1, 12, 64]);  linear_19 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_13: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_20: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_roberta_encoder_layer_3_attention_self_value_weight, p_roberta_encoder_layer_3_attention_self_value_bias);  p_roberta_encoder_layer_3_attention_self_value_weight = p_roberta_encoder_layer_3_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_14: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_20, [1, -1, 12, 64]);  linear_20 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_14: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_14, 1, 2);  view_14 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention_3: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_12, transpose_13, transpose_14, masked_fill);  transpose_12 = transpose_13 = transpose_14 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_15: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_15: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.view.default(transpose_15, [1, sym_size_int_42, 768]);  transpose_15 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_21: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_15, p_roberta_encoder_layer_3_attention_output_dense_weight, p_roberta_encoder_layer_3_attention_output_dense_bias);  view_15 = p_roberta_encoder_layer_3_attention_output_dense_weight = p_roberta_encoder_layer_3_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_8: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_384: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_8, layer_norm_6);  clone_8 = layer_norm_6 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_7: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_384, [768], p_roberta_encoder_layer_3_attention_output_layernorm_weight, p_roberta_encoder_layer_3_attention_output_layernorm_bias);  add_384 = p_roberta_encoder_layer_3_attention_output_layernorm_weight = p_roberta_encoder_layer_3_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_22: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_7, p_roberta_encoder_layer_3_intermediate_dense_weight, p_roberta_encoder_layer_3_intermediate_dense_bias);  p_roberta_encoder_layer_3_intermediate_dense_weight = p_roberta_encoder_layer_3_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu_3: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_23: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu_3, p_roberta_encoder_layer_3_output_dense_weight, p_roberta_encoder_layer_3_output_dense_bias);  gelu_3 = p_roberta_encoder_layer_3_output_dense_weight = p_roberta_encoder_layer_3_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_9: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_403: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_9, layer_norm_7);  clone_9 = layer_norm_7 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_8: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_403, [768], p_roberta_encoder_layer_3_output_layernorm_weight, p_roberta_encoder_layer_3_output_layernorm_bias);  add_403 = p_roberta_encoder_layer_3_output_layernorm_weight = p_roberta_encoder_layer_3_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_24: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_roberta_encoder_layer_4_attention_self_query_weight, p_roberta_encoder_layer_4_attention_self_query_bias);  p_roberta_encoder_layer_4_attention_self_query_weight = p_roberta_encoder_layer_4_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view_16: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_24, [1, -1, 12, 64]);  linear_24 = None\n",
              "                    transpose_16: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_16, 1, 2);  view_16 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_25: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_roberta_encoder_layer_4_attention_self_key_weight, p_roberta_encoder_layer_4_attention_self_key_bias);  p_roberta_encoder_layer_4_attention_self_key_weight = p_roberta_encoder_layer_4_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_17: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_25, [1, -1, 12, 64]);  linear_25 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_17: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_17, 1, 2);  view_17 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_26: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_roberta_encoder_layer_4_attention_self_value_weight, p_roberta_encoder_layer_4_attention_self_value_bias);  p_roberta_encoder_layer_4_attention_self_value_weight = p_roberta_encoder_layer_4_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_18: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_26, [1, -1, 12, 64]);  linear_26 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_18: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention_4: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_16, transpose_17, transpose_18, masked_fill);  transpose_16 = transpose_17 = transpose_18 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_19: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_19: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.view.default(transpose_19, [1, sym_size_int_42, 768]);  transpose_19 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_27: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_19, p_roberta_encoder_layer_4_attention_output_dense_weight, p_roberta_encoder_layer_4_attention_output_dense_bias);  view_19 = p_roberta_encoder_layer_4_attention_output_dense_weight = p_roberta_encoder_layer_4_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_10: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_456: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_10, layer_norm_8);  clone_10 = layer_norm_8 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_9: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_456, [768], p_roberta_encoder_layer_4_attention_output_layernorm_weight, p_roberta_encoder_layer_4_attention_output_layernorm_bias);  add_456 = p_roberta_encoder_layer_4_attention_output_layernorm_weight = p_roberta_encoder_layer_4_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_28: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_9, p_roberta_encoder_layer_4_intermediate_dense_weight, p_roberta_encoder_layer_4_intermediate_dense_bias);  p_roberta_encoder_layer_4_intermediate_dense_weight = p_roberta_encoder_layer_4_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu_4: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_28);  linear_28 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_29: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu_4, p_roberta_encoder_layer_4_output_dense_weight, p_roberta_encoder_layer_4_output_dense_bias);  gelu_4 = p_roberta_encoder_layer_4_output_dense_weight = p_roberta_encoder_layer_4_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_11: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_475: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_11, layer_norm_9);  clone_11 = layer_norm_9 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_10: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_475, [768], p_roberta_encoder_layer_4_output_layernorm_weight, p_roberta_encoder_layer_4_output_layernorm_bias);  add_475 = p_roberta_encoder_layer_4_output_layernorm_weight = p_roberta_encoder_layer_4_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_30: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_roberta_encoder_layer_5_attention_self_query_weight, p_roberta_encoder_layer_5_attention_self_query_bias);  p_roberta_encoder_layer_5_attention_self_query_weight = p_roberta_encoder_layer_5_attention_self_query_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:313 in forward, code: self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
              "                    view_20: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_30, [1, -1, 12, 64]);  linear_30 = None\n",
              "                    transpose_20: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_31: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_roberta_encoder_layer_5_attention_self_key_weight, p_roberta_encoder_layer_5_attention_self_key_bias);  p_roberta_encoder_layer_5_attention_self_key_weight = p_roberta_encoder_layer_5_attention_self_key_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:338 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_21: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_31, [1, -1, 12, 64]);  linear_31 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:339 in forward, code: .transpose(1, 2)\n",
              "                    transpose_21: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_21, 1, 2);  view_21 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_32: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_roberta_encoder_layer_5_attention_self_value_weight, p_roberta_encoder_layer_5_attention_self_value_bias);  p_roberta_encoder_layer_5_attention_self_value_weight = p_roberta_encoder_layer_5_attention_self_value_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:343 in forward, code: .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n",
              "                    view_22: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.view.default(linear_32, [1, -1, 12, 64]);  linear_32 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:344 in forward, code: .transpose(1, 2)\n",
              "                    transpose_22: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:363 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
              "                    scaled_dot_product_attention_5: \"f32[1, 12, (s70//(s72**2)), 64]\" = torch.ops.aten.scaled_dot_product_attention.default(transpose_20, transpose_21, transpose_22, masked_fill);  transpose_20 = transpose_21 = transpose_22 = masked_fill = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:372 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
              "                    transpose_23: \"f32[1, (s70//(s72**2)), 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:373 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
              "                    view_23: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.view.default(transpose_23, [1, sym_size_int_42, 768]);  transpose_23 = sym_size_int_42 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_33: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(view_23, p_roberta_encoder_layer_5_attention_output_dense_weight, p_roberta_encoder_layer_5_attention_output_dense_bias);  view_23 = p_roberta_encoder_layer_5_attention_output_dense_weight = p_roberta_encoder_layer_5_attention_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_12: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:389 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_528: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_12, layer_norm_10);  clone_12 = layer_norm_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_11: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_528, [768], p_roberta_encoder_layer_5_attention_output_layernorm_weight, p_roberta_encoder_layer_5_attention_output_layernorm_bias);  add_528 = p_roberta_encoder_layer_5_attention_output_layernorm_weight = p_roberta_encoder_layer_5_attention_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_34: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.linear.default(layer_norm_11, p_roberta_encoder_layer_5_intermediate_dense_weight, p_roberta_encoder_layer_5_intermediate_dense_bias);  p_roberta_encoder_layer_5_intermediate_dense_weight = p_roberta_encoder_layer_5_intermediate_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
              "                    gelu_5: \"f32[1, (s70//(s72**2)), 3072]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_35: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.linear.default(gelu_5, p_roberta_encoder_layer_5_output_dense_weight, p_roberta_encoder_layer_5_output_dense_bias);  gelu_5 = p_roberta_encoder_layer_5_output_dense_weight = p_roberta_encoder_layer_5_output_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_13: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:481 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
              "                    add_547: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.add.Tensor(clone_13, layer_norm_11);  clone_13 = layer_norm_11 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_12: \"f32[1, (s70//(s72**2)), 768]\" = torch.ops.aten.layer_norm.default(add_547, [768], p_roberta_encoder_layer_5_output_layernorm_weight, p_roberta_encoder_layer_5_output_layernorm_bias);  add_547 = p_roberta_encoder_layer_5_output_layernorm_weight = p_roberta_encoder_layer_5_output_layernorm_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:1439 in forward, code: x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
              "                    select: \"f32[1, 768]\" = torch.ops.aten.select.int(layer_norm_12, 1, 0);  layer_norm_12 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_14: \"f32[1, 768]\" = torch.ops.aten.clone.default(select);  select = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_36: \"f32[1, 768]\" = torch.ops.aten.linear.default(clone_14, p_classifier_dense_weight, p_classifier_dense_bias);  clone_14 = p_classifier_dense_weight = p_classifier_dense_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py:1442 in forward, code: x = torch.tanh(x)\n",
              "                    tanh: \"f32[1, 768]\" = torch.ops.aten.tanh.default(linear_36);  linear_36 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
              "                    clone_15: \"f32[1, 768]\" = torch.ops.aten.clone.default(tanh);  tanh = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_37: \"f32[1, 8]\" = torch.ops.aten.linear.default(clone_15, p_classifier_out_proj_weight, p_classifier_out_proj_bias);  clone_15 = p_classifier_out_proj_weight = p_classifier_out_proj_bias = None\n",
              "                    return (linear_37,)\n",
              "            \n",
              "        Graph signature: \n",
              "            # inputs\n",
              "            p_roberta_embeddings_word_embeddings_weight: PARAMETER target='roberta.embeddings.word_embeddings.weight'\n",
              "            p_roberta_embeddings_position_embeddings_weight: PARAMETER target='roberta.embeddings.position_embeddings.weight'\n",
              "            p_roberta_embeddings_token_type_embeddings_weight: PARAMETER target='roberta.embeddings.token_type_embeddings.weight'\n",
              "            p_roberta_embeddings_layernorm_weight: PARAMETER target='roberta.embeddings.LayerNorm.weight'\n",
              "            p_roberta_embeddings_layernorm_bias: PARAMETER target='roberta.embeddings.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_0_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.0.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_0_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.0.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_0_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.0.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_0_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.0.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_0_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.0.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_0_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.0.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_0_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.0.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_0_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.0.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_0_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.0.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_0_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.0.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_0_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.0.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_0_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.0.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_0_output_dense_weight: PARAMETER target='roberta.encoder.layer.0.output.dense.weight'\n",
              "            p_roberta_encoder_layer_0_output_dense_bias: PARAMETER target='roberta.encoder.layer.0.output.dense.bias'\n",
              "            p_roberta_encoder_layer_0_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.0.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_0_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.0.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_1_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.1.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_1_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.1.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_1_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.1.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_1_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.1.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_1_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.1.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_1_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.1.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_1_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.1.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_1_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.1.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_1_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.1.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_1_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.1.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_1_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.1.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_1_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.1.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_1_output_dense_weight: PARAMETER target='roberta.encoder.layer.1.output.dense.weight'\n",
              "            p_roberta_encoder_layer_1_output_dense_bias: PARAMETER target='roberta.encoder.layer.1.output.dense.bias'\n",
              "            p_roberta_encoder_layer_1_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.1.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_1_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.1.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_2_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.2.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_2_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.2.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_2_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.2.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_2_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.2.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_2_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.2.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_2_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.2.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_2_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.2.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_2_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.2.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_2_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.2.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_2_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.2.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_2_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.2.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_2_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.2.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_2_output_dense_weight: PARAMETER target='roberta.encoder.layer.2.output.dense.weight'\n",
              "            p_roberta_encoder_layer_2_output_dense_bias: PARAMETER target='roberta.encoder.layer.2.output.dense.bias'\n",
              "            p_roberta_encoder_layer_2_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.2.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_2_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.2.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_3_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.3.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_3_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.3.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_3_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.3.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_3_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.3.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_3_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.3.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_3_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.3.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_3_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.3.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_3_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.3.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_3_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.3.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_3_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.3.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_3_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.3.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_3_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.3.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_3_output_dense_weight: PARAMETER target='roberta.encoder.layer.3.output.dense.weight'\n",
              "            p_roberta_encoder_layer_3_output_dense_bias: PARAMETER target='roberta.encoder.layer.3.output.dense.bias'\n",
              "            p_roberta_encoder_layer_3_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.3.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_3_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.3.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_4_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.4.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_4_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.4.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_4_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.4.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_4_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.4.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_4_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.4.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_4_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.4.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_4_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.4.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_4_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.4.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_4_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.4.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_4_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.4.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_4_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.4.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_4_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.4.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_4_output_dense_weight: PARAMETER target='roberta.encoder.layer.4.output.dense.weight'\n",
              "            p_roberta_encoder_layer_4_output_dense_bias: PARAMETER target='roberta.encoder.layer.4.output.dense.bias'\n",
              "            p_roberta_encoder_layer_4_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.4.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_4_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.4.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_5_attention_self_query_weight: PARAMETER target='roberta.encoder.layer.5.attention.self.query.weight'\n",
              "            p_roberta_encoder_layer_5_attention_self_query_bias: PARAMETER target='roberta.encoder.layer.5.attention.self.query.bias'\n",
              "            p_roberta_encoder_layer_5_attention_self_key_weight: PARAMETER target='roberta.encoder.layer.5.attention.self.key.weight'\n",
              "            p_roberta_encoder_layer_5_attention_self_key_bias: PARAMETER target='roberta.encoder.layer.5.attention.self.key.bias'\n",
              "            p_roberta_encoder_layer_5_attention_self_value_weight: PARAMETER target='roberta.encoder.layer.5.attention.self.value.weight'\n",
              "            p_roberta_encoder_layer_5_attention_self_value_bias: PARAMETER target='roberta.encoder.layer.5.attention.self.value.bias'\n",
              "            p_roberta_encoder_layer_5_attention_output_dense_weight: PARAMETER target='roberta.encoder.layer.5.attention.output.dense.weight'\n",
              "            p_roberta_encoder_layer_5_attention_output_dense_bias: PARAMETER target='roberta.encoder.layer.5.attention.output.dense.bias'\n",
              "            p_roberta_encoder_layer_5_attention_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.5.attention.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_5_attention_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.5.attention.output.LayerNorm.bias'\n",
              "            p_roberta_encoder_layer_5_intermediate_dense_weight: PARAMETER target='roberta.encoder.layer.5.intermediate.dense.weight'\n",
              "            p_roberta_encoder_layer_5_intermediate_dense_bias: PARAMETER target='roberta.encoder.layer.5.intermediate.dense.bias'\n",
              "            p_roberta_encoder_layer_5_output_dense_weight: PARAMETER target='roberta.encoder.layer.5.output.dense.weight'\n",
              "            p_roberta_encoder_layer_5_output_dense_bias: PARAMETER target='roberta.encoder.layer.5.output.dense.bias'\n",
              "            p_roberta_encoder_layer_5_output_layernorm_weight: PARAMETER target='roberta.encoder.layer.5.output.LayerNorm.weight'\n",
              "            p_roberta_encoder_layer_5_output_layernorm_bias: PARAMETER target='roberta.encoder.layer.5.output.LayerNorm.bias'\n",
              "            p_classifier_dense_weight: PARAMETER target='classifier.dense.weight'\n",
              "            p_classifier_dense_bias: PARAMETER target='classifier.dense.bias'\n",
              "            p_classifier_out_proj_weight: PARAMETER target='classifier.out_proj.weight'\n",
              "            p_classifier_out_proj_bias: PARAMETER target='classifier.out_proj.bias'\n",
              "            c_roberta_lifted_tensor_0: CONSTANT_TENSOR target='roberta.lifted_tensor_0'\n",
              "            b_roberta_embeddings_position_ids: BUFFER target='roberta.embeddings.position_ids' persistent=False\n",
              "            b_roberta_embeddings_token_type_ids: BUFFER target='roberta.embeddings.token_type_ids' persistent=False\n",
              "            input_ids: USER_INPUT\n",
              "    \n",
              "            # outputs\n",
              "            linear_37: USER_OUTPUT\n",
              "    \n",
              "        Range constraints: {s72: VR[1, 2], s70: VR[0, 514]}\n",
              "\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "classifier = learner_inf.model.hf_model.eval()\n",
        "\n",
        "torch.onnx.export(\n",
        "    classifier,\n",
        "    torch.LongTensor([[0] * 512]),\n",
        "    'models/news-classifier-final-version.onnx',\n",
        "    verbose=True,\n",
        "    input_names=['input_ids'],\n",
        "    output_names=['output'],\n",
        "    opset_version=14,\n",
        "    dynamic_axes={\n",
        "        'input_ids': {0: 'batch_size', 1: 'sequence_len'},\n",
        "        'output': {0: 'batch_size'}\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import shape_inference\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "# 1. Define Paths\n",
        "onnx_model_path = 'models/news-classifier-final-version.onnx'\n",
        "inferred_model_path = 'models/news-classifier-inferred.onnx'\n",
        "quantized_model_path = 'models/news-classifier-quantized.onnx'\n",
        "\n",
        "print(\"--- Starting Shape Inference ---\")\n",
        "try:\n",
        "    # 2. Load the model\n",
        "    model = onnx.load(onnx_model_path)\n",
        "\n",
        "    # 3. Force Shape Inference\n",
        "    # This populates the value_info field of the model graph,\n",
        "    # which is what the quantizer needs to determine bit-depth requirements.\n",
        "    inferred_model = shape_inference.infer_shapes(model)\n",
        "\n",
        "    # 4. Save the fixed model\n",
        "    onnx.save(inferred_model, inferred_model_path)\n",
        "    print(f\"Successfully saved inferred model to: {inferred_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Shape inference failed: {e}\")\n",
        "\n",
        "print(\"\\n--- Starting Dynamic Quantization ---\")\n",
        "# 5. Run Quantization\n",
        "# We use QInt8 (signed) as it is generally better optimized for modern CPUs\n",
        "quantize_dynamic(\n",
        "    model_input=inferred_model_path,\n",
        "    model_output=quantized_model_path,\n",
        "    weight_type=QuantType.QInt8\n",
        ")\n",
        "\n",
        "print(f\"Success! Quantized model saved to: {quantized_model_path}\")\n",
        "\n",
        "# 6. Optional: Compare file sizes\n",
        "import os\n",
        "original_size = os.path.getsize(onnx_model_path) / (1024 * 1024)\n",
        "quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nModel Size Comparison:\")\n",
        "print(f\"Original: {original_size:.2f} MB\")\n",
        "print(f\"Quantized: {quantized_size:.2f} MB\")\n",
        "print(f\"Reduction: {((original_size - quantized_size) / original_size) * 100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "K8CXdk2kGp3F",
        "outputId": "24e16139-a8fd-4bcf-c63f-6bfb3d7dc4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Shape Inference ---\n",
            "Successfully saved inferred model to: models/news-classifier-inferred.onnx\n",
            "\n",
            "--- Starting Dynamic Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InferenceError",
          "evalue": "[ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (768) vs (8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2264430125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 5. Run Quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# We use QInt8 (signed) as it is generally better optimized for modern CPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m quantize_dynamic(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mmodel_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantized_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/quantize.py\u001b[0m in \u001b[0;36mquantize_dynamic\u001b[0;34m(model_input, model_output, op_types_to_quantize, per_channel, reduce_range, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, extra_options)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_opset_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m     quantizer = ONNXQuantizer(\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mper_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/onnx_quantizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, per_channel, reduce_range, mode, static, weight_qType, activation_qType, tensors_range, nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_gemm_with_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# We need to update value_infos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_and_reload_model_with_shape_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_info\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mot\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/quant_utils.py\u001b[0m in \u001b[0;36msave_and_reload_model_with_shape_infer\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_tmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_as_external_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_with_shape_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/quant_utils.py\u001b[0m in \u001b[0;36mload_model_with_shape_infer\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_with_shape_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0minferred_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_identified_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-inferred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m     \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_shapes_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferred_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferred_model_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0madd_infer_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnx/shape_inference.py\u001b[0m in \u001b[0;36minfer_shapes_path\u001b[0;34m(model_path, output_path, check_type, strict_mode, data_prop)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_shapes_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInferenceError\u001b[0m: [ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (768) vs (8)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXpFGJ4BLjjZ"
      },
      "source": [
        "# ONNX Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnAvM_OULzYi"
      },
      "source": [
        "## Normal ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ2DTh1ELmeL"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as rt\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "\n",
        "class_labels = list(encode_label_types.keys())\n",
        "\n",
        "inf_session = rt.InferenceSession('models/news-classifier-final-version.onnx')\n",
        "input_name = inf_session.get_inputs()[0].name\n",
        "output_name = inf_session.get_outputs()[0].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "65cd16e9f75c4b0ea97e7ca46d3e9553",
            "0e7dc1b5585642c78181f06d80eb6e33",
            "f3e8c38488434b1b8abe12e9eaeb0beb",
            "2c0d18881ca148159109fbe4af74a43f",
            "83de217ef82e46d380838ba4a7172bd1",
            "e218a23fec4547aca942ecb597145ada",
            "327c4744a0724d3abf64fad4828ea914",
            "21be3768a743460c9228ac3f5150654f",
            "1e971d5d13054feaad66cf913783e687",
            "73303ffc8db6462797684e63d243cba2",
            "c9bc79339c164b39b1cc617141e61d85"
          ]
        },
        "id": "CHNlselRMGCZ",
        "outputId": "6a824b39-b560-4b5c-8332-0ccd530d35b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9229 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65cd16e9f75c4b0ea97e7ca46d3e9553"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "preds = []\n",
        "for idx, row in tqdm(valid_df.iterrows(), total=valid_df.shape[0]):\n",
        "  desc = row['text']\n",
        "  input_ids = tokenizer(desc)['input_ids'][:512]\n",
        "\n",
        "  probs = inf_session.run([output_name], {input_name: [input_ids]})[0]\n",
        "  probs = torch.FloatTensor(probs)\n",
        "\n",
        "  masks = torch.sigmoid(probs) >= 0.5\n",
        "  labels = [class_labels[idx] for idx, mask in enumerate(masks[0]) if mask]\n",
        "\n",
        "  pred_genres = [0] * len(encode_label_types)\n",
        "  for label in labels:\n",
        "    pred_genres[encode_label_types[label]] = 1\n",
        "  preds.append(pred_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svMa5SB6NGVN",
        "outputId": "f25512f1-6492-4d10-8faf-c2444a7bea13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Micro) = 0.8393677053030038\n",
            "F1 Score (Macro) = 0.8249254059538107\n"
          ]
        }
      ],
      "source": [
        "metric_measures(valid_df, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X00cpvON8ja"
      },
      "source": [
        "## Quantized ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXQvwlTjN35N"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as rt\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "\n",
        "class_labels = list(encode_genre_types.keys())\n",
        "\n",
        "inf_session = rt.InferenceSession('models/book-classifier-quantized.onnx')\n",
        "input_name = inf_session.get_inputs()[0].name\n",
        "output_name = inf_session.get_outputs()[0].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3a8ec0b8f705435f91386594f3feb5d3",
            "ffcc8117c2694ce18e25292dc7507177",
            "de04fd889168437280df274f2854f09c",
            "9aa7f3123d3244c3a0445b3165e4f0af",
            "a2010abcab0944a487ae7502123a05d7",
            "a2814ab6b8054b5f9d389c0521e9787e",
            "eab85133793a408bb2e0bca45280a01a",
            "179c5a7a207d4f2a8d9f0d732971ca9c",
            "649c4d7588004044a92ed53eaa1f9518",
            "9d6e92f91401437b8d6c56fe117c1e32",
            "d6441dae8acb4bf7b615d25960d3ff95"
          ]
        },
        "id": "GnHUnQdFOECs",
        "outputId": "0268a190-64a0-4a5d-e2e1-1c89c8115215"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a8ec0b8f705435f91386594f3feb5d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/610 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preds = []\n",
        "for idx, row in tqdm(valid_df.iterrows(), total=valid_df.shape[0]):\n",
        "  desc = row['description']\n",
        "  input_ids = tokenizer(desc)['input_ids'][:512]\n",
        "\n",
        "  probs = inf_session.run([output_name], {input_name: [input_ids]})[0]\n",
        "  probs = torch.FloatTensor(probs)\n",
        "\n",
        "  masks = torch.sigmoid(probs) >= 0.5\n",
        "  labels = [class_labels[idx] for idx, mask in enumerate(masks[0]) if mask]\n",
        "\n",
        "  pred_genres = [0] * len(encode_genre_types)\n",
        "  for label in labels:\n",
        "    pred_genres[encode_genre_types[label]] = 1\n",
        "  preds.append(pred_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzqtaeWbOFmJ",
        "outputId": "c144cb42-66ba-4746-c6ce-c396feea9e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score (Micro) = 0.5283995342217622\n",
            "F1 Score (Macro) = 0.16765976287838563\n"
          ]
        }
      ],
      "source": [
        "metric_measures(valid_df, preds) #"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0169202727ad4819b577f54621336be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1519c680ad14433ca2e4e34e1afe09c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "179c5a7a207d4f2a8d9f0d732971ca9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8ec0b8f705435f91386594f3feb5d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffcc8117c2694ce18e25292dc7507177",
              "IPY_MODEL_de04fd889168437280df274f2854f09c",
              "IPY_MODEL_9aa7f3123d3244c3a0445b3165e4f0af"
            ],
            "layout": "IPY_MODEL_a2010abcab0944a487ae7502123a05d7"
          }
        },
        "51e450aa711742d6ac966abe32777d87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "649c4d7588004044a92ed53eaa1f9518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9aa7f3123d3244c3a0445b3165e4f0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6e92f91401437b8d6c56fe117c1e32",
            "placeholder": "​",
            "style": "IPY_MODEL_d6441dae8acb4bf7b615d25960d3ff95",
            "value": " 610/610 [03:21&lt;00:00,  4.89it/s]"
          }
        },
        "9d6e92f91401437b8d6c56fe117c1e32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2010abcab0944a487ae7502123a05d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2814ab6b8054b5f9d389c0521e9787e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d1ed7c712c4ba09be125f845a4bc0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa01198a6c0b49eda0556de50a065bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee7acbc31a3e4fadba58e37bcca3ca16",
              "IPY_MODEL_c5d841848a98435bae0046c4ce5772c9",
              "IPY_MODEL_c32d84d5b6f14a1b96cdce53b78eb07e"
            ],
            "layout": "IPY_MODEL_ca2a539b07254fd6aa4a7ef638d6345f"
          }
        },
        "c32d84d5b6f14a1b96cdce53b78eb07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1519c680ad14433ca2e4e34e1afe09c7",
            "placeholder": "​",
            "style": "IPY_MODEL_0169202727ad4819b577f54621336be7",
            "value": " 9229/9229 [2:22:10&lt;00:00,  1.22it/s]"
          }
        },
        "c5d841848a98435bae0046c4ce5772c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51e450aa711742d6ac966abe32777d87",
            "max": 9229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9b791579ee7413381f0fa7531b38825",
            "value": 9229
          }
        },
        "ca2a539b07254fd6aa4a7ef638d6345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6441dae8acb4bf7b615d25960d3ff95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de04fd889168437280df274f2854f09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_179c5a7a207d4f2a8d9f0d732971ca9c",
            "max": 610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_649c4d7588004044a92ed53eaa1f9518",
            "value": 610
          }
        },
        "e4862d60556b4e999f6ac605520b90ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9b791579ee7413381f0fa7531b38825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eab85133793a408bb2e0bca45280a01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee7acbc31a3e4fadba58e37bcca3ca16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4d1ed7c712c4ba09be125f845a4bc0f",
            "placeholder": "​",
            "style": "IPY_MODEL_e4862d60556b4e999f6ac605520b90ba",
            "value": "100%"
          }
        },
        "ffcc8117c2694ce18e25292dc7507177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2814ab6b8054b5f9d389c0521e9787e",
            "placeholder": "​",
            "style": "IPY_MODEL_eab85133793a408bb2e0bca45280a01a",
            "value": "100%"
          }
        },
        "5da3eca460664127a364c178a1228067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f00910d5e5549c4917d601a5c4a56d4",
              "IPY_MODEL_f46c2c4bdb624ac08d6ee56731c19143",
              "IPY_MODEL_e6827bc422ff4b0fa53331cd193f2f8c"
            ],
            "layout": "IPY_MODEL_bb2ed2d35fb04aa8b710d5da1b25bf08"
          }
        },
        "1f00910d5e5549c4917d601a5c4a56d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c66f470afd42b699805e158736adb2",
            "placeholder": "​",
            "style": "IPY_MODEL_046a66649a7f4718811474a4edc661a7",
            "value": "Downloading builder script: "
          }
        },
        "f46c2c4bdb624ac08d6ee56731c19143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06c7f473498b48349458479b01bea4b1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_363e896156bb4c99a4710f769796efb6",
            "value": 1
          }
        },
        "e6827bc422ff4b0fa53331cd193f2f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6715d55e80614626b4d27dca7f6488e5",
            "placeholder": "​",
            "style": "IPY_MODEL_ba196398a17044e89ef11f09eda2db8d",
            "value": " 4.53k/? [00:00&lt;00:00, 80.6kB/s]"
          }
        },
        "bb2ed2d35fb04aa8b710d5da1b25bf08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c66f470afd42b699805e158736adb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "046a66649a7f4718811474a4edc661a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06c7f473498b48349458479b01bea4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "363e896156bb4c99a4710f769796efb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6715d55e80614626b4d27dca7f6488e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba196398a17044e89ef11f09eda2db8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d78cab50a560408ea5dbbbda451b4368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff44dd8657b44db2a114fa08d4996751",
              "IPY_MODEL_6cc4bd3bda294711b890ed2dd26bbb11",
              "IPY_MODEL_7d5a6e219cc748ef869f034f51ccd57b"
            ],
            "layout": "IPY_MODEL_0db651db8d7e4b9e9f872124c540d3bb"
          }
        },
        "ff44dd8657b44db2a114fa08d4996751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4569feb4e77d4b328066d742c8f6a092",
            "placeholder": "​",
            "style": "IPY_MODEL_3d9a5678429a41599388c07113d4fdf9",
            "value": "Downloading extra modules: "
          }
        },
        "6cc4bd3bda294711b890ed2dd26bbb11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30aed96ce9549499cb1ed1b4d4b0732",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c63f276bb31f4051ab3e3e0000cbe274",
            "value": 1
          }
        },
        "7d5a6e219cc748ef869f034f51ccd57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13dad67435aa48c683cbd9494d591b53",
            "placeholder": "​",
            "style": "IPY_MODEL_b22dc52b21d949b2b9f8b85923ccef11",
            "value": " 3.32k/? [00:00&lt;00:00, 54.3kB/s]"
          }
        },
        "0db651db8d7e4b9e9f872124c540d3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4569feb4e77d4b328066d742c8f6a092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9a5678429a41599388c07113d4fdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e30aed96ce9549499cb1ed1b4d4b0732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c63f276bb31f4051ab3e3e0000cbe274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13dad67435aa48c683cbd9494d591b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22dc52b21d949b2b9f8b85923ccef11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a86cfcab08cc40fdb7477bebd3591b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1ecbee732e7415081d916340787de6a",
              "IPY_MODEL_0b0aafb41ece4d3892e53b266c07a4c4",
              "IPY_MODEL_c452f56070b94004abeba3850c969bb0"
            ],
            "layout": "IPY_MODEL_e5627b537eba4b098b797a461ed38427"
          }
        },
        "a1ecbee732e7415081d916340787de6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7661cc330db4af5a6928c2bb2bd52cb",
            "placeholder": "​",
            "style": "IPY_MODEL_f72c69178b124b389df63c8c28268c31",
            "value": "100%"
          }
        },
        "0b0aafb41ece4d3892e53b266c07a4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c603a5193304cd09204c17390946997",
            "max": 9229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcafa6fde03483a95c8ee1315d68641",
            "value": 9229
          }
        },
        "c452f56070b94004abeba3850c969bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2e34b0d41094d4a82228fb49e1eb9a5",
            "placeholder": "​",
            "style": "IPY_MODEL_acff09a9072f468d85fa55fb6a4d5b52",
            "value": " 9229/9229 [2:09:04&lt;00:00,  1.00s/it]"
          }
        },
        "e5627b537eba4b098b797a461ed38427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7661cc330db4af5a6928c2bb2bd52cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72c69178b124b389df63c8c28268c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c603a5193304cd09204c17390946997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdcafa6fde03483a95c8ee1315d68641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2e34b0d41094d4a82228fb49e1eb9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acff09a9072f468d85fa55fb6a4d5b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65cd16e9f75c4b0ea97e7ca46d3e9553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e7dc1b5585642c78181f06d80eb6e33",
              "IPY_MODEL_f3e8c38488434b1b8abe12e9eaeb0beb",
              "IPY_MODEL_2c0d18881ca148159109fbe4af74a43f"
            ],
            "layout": "IPY_MODEL_83de217ef82e46d380838ba4a7172bd1"
          }
        },
        "0e7dc1b5585642c78181f06d80eb6e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e218a23fec4547aca942ecb597145ada",
            "placeholder": "​",
            "style": "IPY_MODEL_327c4744a0724d3abf64fad4828ea914",
            "value": "100%"
          }
        },
        "f3e8c38488434b1b8abe12e9eaeb0beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21be3768a743460c9228ac3f5150654f",
            "max": 9229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e971d5d13054feaad66cf913783e687",
            "value": 9229
          }
        },
        "2c0d18881ca148159109fbe4af74a43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73303ffc8db6462797684e63d243cba2",
            "placeholder": "​",
            "style": "IPY_MODEL_c9bc79339c164b39b1cc617141e61d85",
            "value": " 9229/9229 [2:08:17&lt;00:00,  1.27it/s]"
          }
        },
        "83de217ef82e46d380838ba4a7172bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e218a23fec4547aca942ecb597145ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "327c4744a0724d3abf64fad4828ea914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21be3768a743460c9228ac3f5150654f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e971d5d13054feaad66cf913783e687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73303ffc8db6462797684e63d243cba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bc79339c164b39b1cc617141e61d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}